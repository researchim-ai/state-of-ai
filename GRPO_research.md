# Group Relative Policy Optimization (GRPO): Обзор и применение

## Теоретические основы GRPO

**Group Relative Policy Optimization (GRPO)** – это метод обучения с подкреплением (RL), предложенный командой DeepSeek для улучшения способности моделей к рассуждению. Он представляет собой вариант классического метода оптимизации политики **Proximal Policy Optimization (PPO)**, адаптированный для задач с оценкой на основе сравнений ответов модели ([](https://arxiv.org/pdf/2402.03300#:~:text=%E2%80%A2%20We%20introduce%20Group%20Relative,PPO)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=Group%20Relative%20Policy%20Optimization%20,to%20improve%20models%20on%20helpfulness)). В основе GRPO лежит идея обучения *политики* (например, языковой модели) на основе градиентов политики (policy gradients) с использованием функции *преимущества* (advantage), но с особым способом расчёта этого преимущества – относительно группы сгенерированных ответов, а не через отдельную ценностную функцию (критика). Это позволяет отказаться от отдельной модели-оценщика (value model) и снизить сложность обучения ([](https://arxiv.org/pdf/2402.03300#:~:text=%E2%80%A2%20We%20introduce%20Group%20Relative,PPO)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=Group%20Relative%20Policy%20Optimization%20,to%20improve%20models%20on%20helpfulness)).

**Политика и функция преимущества в RL.** В контексте RL **политика** определяет поведение агента – например, для языковых моделей это вероятность сгенерировать определённую последовательность токенов в ответ на запрос. Методы типа PPO/GRPO относятся к *градиентным методам оптимизации политики*, которые обновляют параметры политики, максимизируя ожидаемое вознаграждение. Для устойчивости обновлений вводится *функция преимущества* $A(s,a)$, отражающая, насколько выбранное действие $a$ в состоянии $s$ лучше среднего ожидаемого результата. В классическом подходе преимущество оценивается как разность полученной награды $R$ и базового уровня (baseline), обычно задаваемого ценностной функцией $V(s)$ или критиком (который приближает ожидаемую суммарную награду) ([](https://arxiv.org/pdf/2402.03300#:~:text=%E2%80%A2%20We%20introduce%20Group%20Relative,PPO)). GRPO радикально упрощает этот этап, вычисляя baseline не с помощью отдельной нейросети-критика, а статистически по группе пробных действий самой политики.

**Алгоритм GRPO.** Обучение по GRPO включает следующие основные шаги ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=1,This%20is%20different)):

1. **Генерация выборки действий (Sampling):** Для каждого входного запроса (состояния) текущая политика генерирует *несколько* вариантов ответа $o_1, o_2, \dots, o_G$ (т.е. группу из $G$ потенциальных действий, а не одно как в PPO) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=1,This%20is%20different)). За счёт случайности (например, сэмплирование с температурой в языковой модели) эти ответы различаются, предлагая варианты решений одной задачи.

2. **Оценка награды (Reward Scoring):** Каждый сгенерированный ответ оценивается с помощью функции *награды* $r_i = R(o_i)$ ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=2,This%20is%20different)). Функция награды может быть реализована разными способами: через обученную модель вознаграждения (reward model), предсказывающую предпочтение человека, или через заранее заданные правила/метрики (например, правильность решения задачи, соответствие формату ответа). В контексте обучения моделей с обратной связью человека (RLHF) обычно используется нейросеть-модель вознаграждения, обученная на сравнениях качества ответов.

3. **Вычисление преимущества (Advantage Calculation):** Вместо обращения к критику для вычисления baseline, GRPO использует *среднюю награду по группе ответов* в качестве базового уровня. Преимущество каждого ответа рассчитывается относительно среднего: $$A_i = r_i - \bar{r},$$ где $\bar{r} = \frac{1}{G}\sum_{j=1}^{G} r_j$ – средняя награда по группе ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=3,This%20is%20different)). Часто для стабильности обучения это значение нормализуют на стандартное отклонение группы: $$\tilde{A}_i = \frac{r_i - \bar{r}}{\sigma(r_1,\dots,r_G)},$$ чтобы преимущества имели масштаб порядка единиц ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=Once%20we%20have%20our%20set,deviation%20of%20all%20the%20rewards)). Такая группо-относительная нормировка отражает, *на сколько лучше или хуже среднего* оказался данный вариант ответа. Если $r_i$ выше среднего $\bar{r}$, то $A_i$ положительное (модель превзошла свой же средний уровень на этой задаче); если ниже – отрицательное. **Важно:** все ответы сравниваются *на одном и том же запросе*, что соответствует принципу, по которому обучаются модели вознаграждения – они учатся на сравнениях нескольких ответов для одного вопроса ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=Image)). Таким образом, GRPO напрямую интегрирует сравнительную природу сигнала вознаграждения.

4. **Обновление политики (Policy Optimization):** Политика обновляется посредством градиентного шага, максимизирующего полученные преимущества. Целевая функция GRPO включает слагаемое с преимуществом и регуляризационное слагаемое на расхождение политики с эталонной (референсной) моделью ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=relative%20to%20this%20baseline,This%20is%20different)). Концептуально, градиент обновления пропорционален суммарному преимуществу генераций: политика усиливает вероятности тех токенов, которые привели к выше среднего награде, и подавляет те, что дали хуже результат. Одновременно вводится штраф за отклонение от первоначальной политики, о котором подробнее ниже. Таким образом, оптимизационная цель GRPO на параметры $\theta$ можно упростить до вида: $$L_{GRPO}(\theta) = -\mathbb{E}\_{q} \sum_{i=1}^{G}\sum_{t} \left[ \frac{\pi_{\theta}(o_{i,t}|q, o_{i,<t})}{\pi_{\text{ref}}(o_{i,t}|q, o_{i,<t})} \, A_i \right] + \beta \, D_{\text{KL}}(\pi_{\theta} \,\|\, \pi_{\text{ref}}),$$ где $q$ – запрос, $\pi_{\text{ref}}$ – распределение *референсной* (исходной) политики, $D_{\text{KL}}$ – дивергенция Кульбака–Лейблера между текущей и референсной политиками, а $\beta$ – коэффициент штрафа ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=The%20objective%20is%20to%20maximize,i%2Ct%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=GRPOConfig,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)). В реализации также применяют технику клиппинга как в PPO для ограничения отношения вероятностей в пределах $[1-\epsilon,\,1+\epsilon]$, предотвращая чрезмерно большой шаг обновления ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=In%20the%20original%20paper%2C%20this,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)). При $\mu=1$ (одно обновление на генерацию) формула сводится к неклиппированному варианту ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=GRPOConfig,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)).

На интуитивном уровне, GRPO стремится *максимизировать относительное преимущество* действий, сохраняя при этом новую политику недалёкой от исходной. Благодаря использованию среднего вознаграждения группы как baseline, метод существенно уменьшает дисперсию оценки преимущества, что стабилизирует обучение без необходимости учить ценностную сеть ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=You%E2%80%99ll%20notice%20the%20values%20center,bad%20ones%20in%20this%20batch)) ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=This%20is%20pretty%20similar%20to,we%20should%20reinforce%20the%20behavior)). Фактически, GRPO достигает того же, что делал бы идеально обученный критик: даёт оценку, какова "нормальная" награда для данного запроса при текущей политике, и наказывает/поощряет отклонения от этого уровня. Но вместо ресурсов на обучение критика, модель сама *эмпирически* оценивает свой прогресс, генерируя несколько ответов и усредняя их оценки.

**Роль эталонной политики и KL-штраф.** В процессе обучения GRPO используется **референсная (эталонная) модель** – обычно копия исходной политики до RL-шагов (например, модель после супервизорного дообучения, SFT). Эта модель нужна для вычисления *дивергенции Кульбака–Лейблера (KL)*, которая включена в loss. Зачем это нужно? При чистом максимизации награды существует риск, что новая политика будет слишком сильно отклоняться от исходной – например, эксплуатировать какие-то уязвимости функции награды, генерируя неестественные тексты, лишь бы получить высокий балл (явление *reward hacking*). Чтобы этого избежать, GRPO накладывает регуляризацию: новая политика не должна уходить далеко от исходной распределения ответов ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=Without%20getting%20too%20deep%20into,far%20from%20the%20original%20ones)) ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=The%20intuition%20behind%20enforcing%20the,training)). На практике, для каждого сгенерированного токена вычисляется KL-дивергенция между вероятностями нового и референсного моделей; суммарный штраф пропорционален этой дивергенции ограничивает обновление политики. Такой подход концептуально близок к методу доверительной области (TRPO) или PPO с KL-пенальти, но в GRPO KL-вклад включён непосредственно в функцию потерь, как отдельный член, а не через модификацию награды ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=relative%20to%20this%20baseline,This%20is%20different)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=,part%20of%20the%20reward%20signal)).

Итого, **GRPO** сочетает: (a) *относительную оценку награды* внутри группы ответов – для устранения необходимости ценностной функции, и (b) *регуляризацию по KL* с исходной политикой – для стабильности и сохранения навыков модели. Это делает метод эффективным и относительно простым в реализации, особенно для задач дообучения больших языковых моделей, где прямой расчёт критика затруднителен.

## Отличия GRPO от PPO, TRPO и других методов оптимизации политики

Метод GRPO во многом основывается на идеях PPO, однако содержит несколько ключевых отличий и улучшений по сравнению с PPO, TRPO и прочими подходами в оптимизации политики.

- **Отсутствие отдельной ценностной функции (критика):** Главное отличие – GRPO исключает необходимость обучать ценностную модель для оценки baseline ([](https://arxiv.org/pdf/2402.03300#:~:text=%E2%80%A2%20We%20introduce%20Group%20Relative,PPO)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=modifies%20the%20traditional%20Proximal%20Policy,to%20improve%20models%20on%20helpfulness)). В классических алгоритмах *Actor-Critic* (к которым относятся PPO и Trust Region Policy Optimization, TRPO) актёр-политика обновляется на основе преимущества $A = R - V$, где $V$ предсказывается критиком. Обучение критика требует значительных вычислительных ресурсов (часто критик – модель сравнимого размера с самой политикой) и может быть сложно, особенно на длинных эпизодах. В PPO ценностная функция обучается параллельно с политикой, что удваивает объём параметров для хранения и обновления ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=The%20problem%20is%20that%20PPO,blue%20and%20yellow%20boxes%20below)) ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=In%20PPO%20both%20the%20policy,GRPO%20drops%20the%20value%20model)). GRPO полностью убирает критик из схемы, заменив его на групповой baseline из нескольких проб политики. Это *снижает требования к памяти и вычислениям почти вдвое* относительно PPO ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=TLDR%20,the%20code%20and%20hardware%20requirements)) ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=In%20PPO%20both%20the%20policy,GRPO%20drops%20the%20value%20model)), упрощает тренировочный pipeline и устраняет потенциальные ошибки, связанные с несовершенным обучением критика.

- **Групповая оценка преимущества вместо GAE:** В PPO/TRPO часто применяется *обобщённая оценка преимущества* (GAE) для снижения дисперсии и учёта долгосрочных возвратов. В контексте RLHF задача обычно одношаговая (ответ на один запрос), поэтому GAE упрощается до $A = R - V$. GRPO предлагает альтернативу: *групповое преимущество*, когда для каждого запроса политика оценивает саму себя, генерируя несколько возможных ответов и сравнивая их оценки ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=3,This%20is%20different)) ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=You%E2%80%99ll%20notice%20the%20values%20center,bad%20ones%20in%20this%20batch)). Средняя награда группы служит оценкой "нормальной" реакции текущей модели, а отклонение отдельного ответа от этого среднего – и есть его преимущество. Этот подход лучше соответствует природе модели вознаграждения, которая обучена на сравнении нескольких вариантов ответа для одного вопроса ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=Image)). Кроме того, такая нормировка делает сигнал обучения *относительным* (преимущества примерно распределены вокруг 0), что облегчает настройку параметров обучения и стабильность градиентов ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=You%E2%80%99ll%20notice%20the%20values%20center,bad%20ones%20in%20this%20batch)). По сути, GRPO реализует идею базовой линии (baseline) из классического REINFORCE: вычитание среднего награды по батчу снижает дисперсию градиента, но здесь *батч берётся по одному состоянию (запросу)*, что ещё точнее.

- **Включение KL-дивергенции непосредственно в функцию потерь:** В TRPO обновление политики ограничивается жёстким условием на KL-дивергенцию между новой и старой политиками (trust region), а PPO вводит это ограничение софт-способами – через **клиппинг вероятностного отношения** или добавление штрафа по KL в целевую функцию ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=b)). В оригинальном PPO для RLHF (например, в InstructGPT) часто применяют комбинацию: целевая функция – награда от модели вознаграждения *минус* коэффициент $\beta$ на KL между текущей и исходной политикой (чтобы модель не отклонялась от предобученной) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=relative%20to%20this%20baseline,This%20is%20different)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=,part%20of%20the%20reward%20signal)). GRPO делает KL-штраф более явным и обязательным компонентом лосса, рассматривая его отдельно от награды ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=relative%20to%20this%20baseline,This%20is%20different)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=,part%20of%20the%20reward%20signal)). Таким образом достигается похожий эффект сдерживания больших обновлений политики, что и в TRPO/PPO, но встроенным в формулу оптимизации. В результате GRPO обладает **стабильностью обновлений**, сравнимой с TRPO, но без необходимости вычислять сложные второй производные или проектировать шаг на доверительную область – достаточно правильно выбрать коэффициент $\beta$ и (опционально) применять клиппинг, как предложено в их реализации ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=In%20the%20original%20paper%2C%20this,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)).

- **Область применимости и простота настройки:** Классические методы вроде TRPO изначально разрабатывались для продолжительных эпизодов в игровом RL (атари, робототехника и т.п.), где нужно аккуратно аппроксимировать ценность состояния. GRPO же родился из практических потребностей обучения больших языковых моделей на относительно коротких эпизодах (генерация одного ответа) с *комплексными сигналами вознаграждения*. Он хорошо приспособлен к сценариям, где можно зафиксировать начальную модель как эталон и не хочется тратить ресурсы на обучающийся критик. За счёт уменьшения количества одновременно обучаемых нейросетей (вместо политики+критика – только политика) GRPO упрощает подбор гиперпараметров и отслеживание метрик. Например, для RLHF больших моделей GRPO заметно облегчает требования к GPU – в эксперименте можно обучить 1-миллиардную модель с 16 ГБ видеопамяти, тогда как PPO потребовал бы почти вдвое больше ресурсов ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=TLDR%20,the%20code%20and%20hardware%20requirements)). Это открывает возможность широкому кругу исследователей *тренировать RLHF-модели на сравнительно доступном железе* ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=TLDR%20,the%20code%20and%20hardware%20requirements)).

- **Сравнение схем PPO vs GRPO:** На изображении ниже схематично показаны отличия между классическим RLHF на основе PPO и новым подходом GRPO. Видно, что PPO-тренинг задействует четыре модели: обучаемую политику, замороженную референсную модель (для KL-пенальти), модель вознаграждения и обучаемого критика (value model). В GRPO-подходе ценностная модель отсутствует; вместо неё политика генерирует группу ответов ($o_1 \dots o_G$), которые оценивает модель вознаграждения, после чего происходит **групповое вычисление** преимуществ $A_1 \dots A_G$ для обновления политики ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/)) *Figure: Схема сравнения PPO и GRPO. В PPO (верхняя диаграмма) помимо модели-политики используется ценностная модель для расчёта преимущества (через обобщённую оценку GAE); новое состояние политики ограничивается KL-дивергенцией с референсной моделью. В GRPO (нижняя диаграмма) ценностная сеть не нужна – baseline оценивается как средний результат группы ответов. Это значительно сокращает ресурсы обучения ([](https://arxiv.org/pdf/2402.03300#:~:text=%E2%80%A2%20We%20introduce%20Group%20Relative,PPO)). Референсная модель и модель вознаграждения остаются частью цикла, обеспечивая сигнал качества и ограничение на сдвиг распределения.*

Другие методы оптимизации политики в контексте обучения с подкреплением для языковых моделей также можно сравнить с GRPO. Например, **REINFORCE** (ванильный Policy Gradient) не использует критика, но страдает от большой дисперсии градиента – GRPO решает это за счёт группового baseline. **A2C/A3C (Advantage Actor-Critic)** и аналогичные методы используют обучаемый критик, что роднит их с PPO и отличает от безкритикового подхода GRPO. В последнее время появились и *альтернативы RLHF*, не требующие явного шага RL – например, **Direct Preference Optimization (DPO)** или обучение путем отклонения (RFT – Rejection Fine-Tuning). Эти методы формулируют задачу обучения на человеческих предпочтениях как непосредственно оптимизацию политики под заданным критерием без явной формулы возврата. В работе DeepSeekMath отмечается, что и такие подходы можно рассматривать в единой парадигме наряду с PPO/GRPO – по сути, как упрощённые или предельные случаи RL-алгоритмов ([](https://arxiv.org/pdf/2402.03300#:~:text=%E2%80%A2%20We%20provide%20a%20unified,summarize%20several%20potential%20directions%20to)). Однако, в отличие от DPO/RFT, GRPO остаётся именно методом *обучения с подкреплением*: он использует механизм проб и оценок, градиентную оптимизацию награды, и может применяться не только на заранее собранных предпочтениях, но и в онлайновом режиме.

**Вывод:** GRPO можно воспринимать как эволюционное развитие PPO, ориентированное на простоту и эффективность при обучении больших моделей. Он сохраняет ключевые черты (градиент политики с ограничением обновления), но устраняет громоздкую часть (критика), заменив её статистической оценкой по группе. Благодаря этому GRPO позволяет достичь сопоставимых результатов с меньшими затратами, что подтверждается практикой – модель DeepSeek-R1 с GRPO смогла достичь качества, конкурирующего с крупнейшими проприетарными моделями, затратив на обучение значительно меньше ресурсов ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=TLDR%20,the%20code%20and%20hardware%20requirements)).

## Применение GRPO в машинном обучении и обучении с подкреплением

**Контекст RLHF и обучение LLM.** Наибольшую известность GRPO получил в контексте *обучения больших языковых моделей* по схемам RLHF (обучение с подкреплением с человеческой обратной связью) и смежным методикам. Изначально метод был предложен для улучшения математического рассуждения LLM в работе **DeepSeekMath (2024)** ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=Group%20Relative%20Policy%20Optimization%20,to%20improve%20models%20on%20helpfulness)). В этой работе модель 7B параметров дообучали на решении математических задач: после этапов предобучения и supervised fine-tuning, применили фазу RL с GRPO, чтобы модель научилась более точно и обоснованно решать задачи, получая сигнал награды за правильность решения и качество обоснования. Отказ от критика позволил исследователям эффективно проводить RL-файнтюнинг даже на относительно ограниченных вычислительных ресурсах, что критически важно при работе с LLM такого размера. Результат – существенный рост показателей точности на математических бенчмарках: например, на наборе задач GSM8K качество выросло с ~83% до ~88% за счёт применения GRPO ([](https://arxiv.org/pdf/2402.03300#:~:text=%28PPO%29%20%28Schulman%20et%20al,We)) (после уже хорошего результата, достигнутого инструкционным дообучением). Авторы отмечают, что GRPO дал значимый выигрыш не только на целевых математических задачах, но и улучшил способность модели к рассуждению на смежных, даже внеобластных задачах ([](https://arxiv.org/pdf/2402.03300#:~:text=Proximal%20Policy%20Optimization%20%28PPO%29,offline)), что говорит о том, что RL-шаг помог выработать более общие стратегии решения.

После успеха в DeepSeekMath, тот же подход был применён для построения модели **DeepSeek-R1 (2024/2025)** – открытой LLM, сфокусированной на сложных многошаговых рассуждениях (reasoning). В её обучении чередовались этапы стандартного SFT и этапы RL с GRPO ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=Recap%3A%20How%20R1%20Used%20GRPO)) ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=1,to%20be%20helpful%20and%20harmless)). В частности, на стадии *R1-zero* модель обучалась исключительно через RL (без human feedback, только на синтетических или программно заданных сигналах награды) с целью научиться формату решения задач и цепочкам рассуждений ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=1,Judge%20to%20filter%20incorrect%20responses)). Применялись *детерминистические* (rule-based) функции награды: например, начислялась позитивная награда за соблюдение требуемого формата ответа (отделение рассуждений в `<reasoning>` и ответа в `<answer>`), за математическую правильность решения, за корректное компилирование кода и т.п. ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=In%20building%20DeepSeek%20R1%2C%20the,like%20format%2C%20mathematics%2C%20and%20coding)). GRPO отлично подошёл для такой схеме – благодаря ему можно было легко оптимизировать по нескольким одновременно заданным критериям (формат, правильность), просто вычисляя группу метрик-вознаграждений и комбинируя их в одну суммарную награду. В отчёте показано, что даже без обучения на человеческих предпочтениях, чисто RL-подход (R1-zero) дал огромный прирост в навыках рассуждения: например, вероятность решить задачу соревнования AIME с первой попытки выросла с 15.6% до 71.0% ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=Image%3A%20prompt)). Это колоссальное улучшение, достигнутое в том числе благодаря эффективной реализации RL-обучения (GRPO) на ограниченном наборе правил – модель научилась более глубоко прорабатывать решение, генерировать длинные цепочки вывода.

Помимо проектов DeepSeek, методикой GRPO заинтересовалось сообщество open-source. Сообщается, что команда, разрабатывающая модель **Qwen** (от Alibaba), также применила GRPO для RL-доускорения своей модели ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=modifies%20the%20traditional%20Proximal%20Policy,to%20improve%20models%20on%20helpfulness)). Вероятно, это использовано для улучшения "helpfulness" – т.е. полезности и соответствия инструкциям, где сигналом награды служат либо правила (например, за вежливый ответ, отсутствие запрещённого контента), либо обученная модель вознаграждения с человеческими предпочтениями. GRPO в таких задачах удобен тем, что позволяет легко интегрировать *произвольные функции награды*: будь то проверка ответа на соответствие каким-то шаблонам или вызов внешнего оценщика. Достаточно, чтобы для каждого запроса можно было вычислить численный балл качества ответа – дальше GRPO способен этот сигнал усилить, подстраивая политику под него ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=wouldn%E2%80%99t%20you%20just%20have%20the,weights%20of%20the%20reward%20signals)). В литературе отмечено, что GRPO фактически открывает простор для экспериментов с новыми видами сигналов: *"что угодно, что можно закодировать как функцию от ответа, годится для оптимизации"* ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=wouldn%E2%80%99t%20you%20just%20have%20the,weights%20of%20the%20reward%20signals)). Это может быть даже вызов внешнего API или другой модели в реальном времени, возвращающей оценку – т.е. GRPO превращает задачу RLHF в более общую задачу RL с *плагиной* вместо среды.

**Вне RLHF:** хотя GRPO разработан с прицелом на задачи дообучения языковых моделей, его идея применима и к другим сценариям RL. Теоретически любой эпизод обучения, где можно параллельно получать несколько реализаций траектории и оценок, может воспользоваться групповым baseline. Например, для задач с известной функцией награды можно при каждом состоянии запускать несколько симуляций агента и усреднять полученные возвраты как baseline. Это напоминает методы типа *Monte Carlo* оценки с контролем вариации. Однако в традиционных динамических средах (игры, управление) получение многих вариантов развития из одного состояния может быть затруднительно или дорого. Поэтому на классических задачах вроде Atari или MuJoCo GRPO напрямую пока не заменил собой PPO/A2C. Тем не менее, в недавней работе предложен гибридный подход **Hybrid GRPO (2025)** ([[2502.01652] Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization](https://arxiv.org/abs/2502.01652#:~:text=,detailed%20mathematical%20comparison%20between%20PPO)), сочетающий эмпирическую оценку группы с классическим критиком. Авторы показывают, что можно вернуть обучаемую ценностную функцию в схему GRPO, используя её *вместе* с групповым преимуществом – это улучшает сэмпл-эффективность и стабильность, особенно в длительных эпизодах ([[2502.01652] Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization](https://arxiv.org/abs/2502.01652#:~:text=and%20Group%20Relative%20Policy%20Optimization,Experimental%20validation%20in%20a%20controlled)) ([[2502.01652] Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization](https://arxiv.org/abs/2502.01652#:~:text=learning,demonstrates%20that%20Hybrid%20GRPO%20achieves)). Гибридный метод стремится взять лучшее от обоих: низкую дисперсию оценки от группового baseline и способность критика обобщать на новые состояния. Такой интерес свидетельствует, что идеи GRPO начинают проникать и в более широкие области RL, за пределами обучения LLM.

Резюмируя, **основное применение GRPO на сегодня – это пост-обучение больших моделей (LLM) с подкреплением**, в частности с использованием сигналов от людей или детерминистических правил. Он доказал эффективность в улучшении качества рассуждений, точности и соблюдения требований моделей без колоссальных вычислительных затрат. Тот факт, что GRPO существенно снизил барьер по ресурсам для RLHF ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=TLDR%20,the%20code%20and%20hardware%20requirements)), сделал RL-доускорение доступным даже для исследователей без суперкомпьютеров – что стимулировало волну экспериментов и внедрений этого метода в open-source сообществе.

## Примеры использования, реализации и код

После появления GRPO быстро появились открытые реализации этого алгоритма, позволяющие инженерам и исследователям применять его на практике. Рассмотрим некоторые примеры и доступные инструменты для GRPO.

**Hugging Face TRL (Transformer RL) библиотека.** Команда Hugging Face добавила поддержку GRPO в свою библиотеку `trl` – набор инструментов для RL с языковыми моделями. Класс `GRPOTrainer` реализует обучение политики методом GRPO, что позволяет легко запустить RL-тренинг для любой модели, совместимой с трансформерами. Ниже приведён упрощённый пример использования `trl` для обучения модели с GRPO (адаптировано из документации ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=,from%20trl%20import%20GRPOConfig%2C%20GRPOTrainer))):

```python
from datasets import load_dataset
from trl import GRPOConfig, GRPOTrainer

# Загружаем набор данных (список текстовых запросов)
dataset = load_dataset("trl-lib/tldr", split="train")

# Определяем функцию награды: например, хотим поощрять ответы длиной близкой к 20 символам
def reward_len(completions, **kwargs):
    # Для каждой строки-ответа вычисляем отрицательное отклонение длины от 20
    return [-abs(20 - len(completion)) for completion in completions]

# Конфигурация GRPO-тренера
config = GRPOConfig(
    output_dir="exp-model-GRPO",
    train_batch_size=4,
    # ... прочие параметры обучения (скорость обучения, G=количество ответов и т.д.)
)

trainer = GRPOTrainer(
    model="Qwen/Qwen2-0.5B-Instruct",  # название модели или объект модели
    reward_funcs=reward_len,           # можно передать список нескольких функций награды
    train_dataset=dataset,
    args=config,
)

trainer.train()
```

В этом примере модель **Qwen-0.5B-Instruct** дообучается на датасете TLDR (краткие обзоры) с искусственной функцией награды, поощряющей определённую длину ответа. Обратите внимание на структуру: вместо явного критика мы передаём список функций `reward_funcs`, которые оценивают качество сгенерированного ответа (можно задать несколько аспектов). Trainer автоматически будет генерировать для каждого запроса несколько вариантов ответа, вычислять указанные награды, нормировать их в преимущества и производить шаг обновления политики по формуле GRPO. Библиотека также логирует ключевые метрики: среднюю длину ответа, среднюю награду и её дисперсию по группе, значение KL-различия с референсной моделью и т.д., что помогает мониторить процесс обучения ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=Logged%20metrics)).

Важно отметить, что `trl` позволяет задать несколько функций награды одновременно. Например, можно одновременно использовать **модель вознаграждения** (предсказывающую предпочтение пользователя) и дополнительные детерминистические критерии (форматирование, отсутствие запрещённых фраз и пр.). В этом случае итоговая награда будет суммироваться или задаваться как взвешенная комбинация, а GRPO будет оптимизировать политику поднимать суммарную награду. Такой подход очень гибок: достаточно запрограммировать необходимую метрику как функцию на Python, и её можно сразу включить в цикл обучения модели ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=wouldn%E2%80%99t%20you%20just%20have%20the,weights%20of%20the%20reward%20signals)).

**Пример кода с правилами (DeepSeek-R1):** В блоге Oxen.ai, посвящённом воспроизведению результатов DeepSeek-R1, приводятся примеры простых функций награды, использованных для обучения reasoning-модели. Например, *Accuracy reward* проверяет, является ли ответ числом и совпадает ли оно с правильным (для задач по математике или программированию), *Format reward* – проверяет наличие в тексте специальных тегов `<reasoning>` и `<answer>` ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=In%20building%20DeepSeek%20R1%2C%20the,like%20format%2C%20mathematics%2C%20and%20coding)). Эти проверки реализованы через регулярные выражения и простые условия. Примечательно, что в R1-zero не использовалась большая нейросеть-оценщик вовсе – только такие правила, что ещё больше снизило требования к памяти (по сути, в цикле обучения были только политика и замороженная референсная модель) ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=The%20Reward%20Signals)) ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=You%20don%E2%80%99t%20even%20need%20a,huge%20reduction%20in%20GPU%20requirements)). В общем случае, конечно, для тонкой настройки модели под предпочтения пользователей нужна модель вознаграждения. Но этот пример демонстрирует, насколько *минималистичным* может быть RL-пайплайн с GRPO: даже без каких-либо данных с разметкой и обученных оценщиков, модель можно улучшать, задав несколько логических критериев качества.

**Open-source проекты и репозитории.** Сообществом запущены инициативы по воспроизведению GRPO-тренинга на своих данных. Например, на Hugging Face Hub доступны демо-скрипты и ноутбуки по обучению "с нуля" reasoning-моделей с помощью GRPO (репозитории с названием вроде `train-deepseek-r1` или `mini-deepseek-r1` ([mini-deepseek-r1-aha-grpo.ipynb - GitHub](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb#:~:text=mini,We%20will))). Появились статьи и туториалы, объясняющие теорию и практику GRPO, ориентированные на разработчиков. Среди них можно отметить блог-пост *"A vision researcher's guide to PPO & GRPO"* (Yuge Shi, 2025), где автор доступно расписывает математику алгоритма и делится впечатлениями от его применения ([A vision researcher’s guide to some RL stuff: PPO & GRPO - Yuge (Jimmy) Shi](https://yugeten.github.io/posts/2025/01/ppogrpo/#:~:text=This%20is%20a%20deep%20dive,tech%20report%20in%20the%20end)), или разбор от Phil Schmid *"How DeepSeek-R1 was trained"* ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=Group%20Relative%20Policy%20Optimization%20,to%20improve%20models%20on%20helpfulness)), откуда мы заимствовали многие разъяснения. Эти материалы помогают быстрее понять отличия нового подхода и начать эксперименты с ним.

В целом, реализация GRPO несложна для тех, кто знаком с PPO. Изменения сводятся к отказу от критика (и соответствующего loss на MSE ошибки ценности) и к добавлению процедуры: для каждого состояния сэмплировать $G$ действий, вычислить их награды и нормировать преимущества. Многие фреймворки RL уже добавляют поддержку этого: помимо HF TRL, энтузиасты публикуют реализации на PyTorch Lightning, JAX/Flax и др. Поэтому воспроизвести алгоритм можно в несколько десятков строк кода ([ai/grpo.md at bc663e59f67e1b20d48f1522ed3459e7a651f7d0 - ai - Fordj](https://fordj.org/rvba/ai/src/commit/bc663e59f67e1b20d48f1522ed3459e7a651f7d0/grpo.md#:~:text=constraint%20from%20the%20reward%20calculation,how%20we%20handle%20expected%20returns)) ([ai/grpo.md at bc663e59f67e1b20d48f1522ed3459e7a651f7d0 - ai - Fordj](https://fordj.org/rvba/ai/src/commit/bc663e59f67e1b20d48f1522ed3459e7a651f7d0/grpo.md#:~:text=,calculation%20instead%20of%20value%20functions)), опираясь на публичные описания. Это позволяет исследователям *творчески экспериментировать* с GRPO: пробовать разные способы сэмплирования (например, вариативная температура или энтропия при генерации нескольких ответов), различные стратегии нормировки наград (как именно масштабировать или усекать преимущества), адаптивную настройку коэффициента KL и пр. – всё это открытые вопросы, ответы на которые постепенно появляются в новых работах.

## Актуальные исследования и релевантные источники

Метод GRPO появился совсем недавно, но уже привлёк большое внимание благодаря сочетанию эффективности и простоты. Ниже перечислены ключевые исследования и материалы, связанные с GRPO, которые могут быть полезны для углубления знаний:

- **DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models** (Zhihong Shao et al., 2024) – оригинальная статья, в которой впервые предложен GRPO ([](https://arxiv.org/pdf/2402.03300#:~:text=%E2%80%A2%20We%20introduce%20Group%20Relative,by%20solely%20using%20the%20instruction)) ([](https://arxiv.org/pdf/2402.03300#:~:text=Proximal%20Policy%20Optimization%20%28PPO%29,offline)). Описывает архитектуру обучения математической LLM, вводит термин *Group Relative Policy Optimization*, показывает преимущества отказа от критика и сравнивает GRPO с другими подходами (PPO, DPO, RFT) в единой парадигме. Доступна на arXiv: arxiv.org/abs/2402.03300.

- **DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning** (2024) – технический отчет (Tech Report) по модели DeepSeek-R1. В нём подробно описан многоэтапный процесс обучения модели для сложных рассуждений, где чередуются этапы SFT и RL (с использованием GRPO). Статья показывает, как GRPO можно применять на разных стадиях для различных целей: сначала обучить модель формату и пошаговости решения (rule-based rewards), затем – общему соответствию человеческим предпочтениям (в финальной стадии RLHF). ArXiv ID: arxiv.org/abs/??? (можно найти по названию).

- **Hugging Face TRL Documentation – GRPO Trainer** – официальная документация библиотеки 🤗TRL, раздел *GRPO Trainer*. Содержит описание математической формулы GRPO, псевдокод алгоритма и примеры использовани ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=The%20objective%20is%20to%20maximize,i%2Ct%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=GRPOConfig,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref))】. Полезно для понимания реализации и быстрого старта с кодом.

- **Блог-посты и разборы:** 
  - *Phil Schmid – "How DeepSeek-R1 was trained"* (Jan 2025 ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=Group%20Relative%20Policy%20Optimization%20,to%20improve%20models%20on%20helpfulness)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=,part%20of%20the%20reward%20signal))】 – краткое и понятное объяснение GRPO и ключевых отличий от PPO, с примерами из практики.
  - *Oxen.ai – "Why GRPO is Important and How it Works"* (Greg, Feb 2025) – глубокий разбор мотивации GRPO, его реализации, с иллюстрациями и примерами кода на nump ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=Once%20we%20have%20our%20set,deviation%20of%20all%20the%20rewards)) ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=Without%20getting%20too%20deep%20into,far%20from%20the%20original%20ones))】. Автор подчеркивает снижение вычислительных требований (~в 2 раза против PPO) и делится личным опытом успешного обучения 1B-модели на одной GPU.
  - *Yuge Shi – "A vision researcher’s guide to PPO & GRPO"* (Jan 2025 ([A vision researcher’s guide to some RL stuff: PPO & GRPO - Yuge (Jimmy) Shi](https://yugeten.github.io/posts/2025/01/ppogrpo/#:~:text=This%20is%20a%20deep%20dive,tech%20report%20in%20the%20end))】 – детальное пошаговое введение, рассчитанное на новичков в RL, где GRPO сопоставляется с PPO на интуитивном уровне. Полезно для тех, кто хочет разобраться в формулах.
  - *Unsloth.ai – "Train your own R1 reasoning model locally (GRPO)"* (2025) – практическое руководство по запуску обучения модели с GRPO на локальном компьютере. Демонстрирует, что с помощью GRPO можно воспроизвести часть обучения DeepSeek-R1 своими силами.

- **Hybrid GRPO (Soham Sane, 2025)** – исследовательская работа, развивающая идеи GRP ([[2502.01652] Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization](https://arxiv.org/abs/2502.01652#:~:text=,detailed%20mathematical%20comparison%20between%20PPO)) ([[2502.01652] Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization](https://arxiv.org/abs/2502.01652#:~:text=and%20Group%20Relative%20Policy%20Optimization,Experimental%20validation%20in%20a%20controlled))】. Автор предлагает сочетать групповое преимущество с классическим критиком (ценностной функцией), чтобы улучшить сэмпл-эффективность и уменьшить вариативность оценок. Статья содержит формальное сравнение PPO, стандартного GRPO (как в DeepSeek) и гибридного варианта, а также эксперименты в контролируемой среде. Этот труд указывает направление, куда может эволюционировать GRPO для более общих применений.

- **Репозитории и код:** 
  - Репозиторий DeepSeek (возможно, на GitHub) – может содержать скрипты или псевдокод реализации GRPO из оригинальной работы.
  - Пример реализации GRPO на несколько сотен строк кода – ссылка из LinkedIn поста Jen W ([ai/grpo.md at bc663e59f67e1b20d48f1522ed3459e7a651f7d0 - ai - Fordj](https://fordj.org/rvba/ai/src/commit/bc663e59f67e1b20d48f1522ed3459e7a651f7d0/grpo.md#:~:text=DeepSeek%20just%20made%20waves%20,scores%2C%20significantly%20reducing%20training%20resources)) ([ai/grpo.md at bc663e59f67e1b20d48f1522ed3459e7a651f7d0 - ai - Fordj](https://fordj.org/rvba/ai/src/commit/bc663e59f67e1b20d48f1522ed3459e7a651f7d0/grpo.md#:~:text=Here%27s%20what%20happened%3A))】, где инженер делится опытом, как его собственная реализация PPO для математических задач невольно превратилась в GRPO (с отделением KL и усреднением результатов группы).
  - Gist с демо GRPO (упоминается на Oxen.ai ([Why GRPO is Important and How it Works](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/#:~:text=reference%3A))】 – содержит код для обучения маленькой LLaMA с GRPO, показывая, что запуск возможен даже в ограниченной среде.

Заключая, **Group Relative Policy Optimization** представляет собой значимый шаг вперёд в области RL-подходов для обучения моделей. Теоретически изящный (использует относительно простой трюк baseline, известный в RL, в новом контексте) и практически эффективный, GRPO уже продемонстрировал свою ценность на реальных задачах. Он снизил порог вхождения для экспериментов с RLHF, позволив более широкому кругу специалистов доучивать большие модели под свои цели. Текущее развитие метода – в исследованиях его свойств, ограничений и возможных улучшений – продолжается. Можно ожидать, что идеи, заложенные в GRPO, найдут применение и в других алгоритмах, когда речь зайдет об оптимизации политики в условиях, где доступно множество оценок или сравнений. Для тех, кто интересуется обучением с подкреплением и крупными моделями, ознакомиться с GRPO и его применениями будет весьма полезно, тем более что доступно уже много материалов и реализованного кода на эту тему.

