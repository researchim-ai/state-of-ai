Ребята из **DeepSeek** хотели посмотреть, нам SFT вот прям всегда нужен?

В качестве базы взяли свою [DeepSeek V3](https://github.com/deepseek-ai/DeepSeek-V3) (671 миллиард параметров) модельку.  

RL-алгоритм взяли тоже свой - **Group Relative Policy Optimization (GRPO)** который впервые представили в статье  

**DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models**  
https://arxiv.org/abs/2402.03300

![image](https://github.com/user-attachments/assets/f98d631a-f800-4828-8d44-35a7d8063e96)

## DeepSeek RL VS OpenAI RL

Ещё в 2017 году в компании OpenAI разработали алгоритм PPO ([Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)).
Это алгоритм обучения с подкреплениям архитектуры **Actor-Critic**. В такой архитектуре у нас есть две нейронки, которые обучаются -  **policy** (или actor) и **value** (или critic). 
policy у нас так сказать главная нейронка, value - вспомогательная и нужна именно для обучения. policy генерирует действия, а value их оценивает.

В контексте обучения с подкреплением у нас обычно есть следующие составляющие: 

- среда/окружение (environment) - это обычно смоделированная система которая посылает состояния агенту,
- агент - тот кто взаимодействует с этим окружением (**policy** нейронка обычно ассоциируется с этим агентом и генерирует для него действия).

В GRPO нужен только **policy**.


**GRPO:**

Генерируем нескольких ответов на один и тот же вопрос, получаем группу ответов  
Награда нормализуется внутри этой группы ответов. (Group в названии метода становится понятнее)  
Модель учится уже на этих нормализованных наградах.  

Подробнее

Если сравнивать PPO и RLHF от OpenAI с GRPO то имеем следующие улучшения:



Так-то всё здесь:
https://github.com/deepseek-ai/DeepSeek-R1
