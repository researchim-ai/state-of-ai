Отлично. Я подготовлю подробный ресерч на русском языке о современных методах квантизации LLM, включая AWQ, GGUF и другие. Включу математические основы, принципы работы, особенности, плюсы и минусы, а также практические аспекты использования и совместимости.

Как только будет готово — дам знать!


# Квантизация больших языковых моделей: современные методы (2024–2025)

## Введение: зачем нужна квантизация LLM

Большие языковые модели (LLM) обладают огромным числом параметров (до сотен миллиардов), что требует гигантских объёмов памяти и вычислений для хранения и инференса. **Квантизация** – это метод сжатия модели, при котором веса (и иногда активации) представляются с пониженной точностью, например, 8-битовыми или 4-битовыми числами вместо 16- или 32-битных чисел с плавающей запятой. Проще говоря, квантизация “упрощает” представление весов модели, уменьшая объём данных за счёт допустимой потери точности, подобно тому как пересказ сокращает текст книги.

**Зачем это нужно?** Сокращение разрядности чисел напрямую уменьшает объем памяти, занимаемый моделью, и ускоряет вычисления за счёт более простых операций. Это позволяет загружать и выполнять LLM на устройстве с ограниченными ресурсами (GPU/CPU/мобильное устройство), где исходная модель не поместилась бы. Например, перевод весов из FP16 в int4 теоретически даёт 4-кратное уменьшение памяти. Кроме того, работа с более низкой точностью может повышать пропускную способность: современные ускорители (GPU, специализированные чипы) часто поддерживают ускоренные INT8/INT4 вычисления, что потенциально даёт рост скорости инференса. В результате квантизированные модели могут выполняться быстрее и на более дешёвом оборудовании.

Однако, плата за это – снижение точности модели. Неосторожная квантизация может сильно ухудшить качество ответов LLM, особенно если диапазон значений весов и активаций плохо укладывается в низкий битовый формат. Поэтому за последние годы активно развиваются **современные методы квантизации LLM**, позволяющие максимизировать выигрыш в эффективности **без значимой потери качества модели**. Ниже мы рассмотрим основные виды квантизации и конкретные методы, актуальные в 2024–2025 годах, включая: **GPTQ, AWQ, SmoothQuant, 8-битовую квантизацию (bitsandbytes), 4-битовую QLoRA, формат GGUF** и др.

## Основные схемы квантизации моделей

Методы квантизации можно разделить по нескольким признакам:

* **Пост-обучебная квантизация (Post-Training Quantization, PTQ):** веса уже обученной модели напрямую преобразуются в низкую разрядность **без дальнейшего тренинга**. Это самый простой и дешевый подход – достаточно небольшого *калибровочного* набора данных, чтобы настроить параметры квантования (например, масштабы). PTQ “встраивает” ошибку округления в модель, не пытаясь её исправить обучением. Современные PTQ-методы стремятся минимизировать эту ошибку специальными алгоритмами (см. GPTQ, AWQ и др.).

* **Квантизация с учетом обучения (Quantization-Aware Training, QAT):** модель обучается (или дообучается) с имитацией низкой точности на каждом шаге. Грубо говоря, при QAT на этапе тренировки в модель встроены операции квантования/деквантования, благодаря чему параметры адаптируются к низкой точности. Это обычно даёт лучшее качество (ошибка квантования компенсируется обучением), но **крайне затратно** для LLM – требуется полный или частичный повтор обучения модели уже в квантовом режиме. Для моделей масштаба GPT-3 QAT практически не применяется из-за стоимости, однако может использоваться для более мелких моделей или отдельных слоёв.

* **Смешанная точность (Mixed Precision):** компромиссный подход, когда разные части модели хранятся/вычисляются с разной разрядностью. Например, типично в современных нейросетях обучение идёт в FP16/BF16 (16-бит) вместо FP32, что уже можно считать формой квантования с минимальной потерей качества. Для инференса LLM популярна схема: **веса в 8-bit, а вычисления (или части весов) в 16-bit**, либо оставление небольшой доли «проблемных» весов в высокоточной форме. Такой гибридный подход позволяет избежать узких мест точности. Пример: метод LLM.int8() хранит 99.9% весов в int8, а **наиболее “выбивающиеся” по величине элементы – в FP16**, добиваясь почти полного сохранения качества. Также к смешанной точности относится быстро набирающий популярность формат **FP8** (8-битные floats, поддерживаемые на GPU H100) – он позволяет ещё ускорить вычисления, сохраняя динамический диапазон за счёт плавающей запятой.

* **Квантизация весов vs. активаций:** В большинстве случаев квантуют *веса* модели, поскольку они фиксированы и поддаются офлайн-оптимизации. *Активизации* (промежуточные выходы нейронов) тоже можно квантовать до int8 или ниже для ускорения, но это сложнее – их распределения зависят от входа и могут иметь редкие выбросы. Схемы бывают:

  * **Weight-only quantization** – все веса (матрицы параметров) переводятся в низкий битовый формат, а активации продолжают вычисляться в обычном формате (FP16/FP32). Многие современные алгоритмы (GPTQ, AWQ) нацелены именно на веса.
  * **W+A quantization** – квантование и весов, и активаций до int8 или ниже. Это даёт максимум выигрыша (полностью целочисленный инференс), но нужно аккуратно обработать выбросы активаций. Метод SmoothQuant решает эту проблему специальным “сглаживанием” распределений, о нём ниже.
  * **Static vs Dynamic quantization** – для активаций различают *статическое* квантование (масштабы рассчитаны заранее на калибровочных данных и фиксированы) и *динамическое* (масштаб вычисляется “на лету” из текущего батча данных). В LLM часто применяют динамическое int8-квантование активаций: например, в PyTorch `torch.int8` механизм измеряет диапазон активации в каждом вызове слоя и квантует по нему. Динамическое квантование удобнее (не требует предварительной калибровки активаций), но аппаратно сложнее и может приводить к нестабильности качества на неожиданных данных.

Ниже мы последовательно рассмотрим несколько конкретных методов квантизации, применяемых к LLM, их математические идеи, а также сравним их по точности, скорости и совместимости.

## GPTQ – пост-обучебная 4-битная квантизация с минимальной ошибкой

**GPTQ (Generative Pre-trained Transformer Quantization)** – это алгоритм PTQ, предложенный в конце 2022 года, специально для сжатия GPT- моделей. Его ключевая особенность – использование *второго порядка* (approximate second-order) при квантовании весов, что позволяет очень точно подбирать квантизированные значения. Проще говоря, GPTQ учитывает чувствительность модели к изменению каждого веса, например, приближая влияние квантования через гессиан или кривизну ошибки. Это отличает его от naїve-квантования по min/max диапазону.

**Как работает GPTQ?** Алгоритм проходит по слоям (или даже столбцам весовой матрицы) и выполняет квантование весов *с компенсацией ошибки*: после округления очередной группы весов GPTQ вычисляет возникающую разницу в выходе слоя (ошибку) и корректирует оставшиеся веса, уменьшая влияние этой ошибки. Такой **итеративный одномоментный** (one-shot) процесс сходен с решением задачи минимизации MSE между исходными и квантованными выходами слоя. В итоге удаётся существенно снизить накопление ошибок округления. Формально GPTQ минимизирует квадратичное отклонение выхода слоя с квантованными весами от исходного выхода на небольшом наборе данных.

**Параметры GPTQ:** обычно применяется 4-битное квантование (int4) для весов, как оптимальный компромисс. Авторы отмечали, что возможно ужать модель до 3 или даже 2 бит на вес, но при 2 битках качество сильно падает. Рекомендуемый режим – 4 бита на вес **с групповой квантизацией**: веса разбиваются на группы (например, по 128 элементов) с общим масштабом на группу. Групповой размер (group\_size) 128 обычно обеспечивает лучший баланс точности/скорости; если задать `group_size=-1`, будет эквивалентно квантованию каждого столбца раздельно (что улучшает качество, но замедляет вычисления). Также в GPTQ вводят параметр *дэмпфирования* (damp, обычно \~0.1) – он сглаживает экстремальные значения ошибок при коррекции, предотвращая переусиление поправок.

**Качество и эффективность:** GPTQ удивительно хорошо сохраняет точность оригинальной модели. В первоначальной работе было показано, что 175-миллиардная модель (GPT-3) сжата до 3-4 бит на вес **практически не теряет в перплексии и качестве** относительно FP16 модели. Это более чем двукратный выигрыш по памяти **без ухудшения качества**. Впервые стало возможным выполнять инференс GPT-3-175B на одном GPU благодаря GPTQ. В скорости инференса тоже выигрыш: за счёт работы целочисленных ядер 4-битная модель на GPU A100 достигает \~3.25× ускорения относительно FP16.

**Применение:** GPTQ быстро стал **де-факто стандартом квантования LLM для GPU-инференса**. Появились готовые инструменты: например, библиотека `AutoGPTQ` и скрипты от сообщества (TheBloke и др.) позволяют квантовать модели (Llama-2 и др.) и сохранять их в формате `.safetensors`. HuggingFace Transformers с версии 4.31 получил встроенную поддержку GPTQ-моделей через класс `GPTQConfig`. Для использования достаточно вызвать `model.from_pretrained(..., device_map="auto", quantization_config=GPTQConfig(bits=4, ...))`. Также поддерживается несколько форматов сохранения GPTQ (v1, v2).

**Совместимость:** GPTQ-версии моделей обычно рассчитаны на работу *на GPU* – они хранятся зачастую в сжатом виде (например, в 4-bit с собственным кодом разжатия). На CPU напрямую GPTQ-модель не запустить, но можно сконвертировать её в формат GGML/GGUF (см. раздел про GGUF) или выполнить деквантизацию обратно во float. Сообщество активно распространяет GPTQ-модели в HuggingFace для популярных LLM, что делает метод весьма доступным.

**Вывод:** GPTQ – мощный PTQ-метод, позволяющий сжать LLM до 4 бит/вес *с незначительным падением качества*. Его сильные стороны – отсутствие необходимости в дообучении, универсальность (применим к разным моделям) и поддержка современными фреймворками. К недостаткам можно отнести требование иметь некоторый калибровочный датасет (хоть небольшой) и всё еще значительные требования к GPU-памяти для запуска очень больших моделей (модель должна помещаться в память GPU, т.е. выигрыш 4x по памяти всё равно нужен, но если модель превышает VRAM более чем в 4 раза – GPTQ не спасёт).

## AWQ – Activation-Aware Weight Quantization (активационно-осведомлённая квантизация)

**AWQ** – новейший PTQ-метод (MLSys 2024 Best Paper) от MIT HAN Lab, предназначенный для **четырёхбитного квантования весов LLM** с минимальными потерями точности. Ключевая идея AWQ: не все веса одинаково важны для работы модели, и наиболее *значимые* (с точки зрения влияния на активации) нужно обработать особо. AWQ предлагает определять “важность” каналов весов, глядя на распределение **активаций** модели. Отталкиваясь от аксиомы, что *веса сами по себе не расскажут, какие из них критичны – надо смотреть на отклик сети*, AWQ собирает статистику активаций и на её основе выделяет наиболее нагруженные весовые каналы.

**Как работает AWQ?** Алгоритм выполняется в несколько шагов:

1. **Сбор статистики активаций:** прогоняется небольшой набор данных через модель (PTQ-калибровка), и фиксируются распределения активаций в слоях – прежде всего, масштабы и редкие выбросы значений.
2. **Выделение важных весов:** используя эти данные, AWQ определяет \~1% каналов (столбцов весовой матрицы), которые дают наибольший вклад в активациях (грубо, те, при квантовании которых ошибка на выходе слоя была бы максимальна). Именно эти веса называют *salient weights* (ключевые/«заметные»).
3. **Защита ключевых весов:** вместо того, чтобы оставлять эти 1% весов в высоком качестве (что привело бы к нежелательной смешанной разрядности), AWQ применяет **математически эквивалентное преобразование**: масштабирует (увеличивает) эти наиболее важные веса в \$k\$ раз и одновременно компенсирует это, разделив на \$k\$ соответствующие активации (или последующие веса). После такого масштабирования диапазон важных весов расширяется, и при квантовании в 4 бита их относительная ошибка снижается. Иными словами, информация от этих каналов кодируется с большей точностью в рамках 4-битового диапазона. Это своего рода *перераспределение трудности квантования*: изначально активации имели “тяжёлые хвосты” (большие значения), а веса – относительно ровное распределение; AWQ переносит часть разброса с активаций на веса, сделав активации гладкими, а весам позволив занять больше квантовых уровней.
4. **Квантование весов:** после масштабирования всех каналов веса квантируются в 4 бита (например, стандартным равномерным квантованием с нулевой точкой). Благодаря предыдущему шагу, даже квантованные, они дают минимальную ошибку на активациях.

Важно, что AWQ **не требует обучения или обратного распространения ошибок** – вся процедура выполняется аналитически (по статистике) и, как показали эксперименты, не переобучается под небольшое калибровочное множество, сохраняя обобщающую способность.

**Достигаемое качество:** AWQ позволяет сжать модели до 4-битных весов **с пренебрежимо малым снижением качества** на широчайшем спектре задач. Авторы отмечают, что AWQ *превосходит все существовавшие на тот момент методы* на бенчмарках (в том числе GPTQ) и, благодаря хорошей общей способности, успешно квантирует даже **инструкционно-натренированные** LLM (например, разговорные модели) и **мультимодальные** LLM, что ранее было проблемой. Фактически, **защитив всего \~1% весов**, можно устранить львиную долю ошибки квантования. Такой подход близок по идее к ранее известному «Outlier-aware quantization» (учёт выбросов), но реализован более аппаратно-дружелюбно.

**Особенности производительности:** AWQ сохраняет все веса в 4-битном формате, избегая смешанных типов во время вычислений. Это значит, что модель может выполняться на GPU эффективно, без постоянных преобразований типов. Авторами представлен прототип фреймворка **TinyChat** под 4-битные модели, который с помощью слияния операций и оптимальной укладки весов показал >3× ускорение инференса по сравнению с FP16 реализацией HuggingFace на тех же GPU. На практике, уже сейчас имеются реализации AWQ-квантования: библиотека `llm-awq` от MIT HAN Lab, `AutoAWQ` от Intel, а также поддержка в `Transformers` (начиная с v4.33). Чтобы загрузить AWQ-модель, достаточно установить `autoawq` и использовать `AutoModelForCausalLM.from_pretrained(..., quantization_config={"quant_method": "awq", ...})` – трансформеры автоматически подхватят 4-бит веса.

**Совместимость:** Изначально AWQ был ориентирован на GPU. Сейчас поддерживаются архитектуры LLaMA, Mistral и др. в HuggingFace Transformers; недавно добавлена поддержка AWQ-моделей и в инструментах на базе llama.cpp (через формат GGUF – было слито в репозиторий llama.cpp в 2023). Это значит, что квантованную AWQ-модель можно конвертировать и запустить на CPU через GGUF. Формат хранения AWQ обычно – стандартные файлы моделей HF с дополнительным ключом `quant_method": "awq"` в config (как в примере config.json выше).

**Плюсы и минусы:** Преимущества AWQ – **отличное качество при 4-бит**, отсутствие необходимости дообучения, скорость на GPU, универсальность на разных задачах. Недостатки – необходимость иметь небольшой калибровочный набор для активаций (хотя это сотня предложений, сбор которых не проблема) и пока что относительная новизна (не все новые модели имеют готовые AWQ-весы, хотя это быстро меняется). Отметим, что некоторые реализации AWQ фактически делают то, от чего метод теоретически ушёл: **оставляют 1% самых важных весов в FP16** вместо квантования. Это упрощает реализацию (не надо масштабировать веса), но требует хранить небольшую часть модели в повышенной точности. Такой подход тоже возможен: например, загружая модель через `transformers` с AWQ, по умолчанию остальные 99% весов будут в int4, а 1% – в fp16. В любом случае, суммарный выигрыш в памяти и скорости остаётся огромным.

## SmoothQuant – сглаживание выбросов для 8-битных весов и активаций

Первые методы квантования LLM (до 2022 г.) столкнулись с проблемой: хотя **8-битные веса** часто уже почти не снижали качество (например, GPT-3 175B в INT8 терял всего \~1 perplexity), но *активизации* нельзя было напрямую перевести в 8-бит из-за редких очень больших значений. При попытке полноценно выполнить модель в INT8 где-то происходил сбой точности – обычно в слоях внимания или FFN с большими “выбросами” на некоторых токенах.

**SmoothQuant** (Xiao et al., ICML 2023) предложил элегантное решение: **сгладить выбросы активаций путём переноса их масштаба в веса**. Этот метод близок по духу к описанному AWQ (оба родом из одной лаборатории), но ориентирован на 8-бит и формулирован независимо. Основная идея: раз веса легче квантовать, а активации – труднее, то можно “поделиться сложностью” между ними. SmoothQuant выполняет линейное преобразование, эквивалентное умножению на единицу, но перераспределяющее величины: для каждого слоя подбирается вектор масштабов, который домножается на активизации входа слоя, и обратный ему вектор применяется к весам слоя. В результате разброс значений между разными каналами активаций значительно уменьшается (они становятся “ровнее” – smooth), а веса могут немного вырасти в разбросе. Поскольку веса квантовать проще (они статичны и можно точно подобрать масштаб под них), общее качество от этого почти не страдает, зато **все активации теперь укладываются в 8-битный диапазон без перенасыщения**.

**Полный INT8 для LLM:** SmoothQuant продемонстрировал, что после такой обработки \*\*можно квантовать и веса, и активации модели в INT8 (W8A8) со **слишком малой, чтобы заметить** потерей качества. Конкретно, на моделях OPT, BLOOM, LLaMA и других удаётся достичь <0.5 падения точности (например, по GPT-accuracy) при 8-битномInference. Это **двукратное сокращение памяти** и \~1.5× ускорение без специального оборудования. Авторы отмечали, что SmoothQuant позволяет обслуживать даже 530-миллиардную модель на одной машине с 8×A100 GPUs, что раньше было невозможно.

**Метод и поддержка:** SmoothQuant – **training-free PTQ**, не требующий дообучения, только калибровочных данных для оценки масштаба выбросов. Компания Intel интегрировала SmoothQuant в свой инструментарий (Neural Compressor, VNNI). Кроме того, открытый проект LMDeploy предоставил реализацию SmoothQuant для моделей BERT и GPT, позволяющую автоматически выполнить сглаживание и выгрузить 8-битную модель (например, для TensorRT). HuggingFace в документации также описывает шаг *SmoothQuant* как часть INT8 оптимизации. Более того, появился **SmoothQuant+**, расширение метода для 4-бит (W4A16?) квантования, интегрированное во фреймворк vLLM, хотя детали выходят за рамки обзора.

На практике, идеи SmoothQuant легли и в основу некоторых других подходов. Например, предшественник – метод **Outlier Channel Splitting** (Dettmers et al. 2022) – решал проблему выбросов активаций, **разбивая аномально большие компоненты активаций на отдельные каналы** перед квантованием. Этот метод фактически внедрён в `bitsandbytes` (LLM.int8(), см. далее) и работает на уровне реализации слоёв. SmoothQuant же делает это аналитически, “впитывая” выбросы в веса, что более удобно для экспорта модели.

**Итог:** SmoothQuant – важный шаг, показавший, что **полная 8-битная квантизация LLM достижима**. Он адресовал главный изъян INT8 квантования – активации – сохранив эффективность (никаких сверхсложных вычислений при инференсе не добавляется, кроме пары перемножений на константы). С SmoothQuant 8-бит стал новым стандартом: после него появилось множество работ, где 8-битные модели использовались повсеместно (на CPU через AVX512 VNNI, на GPU через Tensor Cores). К недостаткам метода можно отнести разве что необходимость офлайн-предобработки модели (нужно выполнить скрипт сглаживания и сохранить новые веса). Но в экосистеме есть уже готовые квантованные модели, так что пользователь может получить готовую INT8 модель без лишних хлопот. Если же требуется максимальная экономия памяти – 8 бит может быть недостаточно, тогда на сцену выходят 4-битные методы (GPTQ, AWQ и др., которые мы рассмотрели).

## 8-битовая квантизация и библиотека BitsAndBytes (LLM.int8)

Одним из первых практических решений для квантования LLM была реализация **8-битового инференса** от Тим Деттмерса и соавторов (2022) под названием *LLM.int8()*. Она легла в основу известной библиотеки **`bitsandbytes`**, которая теперь служит стандартным backend-ом для 8-битных моделей в HuggingFace.

**Проблема выбросов:** Исследователи заметили, что в трансформерах крупного размера лишь небольшое число внутренних признаков (features) принимает экстремально большие значения (“outliers”) и портит картину при равномерном квантовании. Решение LLM.int8() – **выделить эти выбросы и обрабатывать их с повышенной точностью**. Конкретно, для каждой матричной операции \$Y = XW\$ они разбивают веса/активации на две части:

* «Обычная» часть, которая хорошо квантуется в INT8.
* «Outlier»-часть – несколько процентов самых крупных элементов (например, те самые скрытые признаки, раздувающие активации).

Во время инференса, продукт \$XW\$ считается как сумма двух: \$XW = (XW)*{int8} + (XW)*{fp16}\$, где основная масса вычислений делается в int8, а вклад от выбросов – отдельно в fp16. Благодаря этому, удаётся **полностью восстановить точность модели** по сравнению с FP16-базовым прогоном. Деттмерс в блоге отмечает, что простой 8-бит (без раздельной обработки) даёт заметный провал точности, а полный метод LLM.int8() возвращает модель к исходному уровню. Для этого пришлось сочетать *квантование по вектору* (vector-wise quantization, т.е. отдельный масштаб для каждого канала) и *mixed-precision decomposition* (выделение outlier-компоненты).

**bitsandbytes:** Эта библиотека реализует вышеописанное на CUDA, предоставляя drop-in замену оптимизаторов и линейных слоёв. В Transformers с её помощью можно загрузить модель в 8-бит, указав `model.from_pretrained(..., load_in_8bit=True, device_map="auto")`. В результате все линейные слои модели заменятся на 8-битные, использующие специальный класс `Int8Params` для хранения весов. **Как понять, что outlier-часть хранится?** – при инициализации модель занимает чуть больше памяти, чем чисто 8-бит: помимо int8 весов хранится *ниже ранг* outlierов в FP16 (по умолчанию \~0.05% элементов). В исходной работе деттмерс показал, что такой подход уменьшает память модели примерно в 2 раза, **не теряя качества на задачах вроде вопросов с выбором ответа** (модели 6B и 175B почти совпали с FP16).

**Производительность:** 8-битные вычисления на современных GPU (Ampere, Ada) ускоряются тензорными ядрами, поэтому в идеале int8 может быть в \~2 раза быстрее FP16. Однако из-за overhead (смешивание с FP16 для outliers) реальный выигрыш невелик. Тем не менее, **8-бит инференс часто даже быстрее 16-бит** на тех же GPU, а уж экономия VRAM – вдвое. Поэтому LLM.int8 быстро стал популярным у энтузиастов, запускающих LLM локально. Например, RTX 3090 с 24 ГБ VRAM может в 8-бит поместить модель \~13 млрд параметров, тогда как в FP16 – только 6–7 млрд.

**Ограничения:** 8-битная квантизация `bitsandbytes` ориентирована на **GPU NVIDIA**. Библиотека долгое время была CUDA-специфичной (хотя недавно появилась экспериментальная поддержка ROCm для AMD). Для CPU int8 инференса существуют другие решения (см. GGML/GGUF). Кроме того, bitsandbytes требует CUDA>=11 и не работал на Windows (только через WSL). Но в Linux он интегрируется прозрачно.

**8-бит оптимизаторы:** Помимо инференса, `bitsandbytes` предложил и **8-bit оптимизаторы** для обучения больших моделей (8-bit Adam, LAMB и др.). Идея в том, что градиенты и моменты можно хранить и обновлять в 8-битном формате с небольшими потерями, что значительно экономит память при обучении. Это позволило обучать модели на GPU с меньшей памятью. Например, в статье Dettmers 2022 показано, что 8-битный Adam может работать без потери качества по сравнению с 32-битным, экономя до 75% памяти оптимизатора.

**Вывод:** 8-битный метод (LLM.int8) и bitsandbytes стали первым реально массовым прорывом в квантизации LLM: почти без усилий пользователи получили возможность запускать большие модели на своих GPU. Несмотря на появление 4-битных методов, 8-бит по-прежнему актуален, особенно когда нужно сохранить максимум качества и упростить интеграцию (в transformers 8-bit режим очень стабилен). В таблице сравнения мы учитываем этот метод отдельно.

## QLoRA – эффективное дообучение LLM с 4-битными весами

**QLoRA (Quantized LoRA)** – это не столько отдельный метод квантования, сколько техника, сочетающая квантование и дообучение. Представлен в работе Dettmers et al. 2023, QLoRA позволил fine-tune’ить большие модели (до 65B) на одной GPU 48 ГБ, достигая качества полноценного 16-битного обучения.

**Идея:** Мы можем взять предобученную LLM, **сквантовать её до 4 бит**, *заморозить* квантованные веса, и обучать только небольшие добавочные слои – адаптеры LoRA. Таким образом, базовая модель занимает минимум памяти, а обучаемые параметры – копейки (несколько миллионов весов). Градиенты распространяются через квантованные веса (не меняя их) в адаптеры. Это позволяет эмулировать эффект обучения всей модели, но обновлять лишь маленькие матрицы.

**Ключевые нововведения QLoRA:**

* Используется специальный тип данных **NF4 (NormalFloat4)** для квантования весов. В отличие от простых int4, NF4 – это 4-битный формат чисел, оптимальный для нормально распределённых величин. Фактически, NF4 – разновидность *квантования по квантилям*: диапазон значений разбивается не равномерно, а так, чтобы в каждом из 16 квантовых уровней было \~равное число значений. Это позволяет более эффективно использовать биты, особенно когда распределение весов близко к гауссову. Авторитетно утверждается, что **NF4 на практике значительно превосходит стандартное 4-битное квантование** по точности. (Интуитивно: обычное int4 тратит слишком много уровней на плотную центральную часть распределения и зря “растягивается” на дальние концы с малым числом значений; NF4 исправляет это.)
* Введён приём **Double Quantization (двойное квантование)** – квантование квантов. Чтобы ещё снизить память, *масштабы блоков*, появляющиеся при 4-бит квантовании, сами хранятся не в FP16, а квантованными (например, 8-битами). Это уменьшает накладные расходы хранения множителей без заметного влияния на итоговую точность.
* Использование **Paged Optimizers** – техники оптимизации памяти при обучении, чтобы пиковые потребления во время чекпоинтов градиентов не вызывали OOM. Это более технический момент: грубо, оптимизатор выделяет память странично, избегая резких скачков.

**Результаты:** QLoRA смогла обучить семейство моделей (под названием Guanaco) размером 7B–65B, которые **достигли 99.3% качества ChatGPT на Vicuna-бенчмарке** всего за 24 часа обучения. То есть, маленькая 33-миллиардная модель с QLoRA показывала уровень лучших открытых моделей, сохраняя при этом весь процесс обучения в пределах одной GPU. В работе продемонстрировано, что **4-битное дообучение (NF4) не уступает обычному 16-битному** – на ряде задач точность совпала в пределах погрешности.

**Применение:** Чтобы применить QLoRA, берут модель (обычно в формате `bitsandbytes` 4-bit), добавляют адаптеры через PEFT (Library от HF для LoRA), и обучают как обычно. HuggingFace предоставил готовые примеры и даже GUI (через HuggingFace PEFT и Trainer). После обучения адаптеры можно либо хранить отдельно (в виде дельта-weights), либо слить с моделью: для слияния сначала деквантовать модель в FP16, затем добавить LoRA дельты. Однако интересный факт: инференс можно выполнять и **без слияния** – на лету применяя LoRA к квантованным весам. В bitsandbytes реализована поддержка LoRA прямо в 4-битных линейных слоях, что экономит время и память.

**Совместимость:** QLoRA = NF4 4-bit модель + LoRA. NF4 формат поддерживается `bitsandbytes` (начиная с v0.39). Таким образом, *для инференса* QLoRA-модели нужен `bitsandbytes` и соответствующий настройка (в `AutoModelForCausalLM.from_pretrained` указываем `load_in_4bit=True`, а затем применяем LoRA через PEFT). Если же слить LoRA в модель и сохранить, то получится, по сути, 16-битная модель (с поправленными весами), потерявшая преимущество квантования. Поэтому обычно распространяют именно отдельные LoRA и инструкцию к какой 4-битной модели их применять.

**Ограничения:** 4-битная модель сама по себе (без дообучения) потеряет заметно в качестве относительно 16-битной. Например, LLaMA-65B в чистом PTQ 4-bit (даже NF4) всё же просядет на сложных задачах. Но QLoRA компенсирует это, дообучив под конкретные задачи. Поэтому QLoRA не является методом для сжатия *универсальной* модели без потерь – это скорее способ эффективного **transfer learning**. Тем не менее, многие открытые модели сейчас публикуются сразу в формате QLoRA (например, Llama-2-Chat 7B QLoRA), чтобы пользователи могли дообучить их локально.

В контексте нашего обзора QLoRA интересен тем, что демонстрирует **возможность обучения с квантованием** (то есть сочетание PTQ и QAT): хоть базовые веса и не обновляются, модель учится проявлять максимум возможностей, имея квантованные ограничения. Это направление (обучение с низкой точностью) – активно развивающаяся область, приближающая нас к тому, чтобы в будущем LLM обучались изначально в 8 или 4 битах.

## Формат GGUF и квантование для CPU (GGML-квантования)

Когда речь заходит о развёртывании LLM на устройствах без мощных GPU, на помощь приходят специализированные инструменты для CPU. **GGML/GGUF** – это экосистема и формат хранения, разработанные для эффективного инференса моделей на CPU (например, с помощью `llama.cpp`).

**Что такое GGUF?** Это **формат файла** (расширение `.gguf`), представляющий собой единый бинарный контейнер с весами модели и метаданными. Он создан на основе библиотеки GGML – легковесного С++ фреймворка, реализующего вычитания сети на CPU с оптимизацией под различные инструкции. GGUF пришёл на смену более старому формату GGML (.bin или .ggml файлов): он более гибкий, хранит всю мета-информацию о модели и поддерживает больше квантованных типов. Преимущество GGUF – можно хранить модель в одном файле, включая токенизатор, что удобно для распространения.

**Квантованные типы в GGUF:** Этот формат поддерживает множество схем квантования весов (все – weight-only):

* **Q4\_0, Q4\_1** – оригинальные 4-битные схемы, появившиеся ещё для LLaMA-1. Они используют **блочное квантование**: делят веса каждого слоя на блоки (например, по 32 элемента) и квантуют каждый блок отдельно. Q4\_0 – более простая (без смещения нуля), Q4\_1 – с коррекцией нулевой точки (чуть точнее). Разница: Q4\_1 хранит для каждого блока не только масштаб, но и минимальное значение (смещение), что улучшает распределение квант. уровней.
* **Q5\_0, Q5\_1** – аналогично, 5-битные; дают лучше качество, занимая немного больше памяти.
* **Q8\_0** – 8-бит квантование (обычно используется для слоёв вроде Embed, где 4-бит даёт сильный проигрыш).
* **K-quant (Q2\_K, Q3\_K, Q4\_K\_M, Q5\_K\_S etc.)** – более новые схемы (конец 2023) с **неоднородным квантованием**. Они названы с суффиксом K (например, `Q4_K_M`), где помимо основной разрядности могут комбинироваться разные биты на разные части весов, или общий пул бит на группу весов. Идея: какие-то слои/каналы квантовать грубее (меньше бит), а какие-то тоньше, оставаясь в среднем в рамках бюджетных 4 бит. Например, схема может распределять 4 бита на одни значения и 5 на другие динамически, добиваясь лучшего качества, чем ровно 4 везде. Такая “адаптивная” квантизация приближает GGML-формат к алгоритмическим методам типа AWQ. K-схемы обычно дают прирост качества 0.5-1 perplexity при тех же 4 бит средних. В сообществе появились типы `Q4_K_M` (M – модель Мистраль), `Q5_K_S` (S – Sequoia?), подгоняющие квантование под конкретные модельные особенности.
* **I-quant (например, IQ4) – “iterative quantization”** (начало 2024). Судя по исходникам, I-кванты применяют итеративный алгоритм (возможно, что-то вроде GPTQ) при квантовании, что улучшает точность, но делает инференс на CPU тяжелее (нужно больше вычислений при разжатии). I-кванты появились совсем недавно и их профит/расход ещё исследуется.

Для конечного пользователя все эти сложности скрыты за именами файлов. Например: `llama-2-7b.Q4_1.gguf` – модель LLaMA-2 7B в формате GGUF с квантованием 4-бит Q4\_1; или `guanaco-33B.Q4_K_M.gguf` – 33B модель в 4-битной K-схеме типа M.

**Производительность на CPU:** Квантованные модели .gguf позволяют загружать LLM даже на компьютер без GPU. Они оптимизированы под SIMD-инструкции (AVX, AVX2, AVX-512 VNNI) и могут задействовать многоядерность. Например, LLaMA-13B Q4\_0 может отвечать \~1–2 токена/с на современном 16-ядерном CPU – не быстро, но возможно. Меньшие модели (7B) могут работать в районе 4–5 токенов/с. Чем агрессивнее квантование (меньше бит), тем больше выигрыш в памяти и тем меньше данных надо перемножать – но растёт относительная ошибка и может снижаться качество. Потому пользователи балансируют: для 13B часто берут Q5\_1 или Q4\_K, чтобы качество сильно не падало, для 30B – Q4\_1.

**GGUF vs GPTQ:** Интересный момент – эти подходы часто сравнивают. **GPTQ vs GGML-кванты**: в реддите отмечают, что *GGML-quant (Q4) нацелен на быстродействие, а GPTQ – на точность*. GPTQ хранит веса в формате, требующем разжатия и сложных адресаций – поэтому GPTQ-модель быстрее только на GPU (где можно распараллелить и хранить в памяти как есть). На CPU GPTQ затруднительно, там проще использовать GGUF. С другой стороны, GPTQ в 4 бит чуть качественнее, т.к. тратит биты “умнее” (минимизирует ошибку). Новые K/I квантования в GGUF, впрочем, сокращают этот разрыв.

**Использование GGUF:** HuggingFace Transformers теперь умеет **загружать GGUF-модели** обратно в PyTorch для финетюнинга. То есть, можно взять .gguf, прогнать через `AutoModel.from_pretrained(..., gguf_file=...)` – библиотека деквантирует веса в FP32, и модель готова к дообучению или конверсии. Обратный путь – скрипт `convert-hf-to-gguf.py` позволяет из обычной модели получить .gguf со всеми выбранными типами квантования. Таким образом, экосистемы соединены: разработчики могут тренировать модель в HF, а потом выгружать её для CPU-энтузиастов.

**Другие форматы:** Помимо GGUF/GGML и GPTQ, существуют и другие форматы квантованных моделей:

* **INT8 ONNX/TensorRT:** для продакшена часто модель экспортируют в ONNX с INT8 оптимизацией (обычно через PTQ + калибровка), затем выполняют на TensorRT или OpenVINO. Это более сложный путь, но на специфическом железе (GPU TensorCore, VPU) даёт максимум.
* **FBGEMM / FBGEMM-GPTQ:** в Meta AI выпустили int8 и 4-bit поддержку в своих фреймворках (например, PyTorch FBGEMM backend). Transformers также имеет `BitsAndBytesConfig` и `GPTQConfig` для интеграции разных бэкендов (в меню документации HF видны и другие: EETQ, HQQ, SPQR и т.д., которые соответствуют научным разработкам).
* **Apple Accelerate int8:** на macOS в tensorflow-metal или CoreML тоже есть свои INT8 реализации, хотя для LLM конкретно больше используют вышеупомянутые.

Для полноты картины, **таблица ниже** суммирует обсуждаемые методы квантования LLM по ключевым параметрам:

| **Метод**                    | **Тип квантования**                               | **Биты**              | **Точность (качество)**                                                                                                         | **Производительность**                                                                                                                                          | **Совместимость**                                                                                                                              |
| ---------------------------- | ------------------------------------------------- | --------------------- | ------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| **GPTQ**                     | PTQ, оптим. округление (второй порядок) для весов | 2–4 (обычно 4)        | \~Незначительное падение (4-bit)<br>Незаметно на перплексии; 2-bit – сильное падение                                            | \~3× быстрее FP16 на GPU A100 (int4 TCores); <br>память \~4× меньше (4-bit)                                                                                     | GPU (CUDA): хорошо поддерживается (HF Transformers, AutoGPTQ). <br>CPU: требуется конверсия (GGUF) для запуска                                 |
| **AWQ**                      | PTQ, активационно-осведомлённое квантование весов | 4 (веса)              | Минимальное снижение (≈FP16 на многих задачах)<br>Сохраняет точность даже на instruct и мультимоделях                           | \~3× ускорение инференса (с оптим. ядром);<br>память \~8× меньше FP32 (4-бит)                                                                                   | GPU: HuggingFace/AutoAWQ, TinyChat (CUDA). <br>CPU: поддержка в llama.cpp (через GGUF) появилась недавно                                       |
| **SmoothQuant**              | PTQ, сглаживание активаций, квантование W8A8      | 8 (веса+акт)          | Незначительная потеря (W8A8 почти как FP16)<br>Перплексия рост <0.5 на LLMs                                                     | До **1.56×** быстрее FP16 за счёт int8 ops;<br>память **в 2 раза меньше** (8→16 бит)                                                                            | CPU: эффективность на x86 (AVX512 VNNI). <br>GPU: TensorRT, HF (через int8 + SmoothQuant скрипт). Integrate в vLLM, Intel Neural Compressor    |
| **LLM.int8 (bitsandbytes)**  | PTQ, векторное квантование + разделение выбросов  | 8 (веса, \~0.1% в 16) | \~Полностью сохраняется качество модели<br>(после включения outlier-обработки)                                                  | \~1.3× быстрее FP16 (на GPU с Tensor Cores);<br>память \~2× меньше FP16                                                                                         | GPU: поддерживается (HF `load_in_8bit`). <br>CPU: нет (для CPU лучше GGUF int8)                                                                |
| **bitsandbytes 4-bit (NF4)** | PTQ (часть QLoRA), квантование весов NormalFloat4 | 4 (веса)              | Без дообучения – умеренное падение (нужно адаптировать).<br>С дообучением (QLoRA) – качество как 16-bit fine-tune               | Инференс: ≈FP16 скорость (int4 math на GPU Ampere реализована через int8 эмуляцию); память \~4× меньше FP16.<br>Обучение: 65B модель на 1 GPU 48ГБ              | GPU: HF `load_in_4bit` (CUDA). <br>CPU: нет прямой поддержки (можно конвертировать в GPTQ/GGUF)                                                |
| **QLoRA**                    | QAT (PEFT) – 4-bit веса + обучаемые LoRA          | 4 + LoRA (16)         | Достигает **99% качества** полноразмерной модели<br>(для задач инструкций/чат)                                                  | Обучение: в десятки раз меньше затрат памяти (GPU RAM) и быстрее (нет нужды проходов по всем весам). Инференс: как 4-bit модель с небольшими накладными на LoRA | GPU: поддерживается (HF PEFT + bitsandbytes). <br>CPU: можно применить LoRA к деквантованной модели (т.е. потеря benefits)                     |
| **GGUF (Q4/Q5 квантования)** | PTQ, блочное квантование (равномерное)            | 2–8 (веса)            | Заметное падение при 4-бит (перплексия +1–3), лучше в 5-бит.<br>Квантование Q4\_K и др. улучшает качество (близко к GPTQ 4-bit) | Зависит от бит:<br>4-bit \~2× быстрее 8-bit;<br>но CPU bound (многопоточный C++).<br>Память: \~8× меньше FP32 (4-bit)                                           | CPU: отлично (llama.cpp, GPT4All, etc.). <br>GPU: можно использовать через offload (часть модели в VRAM). HF может конвертировать GGUF→PyTorch |

*(Примечание: оценки скорости даны приблизительно для сравнительной картины; реальная скорость зависит от реализации.)*

## Заключение

Квантизация превратилась из академического интереса в необходимый инструмент для работы с большими языковыми моделями. Сегодня существует богатый арсенал методов, позволяющих сократить размер LLM в несколько раз – от **8-битных методов без потерь качества** до **экстремальных 3-4 битных алгоритмов**, способных уместить десятки миллиардов параметров на одном устройстве.

**Выбор метода** зависит от целей:

* Для ускорения *инференса на GPU без потери качества* – подойдут INT8 (SmoothQuant, bitsandbytes) или 4-bit GPTQ/AWQ, если готовы мириться с незначительным риском снижения точности.
* Для *развёртывания модели на CPU/мобиле* – пригодятся GGUF-кванты (4-8 бит), жертвуя некоторой точностью ради работы без GPU.
* Если нужно *дообучить* большую модель с ограниченной памятью – **QLoRA 4-bit** стала промышленным стандартом финетюнинга LLM в 2023 году.

Важно отметить, что развитие продолжается. В 2024 году мы видим появление гибридных подходов (например, квантование с **разреженностью**, когда модель упрощается не только снижением бит, но и занулением части весов), а также первые попытки обучать модели *сразу в квантованном формате* (чтобы избежать потери информации при последующей квантизации). Одновременно улучшается аппаратная поддержка: NVIDIA внедряет поддержку FP8/INT4 прямо в ядра, x86-процессоры получают инструкции типа AMX INT8.

Таким образом, квантизация уже стала неотъемлемой частью цикла разработки LLM – от пост-обучебной оптимизации до эффективного диплоймента. Грамотное применение рассмотренных методов позволяет разработчикам и исследователям **значительно сэкономить ресурсы**, сделав большие модели доступными и быстродействующими без существенного ущерба для их интеллектуальных возможностей.
