# Обработка естественного языка: классические и современные методы без больших языковых моделей

## Введение

Обработка естественного языка (Natural Language Processing, NLP) — это область искусственного интеллекта, которая занимается анализом и пониманием человеческого языка с помощью компьютеров. В последние годы широкую известность получили большие языковые модели (LLM) вроде GPT-3 и GPT-4, умеющие генерировать связный текст. Однако NLP не сводится только к этим гигантским моделям. Существует богатая история и множество методов NLP, появившихся задолго до эпохи LLM, а также современные подходы, которые *не* опираются на огромные объемы данных и параметры. В этом обзоре мы рассмотрим эволюцию методов NLP без LLM: от ранних правил и статистических моделей до небольших нейросетевых и гибридных решений. Мы постараемся объяснить ключевые концепции понятным языком, ориентируясь на широкую аудиторию, включая новичков. Вы узнаете о классических методах (мешок слов, TF‑IDF, регулярные выражения, n-граммы, лексические базы вроде WordNet), о статистических алгоритмах (наивный Байес, SVM, CRF, HMM, деревья решений), об архитектурах нейросетей до трансформеров (RNN, LSTM, GRU, CNN, механизмы внимания), а также о современных тенденциях — небольших моделях, гибридных и символических системах. Мы также обсудим примеры прикладных задач (распознавание сущностей, POS-теггинг, синтаксический анализ, извлечение отношений, машинный перевод, чат-боты, семантический поиск) и инструменты (spaCy, NLTK, Stanza, Flair, TextBlob, Snips и др.), сопровождая обзор примерами кода на Python и наглядными диаграммами и таблицами.

## Краткая история развития методов NLP (до эпохи LLM)

История NLP насчитывает несколько десятилетий и прошла через смену парадигм. Ниже перечислены ключевые этапы эволюции методов обработки языка до появления трансформеров и LLM:

* **1950–1960-е: Правила и символический ИИ.** Первые программы для работы с языком основывались на жестко запрограммированных правилах и грамматиках, созданных лингвистами. Например, в 1966 году появилась программа-чатбот ELIZA, которая имитировала диалог, подставляя реплики по шаблонам, хотя и не понимала их смысла. В этот период господствовал символический подход: считалось, что для понимания языка компьютеру нужны словари, грамматические правила и логические выводы.

* **1980-е: “Статистическая революция”.** С ростом вычислительных мощностей и доступностью текстовых корпусов произошел сдвиг от ручных правил к статистическим моделям. Алгоритмы начали **учиться на данных**, оценивая вероятности слов и фраз. Появились модели типа n-грамм (учитывающие последовательности из *n* слов), скрытые марковские модели (HMM) для речи и текста, и другие методы, использующие частоты слов. Такие статистические подходы оказались более гибкими и смогли учитывать вероятностную природу языка. Однако и они имели ограничения — требовали много размеченных данных и не всегда улавливали тонкий смысл.

* **1990-е: Первые нейросетевые модели.** К концу 90-х в NLP начали применять нейронные сети, особенно рекуррентные (RNN) для работы с последовательностями. **Рекуррентные нейронные сети** умеют хранить состояние и, теоретически, учитывать контекст из предыдущих слов. В 1997 году была предложена архитектура LSTM (Long Short-Term Memory) — разновидность RNN, решившая проблему “короткой памяти” базовых рекуррентных сетей. LSTM с ее механизмом *долгократкосрочной памяти* позволила значительно лучше учитывать дальний контекст в предложении, преодолев ограничения классических RNN. Тем временем статистические методы по-прежнему развивались: в 1990-е были популярны модели на основе максимальной энтропии (логистическая регрессия) и усовершенствованные стохастические грамматики для парсинга предложений.

* **2000–2010-е: Статистическое машинное обучение и первые достижения глубокого обучения.** В 2000-х годах стандартными инструментами NLP стали алгоритмы машинного обучения: **наивный Байес**, **SVM (машины опорных векторов)** и **CRF (Conditional Random Fields)** – они применялись для классификации текстов, распознавания именованных сущностей, разметки частей речи и т.д. (подробнее об этих методах – в следующих разделах). Около 2010 года начали набирать силу методы *глубокого обучения*: нейросети с несколькими слоями стали показывать всё лучшие результаты в задачах вроде распознавания речи и машинного перевода. В 2013 году появилась модель **word2vec** – нейросетевой подход к обучению **вещественных векторных представлений слов** (*эмбеддингов*), позволивший автоматически извлекать семантику слов из текстов. **Векторные представления** слов заменили традиционные мешки слов, ибо позволяли моделям “понимать” похожесть слов по смыслу. К середине 2010-х seq2seq-модели с вниманием (attention) совершили прорыв в машинном переводе.

* **2017: Появление трансформеров.** Вехой стал 2017 год, когда была предложена архитектура **Transformer** (статья “Attention is All You Need”). Трансформеры использовали механизм *самовнимания* и позволили обучать очень большие языковые модели параллельно на масштабных корпусах текста. Это открывает эпоху LLM: начиная с 2018 года появляются модели типа BERT, GPT и др., содержащие сотни миллиардов параметров и демонстрирующие впечатляющие способности. Однако детальное рассмотрение трансформеров выходит за рамки нашего обзора – мы сосредоточимся на подходах, альтернативных LLM. Важно понимать, что *все предыдущие поколения методов NLP не исчезли*: они продолжают применяться там, где более уместны по точности, скорости или простоте, и нередко интегрируются с новыми подходами.

## Классические методы NLP: от мешка слов до WordNet

На заре NLP, до широкого распространения обучения моделей на больших данных, большинство задач решалось с помощью относительно простых, понятных методов. Эти **классические подходы** заложили фундамент для дальнейшего развития. Рассмотрим их основные виды.

### Представление текста: мешок слов и TF‑IDF

Одним из самых ранних и простых способов представить текст для алгоритмов был подход **“мешок слов” (Bag-of-Words, BoW)**. В этой модели документ трактуется как *неупорядоченное множество слов*, без учета грамматики и порядка следования. Фактически, строится словарь всех встречающихся в корпусе слов, а каждый документ представляется вектором частот: для каждого слова хранится, сколько раз оно встретилось. Например, для предложений «**I love NLP**» и «**NLP loves Python**» BoW-вектор просто отразит, какие слова и как часто присутствуют, игнорируя различия в форме («love» vs «loves») и порядке слов.

Основное преимущество BoW – простота. Такие векторы можно подать на вход классическим алгоритмам машинного обучения. Однако у мешка слов есть и ограничения: **теряется порядок слов и контекст** (фраза «собака укусила человека» в BoW не отличима от «человек укусил собаку»), не учитывается полнозначность слов (часто встречающиеся слова вроде “это”, “и” могут доминировать), размерность вектора равна размеру словаря (что для больших текстов приводит к разреженным векторам огромной длины).

Для частичного устранения этих недостатков вводится мера **TF‑IDF** (Term Frequency–Inverse Document Frequency). TF‑IDF расширяет BoW, взвешивая слова по их важности: учитывается не только частота слова в данном документе (TF), но и редкость этого слова в корпусе документов (IDF). Идея в том, что слова, встречающиеся во всех документах (например, “the”, “это”), получают низкий вес IDF, а уникальные термины, характерные для данного текста, – более высокий. Таким образом, **TF‑IDF понижает значимость общеупотребимых слов и выделяет ключевые слова документа**. Например, если в каждом тексте корпуса есть слово "год", его IDF будет низким, и даже большое количество вхождений не сделает этот термин важным для конкретного документа. Напротив, редкое слово вроде "кварк" получит высокий вес. TF‑IDF стал стандартным подходом в информационном поиске и классификации текстов в 90-х и 2000-х годах, так как он прост в вычислении и часто улучшает качество по сравнению с сырым мешком слов. Однако и TF‑IDF наследует основной недостаток BoW – не учитывает порядок слов и лингвистический контекст; кроме того, он не отражает семантическую близость различных слов (например, синонимы считаются разными признаками). Эти проблемы позже стали решаться с помощью **плотных векторных представлений** – см. раздел про эмбеддинги. Тем не менее, BoW и TF‑IDF до сих пор нередко используются как базовые признаки или быстрые базовые линии при решении новых задач.

### Правила и шаблоны: регулярные выражения

Другим краеугольным камнем классического NLP являются **правил\_based методы**. До появления больших обучаемых моделей многие задачи решались написанием **шаблонов и правил** “вручную”. Например, для извлечения фактов из текста инженеры составляли набор регулярных выражений (regex) или шаблонов на основе частей речи. **Регулярные выражения** – мощный инструмент поиска по строкам, позволяющий задавать шаблоны (например, шаблон `\d{3}-\d{3}-\d{4}` соответствует телефонному номеру в формате XXX-XXX-XXXX). В NLP регэкспы применяются для распознавания простых сущностей, таких как даты, Email-адреса, номера и т.п. Например, можно вытащить все Email из текста, задав шаблон вида `r"[A-Za-z0-9._-]+@[A-Za-z0-9._-]+\.[A-Za-z]+"`. Правила на основе шаблонов могут учитывать и контекст — например, в чат-ботах раннего поколения (вроде ELIZA) ответы генерировались путем сопоставления пользовательской реплики с заранее заданными паттернами (например, фраза “*мне кажется X*” могла соответствовать правилу ответа “*Почему тебе кажется X?*”).

**Символический подход** (ручное кодирование знаний о языке) доминировал в 1960–70-е. Его плюс в том, что правила можно понять и проконтролировать — система будет вести себя предсказуемо при соблюдении всех оговоренных случаев. Такие методы хорошо работают в узких, формальных доменах, где легко прописать все варианты (например, анализ медицинских отчетов по заданному шаблону). Недаром предприятия, требующие высокой точности и контроля (юридические документы, финансовые отчеты), традиционно полагались на rule-based NLP. **Однако у них есть серьёзные минусы:** правила требуют ручной разработки и регулярного обновления (что трудоемко и требует лингвистической экспертизы), трудно охватить все разнообразие естественного языка, правила плохо справляются с шумом и опечатками, и вовсе разваливаются, когда текст выходит за рамки предусмотренных сценариев. Язык очень гибок, и чисто правил\_based система либо будет чрезмерно ограниченной (чтобы избежать ошибок), либо станет содержать тысячи исключений. В реальности, **комбинация правил и обучаемых моделей** часто дает наилучший результат: правила обеспечивают высокую точность на известных структурах, а статистика покрывает непредвиденные случаи. Такой гибридный подход мы рассмотрим далее.

Но даже сегодня *регулярные выражения* и простые правила остаются полезным инструментом. Их применяют на этапе предварительной обработки текста (например, для токенизации или нормализации), для фильтрации заведомо лишней информации, а также в задачах, где требуются **жесткие гарантии** (скажем, извлечь все номера счетов из документа по фиксированному формату). К примеру, прежде чем пытаться разобрать сложное предложение парсером, можно регулярками вырезать все URL и Email, заменив их на специальные маркеры, — так модель не споткнется на странных последовательностях символов. В общем, навыки работы с шаблонами по-прежнему ценны для NLP-инженера.

### N-граммы и языковые модели Маркова

Для учета локального контекста в текстах классический подход – использовать **n-граммы**. *N-грамма* – это последовательность из N подряд идущих слов (или символов). Например, для фразы «я люблю NLP» список 2-грамм (биграмм) будет: \["я люблю", "люблю NLP"]. N-граммы позволяют частично учитывать порядок слов. Особенно широко они применялись в **языковых моделях** до эпохи нейросетей. **Статистическая языковая модель на основе n-грамм** оценивает вероятность последовательностей слов, исходя из частот N-грамм в корпусе. Предполагается (так называемое марковское допущение), что вероятность следующего слова зависит только от пары-тройки предыдущих, а не от всего предшествующего контекста. Например, триграммная модель оценивает P(слово | два предыдущих слова). Если в тренировочном корпусе часто встречается фраза "машинное обучение", то модель даст высокую вероятность слову "обучение" после "машинное". Такие модели успешно применялись в автодополнении текста, распознавании речи и машинном переводе вплоть до 2010-х. Их преимущества: относительная простота реализации и быстрая работа, если *n* мало. Но **главный недостаток n-граммных моделей – экспоненциальный рост вариантов с увеличением N**. Для больших N данных не хватает: даже при N=3 можно не встретить в корпусе какую-то комбинацию, и вероятность будет нулевая (проблема разреженности). Приходится вводить сглаживание вероятностей. Кроме того, n-граммы, как и BoW, не улавливают дальнодействующих зависимостей: модель на биграммах не “помнит” слова более чем за одно назад, из-за чего, например, не может связать подлежащее и сказуемое, если между ними длинная конструкция.

Несмотря на эти ограничения, n-граммы долго были основой **статистического машинного перевода (SMT)**. Например, в знаменитом методом IBM Models (80-е годы) перевод строился по вероятностям биграмм и триграмм в языках, а также вероятностям соответствия слов между языками. Даже после появления нейронных сетей **algorithms на n-граммах** (например, Okapi BM25 в поисковых системах) остаются сильными базами: они очень быстрые и часто достаточно точные для простых задач. N-граммы также используются как признаки в моделях классификации: например, для определения спам-сообщений могут учитываться часто встречающиеся 2-3-граммы слов. В целом, n-граммные модели – важный кирпичик в здании NLP, и понимание их работы полезно для понимания более сложных подходов.

### Лексические базы знаний: WordNet и не только

Помимо статистики и правил, в классическом NLP широко применялись **лексические базы знаний** – специально созданные словари, тезаурусы, онтологии, собирающие информацию о словах и их взаимоотношениях. Наиболее известная из них – **WordNet**. WordNet (разрабатываемый с 1985 г. в Принстонском университете) – это *лексическая база английского языка*, где слова сгруппированы в **синсет**ы (множества синонимов), а синсеты связаны разными семантическими отношениями (родо-видовые отношения — гиперонимы и гипонимы, часть–целое, ассоциации и т.д.). Проще говоря, WordNet одновременно выполняет роль толкового словаря и тезауруса: для каждого понятия указаны близкие по смыслу слова, более общие и более частные термины, примеры употребления. Например, в WordNet слово *“автомобиль”* связано как гипоним с синсетом *“транспортное средство”*, имеет синоним *“машина”*, мероним *“руль”* (часть машины) и т.д. Такая структура позволяет компьютеру **расширять понимание слов**: зная, что "метрополия" — гипероним слова "город", система может догадаться, что речь о населенном пункте, даже если прямо не сказано.

WordNet нашел применение во множестве задач: **разрешение лексической многозначности** (выбор значения слова по контексту), **расширение запросов** в поиске (добавление синонимов к запросу), **оценка семантической близости** слов и т.д. Например, с помощью WordNet можно вычислить расстояние между понятиями “кошка” и “тигр” по графу гиперонимов, получив меру сходства по смыслу. Помимо WordNet для английского были созданы аналогичные базы для других языков (RuWordNet для русского и т.п.), а также специализированные лексиконы: словари синонимов/антнонимов, технические глоссарии, онтологии для медицины (UMLS) и пр. В задаче анализа тональности часто использовались **лексиконы сентимента** – списки слов с маркировкой эмоциональной окраски (положительная, отрицательная). Например, словарь AFINN или SentiWordNet предоставляют оценку "полярности" для большого числа слов (happy = +0.8, terrible = –0.9 и т.п.), что позволяет суммировать тональность текста простым подсчетом.

Достоинство лексических баз – они привносят в алгоритм **заранее известные человечеству знания** о языке. Но их слабое место – ограниченность и статичность. WordNet покрывает далеко не все новые термины или специализированные жаргонизмы; сленг, новые значения слов, метафоры – все это уходит за рамки. Тем не менее, даже в наше время, когда векторные модели сами пытаются выводить отношения между словами, *лексические базы не утратили ценности*. Они часто используются в гибридных системах: например, чтобы проверить, не упомянуто ли в тексте что-то из заданного списка (географические названия, медицинские термины), или чтобы обогатить векторные представления (например, добавить к эмбеддингу слова дополнительные измерения, кодирующие его позицию в онтологии). Проекты типа ConceptNet, DBpedia и другие связные базы знаний фактически являются развитием той же идеи лексических ресурсов, но уже в рамках семантической паутины и Linked Data.

Подводя итог разделу: классические методы NLP – это совокупность *простых, интерпретируемых инструментов*, которые позволяют извлекать базовые признаки из текста (частоты слов, n-граммы), находить шаблоны (регэкспы, правила) и пользоваться накопленными знаниями (лексические словари). Они легки в понимании и реализации, а на небольших задачах часто работают “из коробки”. Однако по отдельности эти методы редко способны уловить всю сложность языка. Поэтому далее появились более мощные **обучаемые модели**, которые смогли на основе этих признаков и данных научиться решать сложные задачи NLP.

## Статистические и машинно-обучающиеся подходы в NLP

Переход от чисто ручных методов к обучаемым моделям стал поворотным моментом в NLP. Алгоритмы машинного обучения позволяют **обобщать из примеров**, автоматизируя разработку правил. В 2000-х годах в арсенале NLP широко использовались *классические методы ML*: Naive Bayes, SVM, максимальная энтропия (логистическая регрессия), решающие деревья и их ансамбли, а также модели для данных с последовательной структурой — скрытые марковские модели и условные случайные поля. Рассмотрим кратко эти методы и как они применялись к языку.

### Классификация текстов: наивный Байес, SVM и деревья решений

**Задача классификации текста** (например, определение тематики статьи или тональности отзыва) — одна из базовых в NLP. Решить ее можно, представляя текст в виде набора признаков (например, тех же TF‑IDF весов слов) и применяя алгоритм ML для классификации. Одним из самых ранних и простых алгоритмов был **наивный байесовский классификатор (Naive Bayes)**. Это *вероятностный метод*, который по теореме Байеса оценивает апостериорную вероятность класса документа на основе вероятностей наблюдать слова этого документа при данном классе. Он называется "наивным", потому что делает сильное предположение независимости признаков — считается, что наличие каждого слова в тексте независимо от наличия других, что конечно не совсем так в реальном языке. Несмотря на эту упрощенность, NB часто работает удивительно хорошо на практике. Его плюс — невероятная скорость и простота обучения: достаточно посчитать частоты слов в документах каждого класса. **Наивный Байес часто применяли для автоматической сортировки писем (спам/не спам)**, анализа тональности (положительный/отрицательный отзыв) и других задач категоризации текста. NB особенно хорош, когда признаков очень много (тысячи слов) и нужно устойчиво оценить вероятности – при больших размерностях и относительно небольших данных более сложные модели склонны к переобучению, а NB благодаря “наивности” ведет себя устойчиво. Однако если признаки сильно зависимы или требуется учесть их сочетания, наивный байес может уступать другим методам.

Другой популярный алгоритм – **машины опорных векторов (Support Vector Machines, SVM)**. SVM — это метод обучения линейного классификатора (или нелинейного через ядра), который ищет такую *гиперплоскость* в пространстве признаков, чтобы максимально разделить данные разных классов, оставляя между ними максимальный зазор. В контексте NLP SVM отлично себя показали в задачах классификации текстов и сообщений, особенно многоклассовых и многом меток. Они хорошо работают с разреженными высокомерными данными (как раз случай TF‑IDF векторов). В 2000-х годах SVM била рекорды точности на многих задачах — от категоризации новостей до определения эмоциональной окраски твитов. Например, для классификации отзывов как положительных/отрицательных линейная SVM с признаками-словами зачастую давала высокую точность и превосходила более простые методы. Её недостаток — относительная сложность обучения (квадратичный масштаб с количеством образцов, хотя ядровые SVM еще тяжелее) и меньше интерпретируемости. Но в эпоху до глубокого обучения SVM была одним из основных “рабочих коней” NLP, предлагая хороший баланс **точности и способности работать с большими признаковыми пространствами**.

**Решающие деревья** — еще один традиционный инструмент ML, иногда применявшийся и в NLP. *Дерево решений* рекурсивно делит пространство признаков, задавая вопросы вида «содержит ли текст слово X?», «есть ли слово Y больше N раз?» и т.д., и в листовых узлах выдает класс. Одно дерево обычно менее точно, чем SVM или NB, но его решения легче интерпретировать. В чистом виде деревья для текстов применялись редко (из-за очень высокой размерности признаков дерево становилось глубоким и склонным к переобучению). Но **ансамбли деревьев** — случайные леса (Random Forest) и градиентный бустинг — показывали себя неплохо. Например, в некоторых задачах анализа тональности ансамбли деревьев могли соперничать с линейными моделями. Их плюс в том, что они автоматически учитывают нелинейные комбинации признаков (фактически, разные сочетания слов). Минус — модели получаются громоздкими и менее удобными для обновления. Тем не менее, деревья нашли применение, например, в предсказании тональности отдельных аспектов отзыва (где признаками служат счетчики слов с учётом их расположения относительно упоминаемого аспекта). Также в early NLP встречались системы, где деревья решений обучались для лингвистических задач — скажем, **частеречной разметки** (POS-tagging) для неизвестных слов: есть исследование, где для определения части речи несловарного слова построено дерево решений, учитывающее его суффиксы, заглавную букву и соседей. В наше время деревья и их ансамбли скорее конкурируют с нейросетями в средних по размеру задачах — иногда они предпочтительнее, если данных мало, а признаков достаточно, так как могут обобщать без огромных вычислительных ресурсов.

В целом, для **классификации текста** (тематика, категория, тональность, авторство) на небольших текстовых коллекциях классические методы ML до сих пор актуальны. Они быстрые, не требуют гигантских мощностей и часто достигают удовлетворительной точности. Например, в статье 2019 года показывалось, что простая логистическая регрессия на словах + n-граммах может обойти некоторые сложные нейронные сети на задачах классификации, если грамотно настроить особенности (регуляризацию, отбор слов и т.п.). Поэтому и сегодня, прежде чем бросаться к BERT или GPT, практики часто пробуют **“простые и понятные” методы**, чтобы установить базовый уровень качества и интерпретировать важность признаков.

### Последовательности и разметка: HMM и CRF

Помимо классификации “целиком документов”, много задач NLP требуют **предсказать последовательность меток** для последовательности токенов. Примеры: **частеречная разметка (POS-tagging)**, где каждому слову приписывается грамматическая категория; **распознавание именованных сущностей (NER)**, где нужно выделить и пометить именованные сущности в тексте; **сегментация и разметка по ролям** в предложении и т.д. Для таких задач разработаны специальные последовательностные модели, наиболее известные из которых – **скрытая марковская модель (Hidden Markov Model, HMM)** и **модель условного случайного поля (Conditional Random Field, CRF)**.

**Скрытая марковская модель** — это статистическая модель, описывающая последовательность скрытых состояний, из которых на каждом шаге порождается наблюдаемое событие. В применении к языку классический пример HMM — модель для POS-теггинга. *Состояния* в ней соответствуют частям речи (например, существительное, глагол...), а наблюдения — сами слова. Модель хранит: 1) вероятности переходов между тегами (например, что после прилагательного с вероятностью 40% идет существительное, а с 5% — другое прилагательное и т.д.); 2) вероятности генерации слова каждым тегом (например, P("кошка"|существительное) высокая, а P("кошка"|глагол) практически 0). Обучив эти вероятности на размеченном корпусе (например, на большом объеме текстов с тегами Penn Treebank для английского), можно затем для новой последовательности слов найти *самую вероятную последовательность тегов* — классическая задача расшифровки, решаемая алгоритмом Витерби. HMM были основой многих систем в 80–90-х: **POS-теггеры**, **простейшие NER** (где состояния — метки типа B-PER, I-PER, O...), **модели биграмм и триграмм** для языка тоже можно рассматривать как частный случай HMM (скрытым состоянием является предыдущее слово). HMM привнесли идею учета **локальных зависимостей между соседними метками** и оказались эффективнее отдельных классификаторов на каждый токен. Они довольно быстрые, и их вероятностная интерпретация позволяет мягко справляться с неопределенностью. Однако HMM – *генеративная* модель, она делает сильное предположение о независимости наблюдаемых слов друг от друга (при условии состояний). Кроме того, реализуя связи только между близкими состояниями, HMM не учитывает сразу весь контекст слова.

Для преодоления ограничений HMM были предложены **Conditional Random Fields (условные случайные поля)** – это уже *дискриминативная* модель последовательности, которая напрямую оценивает вероятность меток, условную на наблюдаемой последовательности слов, без попытки моделировать сам процесс генерации слов. Проще говоря, CRF позволяет задавать произвольные *функции признаков* от слов и меток и учится оптимальным образом сочетать эти признаки для всей последовательности сразу (в отличие от HMM, где учитываются только переходы между метками и совпадение метки со словом). **CRF получили большое распространение в задачах NER и POS-tagging** с середины 2000-х, став “золотым стандартом” до эры нейросетей. Например, знаменитый Stanford NER (2005 г.) основан на CRF и учитывает такие признаки, как: текущее слово, его суффиксы, заглавная ли буква, находится ли слово в списке имен собственных, предыдущие и следующие слова, шаблоны цифр (для дат) и т.д. – все эти свойства включаются как характеристики, влияющие на метку *в контексте всего предложения*. CRF фактически решает глобальную задачу: выбирает цепочку меток, **максимизирующую правдоподобие всей последовательности** (в отличие от, например, независимого присвоения меток каждому слову). Алгоритм поддержки обычно напоминает HMM – тоже перебор последовательностей, но теперь веса переходов не обязательно вероятности, а результаты экспоненциальной модели, учитывающей комплекс признаков.

Преимущество CRF – они могут использовать **богатый произвольный набор признаков** слова/окружения (не только сам текст, но и внешние словари, результаты других классификаторов и т.п.), что сильно повышает качество. Классическая связка для NER долгие годы была: локальные признаки + CRF – и она давала точность, близкую к человеческой, на стандартных корпусах именованных сущностей. CRF превосходили HMM, поскольку не страдали от так называемой “проблемы смещения” (label bias) и позволяли учитывать перекрывающиеся признаки. Их недостаток – относительно сложная реализация и обучение (хотя для линейных цепочек все довольно эффективно). Впоследствии нейросети (например, BiLSTM+CRF, см. далее) отчасти заменили ручную инженерную работу по выбору признаков, но сам CRF-модуль до сих пор часто используется на выходе нейронных сетей для последовательностей, чтобы обеспечить глобально оптимальные предсказания меток.

Стоит упомянуть, что для некоторых задач применялись и другие модели последовательностей: например, **максимально энтропийные Марковские модели (MEMM)**, объединяющие идеи CRF и HMM, или **перцептрон Коллинза** для последовательностей. Также для частеречной разметки популярным подходом 90-х была *обучение преобразованиям (transformation-based learning)* — алгоритм, предложенный Эриком Бриллом. Он начинал с простого базового теггера (например, присваивал наиболее частотный тег каждому слову) и затем итеративно применял *правила корректировки* (например: “если текущее слово — глагол, а следующее – прилагательное, поменять тег текущего на существительное”). Эти правила автоматически обучались на данных. Метод Брилла интересен тем, что сочетал обучение и явно выраженные правила, читаемые человеком, но со временем уступил место CRF и нейросетям, которые показывали лучшую точность.

Резюмируя: **HMM и CRF были основными инструментами для задач разметки последовательностей в NLP** до 2010-х. HMM проще и интуитивнее, CRF точнее и гибче. Даже сегодня CRF остается в строю: например, во многих решениях для распознавания именованных сущностей используется нейросетевой слой для генерации признаков слов (векторов), а поверх него — CRF для окончательного предсказания меток с учетом связности (популярная архитектура “BiLSTM+CRF”). Таким образом, идеи этих классических моделей живут, интегрируясь в современные технологии.

## Нейросетевые подходы до трансформеров: RNN, LSTM, CNN, Attention

Следующий этап — **нейронные сети**, которые принесли в NLP качественно новый уровень возможностей. Хотя первые нейросетевые эксперименты с языком были еще в 90-х, по-настоящему глубокое обучение (deep learning) начало доминировать в NLP около 2013–2015 годов. Важно отметить, что речь пока не о гигантских моделях вроде GPT, а о **специализированных нейросетевых архитектурах**, обучаемых под конкретные задачи, зачастую со сравнительно небольшим числом слоев и параметров (сегодня их можно назвать "небольшими" относительно LLM). Расскажем о ключевых архитектурах: **рекуррентных нейросетях (RNN)**, их усовершенствованных версиях GRU/LSTM, **свёрточных сетях (CNN)** для текста, а также о механизме **внимания (attention)**, который предвосхитил трансформеры.

### Рекуррентные нейронные сети (RNN) и «долгая краткосрочная память»

Язык представляет собой последовательность, где порядок слов имеет первостепенное значение. Обычные многослойные перцептроны (полносвязные сети) не учитывают порядка — если просто подать BoW-вектор на вход MLP, он не узнает, какое слово за каким шло. **Рекуррентные нейронные сети (RNN)** были придуманы как архитектура, способная *итеративно обрабатывать последовательность* и запоминать информацию о прошлых шагах за счет *внутреннего состояния (hidden state)*. Идея: мы последовательно проходим по словам предложения; на каждом шаге RNN берет текущий вход (например, вектор слова) и предыдущего скрытого состояния, и выдает новое скрытое состояние и (опционально) выход. Это скрытое состояние и выступает в роли “памяти” о предыдущих элементах. Таким образом, RNN может условно “помнить”, что было до текущего слова. В 1990-х были предложены первые простые RNN (типа сетей Элмана и Джордана). Однако при обучении методом backpropagation through time они столкнулись с проблемой **затухающих градиентов** – вклад дальних шагов в ошибку экспоненциально убывал при распространении градиента назад, из-за чего **длинные зависимости плохо усваивались**. Практически это означало, что стандартная RNN хорошо помнит 2–3 последних слова, но почти не учитывает контекст в 10 шагах назад. Для языка, где предложения могут быть длинными, это серьезное ограничение. Решением стал специальный вид RNN – **LSTM (Long Short-Term Memory)**, предложенный Хохрайтером и Шмидхубером (1997).

**LSTM** вводит более сложный нейронный узел со встроенными *элементами памяти (ячейкой)* и *механизмами управления (гейтами)*, которые решают, какую информацию записывать в память, что стирать и что выдавать наружу. Говоря проще, LSTM может **сохранять градиент** на многих шагах, позволяя информации протекать через длинную цепочку без затухания. Благодаря этому LSTM удалось существенно увеличить “горизонт памяти” сети. Например, LSTM способна в тексте помнить упоминание имени персонажа в начале абзаца, чтобы правильно интерпретировать местоимение "он" ближе к концу абзаца — то, с чем обычная RNN не справилась бы. В 2000-х LSTM понемногу применялись в задачах речи, но настоящий бум их использования в NLP произошел уже после 2013, когда были накоплены большие корпуса для обучения. LSTM стали основой для множества моделей: **двунаправленный LSTM (BiLSTM)** для теггинга и NER (два LSTM проходят по тексту слева-направо и справа-налево, объединяя контекст с двух сторон), **seq2seq с LSTM** для машинного перевода (отдельный LSTM кодирует входное предложение, другой LSTM-декодер генерирует перевод по скрытому представлению) и т.д. Были также предложены упрощенные варианты, например **GRU (Gated Recurrent Unit)** – более легкий вариант LSTM с меньшим числом параметров (объединяет некоторые гейты). GRU популярны там, где требуются более простые модели с почти LSTM-производительностью.

В сумме, **рекуррентные сети научили модели “понимать контекст” и последовательность**. До трансформеров они были основным средством моделирования последовательных данных. Ограничения у них тоже есть: RNN не параллелизуются по последовательности (нельзя обработать все слова одновременно, нужно по порядку), поэтому обучение на длинных секвенциях медленное. К тому же, хотя LSTM справляются лучше с дальними зависимостями, но на очень длинных текстах (100+ слов) и они начинают “забывать”. Тем не менее, вплоть до конца 2010-х BiLSTM-CRF для NER и POS считались state-of-the-art, а seq2seq на LSTM с вниманием доминировали в машинном переводе. Сегодня трансформеры вытеснили RNN из топовых достижений, но понимать принципы RNN важно: многие идеи (типа **“передача состояния вперед”**) сохранились и в новых моделях (в трансформерах роль передачи информации выполняет механизм внимания).

### Свёрточные нейросети (CNN) для текстовых данных

Свёрточные сети (Convolutional Neural Networks, CNN) широко известны своими успехами в обработке изображений, но они нашли применение и в NLP. **CNN для текста** используются главным образом для задач классификации и извлечения локальных признаков. Принцип: рассматривать входной текст как последовательность эмбеддингов слов (матрица размером *длина × размер\_эмбеддинга*), и применять к нему свёрточные фильтры различной ширины, которые будут реагировать на определенные комбинации соседних слов. Например, фильтр ширины 2 обучится детектировать биграмму вроде “не нравится”, а фильтр ширины 3 – фразу вроде “очень не нравится”. Затем берутся максимумы активаций (max-pooling) по всей длине текста, извлекая наиболее сильные срабатывания признака. В результате формируется фиксированный набор фич, которые далее подаются на полносвязный слой для классификации.

Первым, кто систематически исследовал CNN для NLP, был Юн Ким (2014): в статье “Convolutional Neural Networks for Sentence Classification” он показал, что даже **простая однослойная CNN на словах** может дать отличные результаты в классификации предложений по тональности, теме и т.д.. Например, на задаче определения положительный/отрицательный отзыв – CNN с эмбеддингами (инициализированными word2vec) смогла превзойти некоторые сложные до того модели. Интуитивно, **свёртки работают как “детекторы ключевых фраз”**: один фильтр может искать отрицательные конструкции (“no good”, “не рекомендую”), другой – положительные эпитеты (“очень понравилось”), третий – указание на тематику (“графика игры” для отзывов о видеоиграх). После обучения мы получаем n признаков – насколько сильно встретились те или иные n-граммы – и на их основе уже легко принять решение. CNN оказались эффективны, потому что: 1) они учитывают локальный порядок слов (в окне фильтра), 2) они *инвариантны* к положению в тексте (фраза “отличное обслуживание” важна независимо от того, в начале отзыва или конце – max-pooling учтет её в любом случае), 3) у них меньше параметров, чем у RNN для сопоставимой задачи (свёрточные фильтры разделяют веса по всей длине).

Помимо классификации, **CNN применяли и в задачах последовательной разметки** – например, для NER использовали CNN для извлечения признаков слов (символьная CNN по буквам слова позволяет выучить признаки, например, заглавная буква или суффикс, в непрерывном виде). До популяризации трансформеров, библиотека spaCy, например, реализовала свой теггер и NER именно на базе CNN: они обучили CNN, которая сканирует окно слов и на выходе предсказывает метку для центра окна (подход, похожий на sliding window классификатор). Это давало впечатляющую скорость работы, так как свёртки легко параллелятся, и качество, сопоставимое с BiLSTM. **Таким образом, CNN стали альтернативой RNN для выделения локальных зависимостей в тексте**.

Конечно, у CNN тоже есть пределы: они хорошо ловят локальные шаблоны, но без дополнительных ухищрений (типа увеличения рецептивного поля несколькими слоями или dilated convolutions) им трудно моделировать очень дальние отношения. К тому же, текст не обладает строгой двумерной локальной структурой как изображения, так что сверхглубокие CNN в NLP применялись редко. Обычно это 1–3 слоя сверток с разными ширинами фильтров. Тем не менее, **CNN-подходы до сих пор актуальны**, особенно когда важна скорость и простота. Например, Facebook в 2016 г. выпустил библиотеку fastText, в которой для классификации документов используется упрощенный вариант сверток — усреднение эмбеддингов слов (что эквивалентно свертке ширины = длина). **fastText** способен обучаться на миллионы текстов за считанные минуты и показывает качество, близкое к более тяжелым моделям. Он не учитывает порядок (кроме разве что усреднения биграмм), но за счет огромных данных все равно ловит основной сигнал. В некоторых приложениях, где требуется *надежность и скорость* без больших вычислений (например, фильтрация спама на устройстве, классификация обзоров в реальном времени), свёрточные и простые модели остаются привлекательными.

### Механизм внимания (Attention) в доконкурсных моделях

К середине 2010-х в нейросетевом NLP назрела потребность лучше работать с длинными последовательностями и **фокусировать “внимание” модели на важных фрагментах входа**. Классический рекуррентный seq2seq (например, машина перевода) кодировал всё предложение источника в один вектор фиксированной длины, из которого декодировал перевод. Это создавало *информационное бутылочное горлышко*: длинное предложение ужималось в один вектор, неминуемо теряя детали. Решение пришло в 2014 году с работой Баданау и соавт. “Neural Machine Translation by Jointly Learning to Align and Translate”. Они предложили добавлять к seq2seq-модели **механизм внимания (attention)**. Суть: декодер при генерации каждого слова перевода *динамически смотрит* на все скрытые состояния энкодера (то есть на представления каждого входного слова) и вычисляет веса важности — какие слова оригинала наиболее релевантны текущему генерируемому слову. Например, при переводе английского предложения на русский, когда декодер собирается вывести слово “дом”, механизм внимания может показать высокие веса у английского слова "house" и, скажем, прилагательного перед ним, уточняющего размер. Таким образом, **модель учится выравнивать (align) части исходного и перевода**, избавляя энкодер от необходимости кодировать *всё* предложение целиком в одном векторе. Практически реализация attention сводится к вычислению скалярных оценок соответствия между текущим состоянием декодера и каждым состоянием энкодера, нормировке их через softmax в коэффициенты, и вычислению взвешенной суммы состояний энкодера — полученный контекстный вектор и используется для генерации следующего слова декодером.

**Внимание совершило революцию** в нейропереводе: качество перевода резко выросло, особенно на длинных предложениях. Стало возможным переводить предложения длиной 50+ слов без сильной деградации качества, в то время как ранние seq2seq с LSTM справлялись куда хуже. После успехов в переводе, attention добавили и в другие задачи: резюмирование текста, генерация описаний к изображениям (там attention позволял “смотреть” на разные части изображения при описании разных слов), ответ на вопросы (модель внимания выделяла предложения в тексте, содержащие ответ).

Важно отметить, что **первоначальный механизм внимания работал в паре с рекуррентной архитектурой** (энкодером и декодером на LSTM). Attention того периода — это не отдельная модель, а *подмодуль*, улучшающий RNN. Уже в 2016–2017 были идеи заменить рекуррентную часть полностью самовниманием (self-attention), что и произошло с появлением трансформера. Но даже без этого, attention расширил возможности тогдашних моделей. Например, появились архитектуры типа **Memory Networks**, где механизм внимания позволял “выбирать” релевантные предложения из памяти (списка фактов) для ответа на запрос. Это по сути прообраз retrieval-augmented систем.

Математически, внимание можно рассматривать как learnable поиск по ключам и значениями — декодер выдает “запрос” (query), энкодер предоставляет “ключи” (keys) и “значения” (values) – где keys служат для вычисления веса (насколько query похож на key), а value содержат информацию (часто key=value=скрытое состояние). Такой взгляд заложил основу **самовнимания** в трансформерах, где каждый элемент последовательности играет роль query, key, value для всех других.

Итак, **attention-механизм позволил моделям фокусироваться на нужном контексте адаптивно**. На практике это привело к более грамматически и содержательно правильным результатам в генеративных задачах. Например, ранние переводчики на RNN могли ошибаться в согласовании подлежащего и сказуемого по родам и числам, если предложение длинное и вектор состояния “путается”. Attention же практически устранил эту проблему, потому что при генерации глагола модель явно “смотрит” на подлежащее в оригинале и учитывает его род/число. Это *свободно избавило модель от фиксированного размера контекстного вектора*, что «имело большой позитивный эффект на способности системы\*\*. Механизм внимания в комбинации с LSTM использовался вплоть до конца 2010-х. Например, Google’s Neural Machine Translation (GNMT) в 2016 г. применяла 8-слойные LSTM + attention для перевода на продакшене. Лишь спустя несколько лет их заменили на трансформеры, но attention как идея остался в основе.

Подводя итог секции: до появления трансформеров основной прогресс в нейросетевом NLP шёл через *специализированные архитектуры* — рекуррентные и свёрточные сети, улучшенные механизмом внимания. Они значительно подняли планку качества во многих задачах, оставаясь при этом относительно узко заточенными под задачу и умеренными по размеру (считались большими по меркам 2010-х, но малы по меркам 2020-х). Переход к трансформерам и LLM, которые обучаются на громадных объемах данных *без непосредственного наставления под конкретную задачу*, стал следующим шагом. Но важно отметить: **нейросети “первой волны” в NLP (RNN/CNN) до сих пор применяются, особенно когда данные или вычисления ограничены.** Например, если нужно обучить модель на собственном небольшом корпусе (несколько тысяч образцов) и нет доступа к мощным GPU, тренировать BERT с нуля нецелесообразно — гораздо проще обучить небольшую LSTM или даже логрегрессию, и часто результат будет приемлемым. Кроме того, многие идеи (би-двунаправленность, внимание, векторные эмбеддинги) перекочевали из этих ранних разработок в современные модели.

## Современные подходы 2023–2025: маленькие модели, гибридные и символические системы

В настоящее время большие языковые модели доминируют во внимании СМИ, однако *не все задачи и проекты требуют или могут использовать LLM*. Продолжается развитие и применение подходов, не основанных на огромных параметрах. Рассмотрим несколько современных направлений NLP **без использования LLM**:

* **Малые и узкоспециализированные модели.** Не всегда доступен ресурс для разворачивания громоздких моделей вроде GPT-3, и не всегда это нужно. Во многих прикладных случаях достаточно модели с десятками или сотнями миллионов параметров (а то и меньше). Например, **сжатые версии трансформеров** – такие как DistilBERT, TinyBERT – предлагают значительно меньший размер и быстродействие, сохраняя большую долю точности от больших собратьев. Также используют модели на основе архитектур “pre-LLM” – BiLSTM, CNN, или относительно небольшие трансформеры вроде BERT-base (110M параметров) для задач классификации, NER и т.д. К примеру, для классификации отзывов о конкретном продукте можно fine-tune обычный BERT или даже обучить с нуля небольшой двунаправленный LSTM – и получить высокую точность, особенно если данные однородны. **Важный тренд – оптимизация и сжатие моделей**: knowledge distillation (выдержка знаний большой модели в меньшую), квантование (снижение разрядности весов), pruning (отсечение ненужных нейронов). Всё это позволяет использовать NLP-модели на мобильных устройствах и в embedded-системах. Компания Meta (Facebook) в своё время представила библиотеку **fastText**, которая предлагает **мгновенное обучение текстовых классификаторов** с низкими требованиями и отличной скоростью – её модель может весить килобыты и при этом показывать достойные результаты. Это реализовано за счет упрощенной архитектуры (близкой к логистической регрессии на усредненных эмбеддингах) и эффективной реализации. В 2023 году, когда встаёт вопрос об экономии электроэнергии и приватности, такие легковесные модели вновь актуальны. Пользовательские данные (например, переписка на телефоне) можно обрабатывать локально **офлайн небольшими моделями**, без отправки в облако – подход, известный как *on-device AI*.

* **Гибридные системы (нейро-символические методы).** Наблюдается возрождение интереса к сочетанию нейросетевых методов с символическими правилами и знанием. Дело в том, что чисто нейронные модели (включая LLM) страдают от таких проблем, как **галлюцинации, неспособность рассуждать логически, уязвимость к незначительным изменениям формулировок**. Поэтому исследователи и инженеры интегрируют символические компоненты для придания системе структурной устойчивости и знаний. Например, в задаче извлечения отношений из текста современный подход может выглядеть так: сначала **правил\_based модуль** отфильтровывает предложения, где вероятно описывается нужное отношение (по ключевым словосочетаниям), затем **ML-модель** (например, небольшой классификатор или нейросеть) уже точно классифицирует отношение и аргументы. Такой pipeline совмещает точность (благодаря ML на сложных случаях) и надежность (правила отсекают заведомо неподходящее). В других системах правила используются *после* нейросети – например, нейросетевой NER определил сущности, а затем символический модуль проверяет согласованность: если модель выделила “Василий” как ORGANIZATION, а “Газпром” как PERSON, правила могут это исправить (очевидно, наоборот). **Нейросимволические подходы** активно исследуются и показывают, что комбинируя знание (символическое) и обучение (нейронное), можно добиться лучших результатов, чем каждым по отдельности. Сurvey 2023 года отмечает, что такие гибридные решения выигрывают в четырех аспектах: интерпретируемость, обобщение на малых данных, устойчивость к выходу за распределение и возможность восстановления от ошибок. Пример – система анализа медицинских текстов, где нейросеть извлекает потенциальные факты, а затем с помощью базы знаний (медицинской онтологии) проверяется, не противоречит ли факт известным биологическим законам. Если противоречит, модель может быть уведомлена об ошибке и скорректировать вывод. Таким образом, **символические знания вводят “здравый смысл” и проверяемость** в конвейер NLP, что особенно важно в областях с высокой ценой ошибки (медицина, право). Многие крупные фирмы (IBM, Microsoft) вкладываются в исследования по **Neuro-Symbolic AI**. Например, подход IBM Neurologic Decoding позволяет использовать логические ограничения на вывод текста нейросетью.

* **Использование внешних знаний и баз данных.** Смежно с гибридными системами развивается направление **IR + NLP** (Information Retrieval + NLP). Вместо того чтобы заставлять один огромный черный ящик знать все факты, систему делают составной: есть модуль поиска по базе знаний или по интернету, который на запрос находит релевантную информацию, и есть модуль *понимания*, который эту информацию обрабатывает. Такой подход популярен, например, для **советчиков и чат-ботов, не основанных на LLM**. В 2023 году появились open-source проекты чат-ботов, которые вместо генерации ответа из параметров модели **генерируют SQL-запрос к базе** (специально сформулированной knowledge base) или **выполняют поиск по документации**, а затем формируют ответ на основе найденных данных. Это гарантирует, что ответ будет основан на проверенной информации, а не на “галлюцинации”. В меньшем масштабе, даже до LLM, существовали системы **FAQ-ботов**: запрос пользователя сравнивался с базой часто задаваемых вопросов с помощью векторных эмбеддингов (например, doc2vec), и затем возвращался заранее заготовленный ответ для наиболее близкого вопроса. Это тоже способ получить “разумное” поведение, не обучая огромную модель говорить на любую тему – вместо этого используется простое сравнение в векторном пространстве с последующим вытаскиванием шаблонного ответа. В 2023–2025 годах такие решения развиваются под термином *Retrieval-Augmented Generation* (RAG) и *Q/A с поиском*. Но если убрать генерацию, останется просто **семантический поиск и извлечение** – что часто достаточно для бизнес-задач.

* **Автоматизированные и узкие методы, встроенные в продукты.** Современные NLP-инструменты (особенно коммерческие) часто используют комбинацию небольших нейронных моделей и эвристик вместо одной огромной. Например, популярная библиотека **spaCy** изначально (в версиях 1 и 2) включала собственные модели для POS и NER на основе CNN, которые работали очень быстро и весили считанные мегабайты, а точность на английском приближалась к SOTA. То есть разработчики выбрали путь **оптимизации под практику**, а не максимум точности любой ценой. До сих пор spaCy предоставляет два типа моделей: быстрые небольшие (`en_core_web_sm` \~15МБ) и более тяжелые с трансформером (`en_core_web_trf` \~300МБ). Пользователь может сам выбирать, что важнее – скорость или несколько процентов качества. Многие другие утилиты, например **NLTK**, **Stanza**, **Flair**, предлагают облегченные варианты. **Flair** вообще интересен тем, что он предоставляет интерфейс для сочетания разных эмбеддингов: можно использовать не BERT, а, скажем, GloVe + character-LSTM (векторное представление слова + контекст из символьной модели) и получить некое улучшение по сравнению с одним GloVe. **Stanza** (Stanford NLP) – это глубокие BiLSTM модели для различных языков, предобученные на корпусах Universal Dependencies, они не такие большие и вполне точные для лингвистической разметки. Иными словами, сейчас наблюдается сосуществование двух парадигм: 1) *“огромная универсальная модель”* (ее примеры – GPT-4, PaLM и т.п.), 2) *“зоопарк маленьких, но умелых моделей”*. И второй вариант часто выигрывает в реальных проектах, где важна простота развертывания и контроль.

* **Этичность, приватность и кастомизация.** Современные требования к приватности данных подталкивают к использованию не “Software as a Service” (API LLM в облаке), а локальных решений. Например, в банковской или медицинской организации данные не могут отправляться на внешние сервера для анализа языковой моделью. Поэтому приходится строить решения на основе внутренних инструментов: пусть они будут проще, но все данные остаются внутри. Тут снова выручают проверенные методы: можно обучить на своих текстах небольшую модель классификации или извлечения, использовать ключевые слова, регулярки, и обеспечить нужный функционал без нарушения регламентов. Кроме того, большие модели зачастую **нередактируемы и нечувствительны к специфике домена**: если у вас специфическая лексика (например, юридические термины), огромная модель может ошибаться, и вы не сможете легко ее доучить (fine-tuning LLM — отдельная сложная задача). Маленькие же модели можно относительно легко дообучить или даже написать часть правил вручную, добившись идеального отражения требований на вашем корпусе. Это приводит к тому, что *классические и малые методы переживают второе рождение*, когда дело касается узкоспециализированных приложений.

Подытоживая: **“не-LLM” NLP 2023–2025 годов – это комбинация старого и нового**. С одной стороны, используются достижения последних десятилетий (эмбеддинги, BiLSTM, CNN, трансформеры), но в уменьшенном или специальном формате. С другой, активно возвращаются идеи символического ИИ и интеграции знаний, чтобы компенсировать слабости нейросетей. Многие эксперты считают, что будущее — за гибридными системами: большие модели будут отвечать за “понимание” и генерацию, а символические компоненты — за проверку, фактическую точность и логику. Уже сейчас есть примеры, где **символический слой оборачивает нейросеть**, как “правила по охране” (guardrails) — например, при генерации кода ИИ может сначала составить программу, а потом правило проверит, компилируется ли она, и если нет, попросит ИИ исправить. Точно так же и в NLP: нейросеть может найти ответы, но логический модуль проверит непротиворечивость ответа входу. Таким образом, **тренды “non-LLM” NLP** – это эффективность, интеграция знаний, интерпретируемость и приватность.

## Применение методов NLP без LLM: задачи и решения

Рассмотрим теперь, как описанные методы используются в конкретных прикладных задачах NLP. Мы пройдемся по списку наиболее распространенных задач и укажем, какие подходы (без привлечения больших моделей) для них historically и сейчас применяются.

### Распознавание именованных сущностей (NER)

**Задача NER (Named Entity Recognition)** – находить в тексте упоминания определенных типов сущностей, таких как Персоны (PERSON), Организации (ORG), Локации (LOC) и др., и классифицировать эти упоминания по типам. Пример: в предложении *“Компания Apple выпустила новый iPhone в Калифорнии”* нужно выделить “Apple” как ORG, “iPhone” как продукт (если задан такой тип) и “Калифорнии” как LOC.

**Классические методы NER** включают:

* *Правила и словари:* На ранних этапах NER часто делали с помощью списков имен (т.н. **gazetteers**) и шаблонов. Например, можно иметь список всех стран и просто искать их в тексте, помечая как LOC. Шаблоны (регэкспы) помогали ловить, скажем, форматы названий организаций (если в тексте слово заканчивается на “Inc.” или “Ltd.”, то высока вероятность, что это организация). Такие системы писались вручную и могли достичь неплохого охвата в ограниченной области (например, список лекарств, список городов и пр.). Недостаток в том, что за новым данными списки надо дополнять, а уникальные имена (фамилии людей, названия стартапов) они могут не узнать.

* *HMM и модель наивного Байеса:* Ранние статистические NER (конец 90-х) использовали HMM, подобные тем, что применялись для POS-теггинга. Состояния HMM представляли метки BIO для сущностей (B-начало сущности, I-внутри, O-вне сущности). Обучая вероятности переходов (например, что после B-PER может идти I-PER, но не B-LOC) и эмиссий (какие слова характерны для имен людей, какие для локаций), можно было распознавать сущности. Однако HMM не учитывают хорошо произвольные информативные признаки (например, наличие заглавной буквы или суффиксов), поэтому их точность была ограничена.

* *Conditional Random Fields (CRF):* Как уже говорилось, CRF стали очень успешным подходом к NER. С помощью CRF-инструментария (например,### Распознавание именованных сущностей (NER)
  **Задача NER (Named Entity Recognition)** – находить в тексте упоминания определенных типов сущностей (имена людей, названия организаций, географические объекты, даты и т.п.) и помечать их категориями. Примеры: *“Барак Обама” – PERSON, “Google” – ORG, “10 декабря 2023 года” – DATE*. Традиционно без LLM NER решали следующими методами:

* **Правила и словари.** Ранние NER-системы во многом базировались на эвристиках. Например, можно иметь список стран, городов, имен собственных (gazetteer) и простым поиском отмечать их в тексте как геолокации или имена. Также вводились шаблоны: последовательность “Title. Name Lastname” (например, “Dr. John Smith”) почти наверняка PERSON; слова, следующие за “г-н” или “Mr.”, тоже вероятно имя человека; слова в кавычках после “фильм” – название фильма, и т.д. Такие подходы легки для точечных случаев и не требуют обучения. **Недостатки**: ручная работа по сбору и обновлению списков, плохая переносимость на другой домен, неустойчивость к вариациям (например, если в списке нет формы “США” вместо “Соединённые Штаты Америки”, система её не найдет). Тем не менее, *правила всё еще используются* в гибридных NER: современные системы могут сначала запустить ML-модель, а потом отфильтровать или скорректировать ее вывод правилами (например, убедиться, что сокращение “США” помечено как страна, даже если модель пропустила его).

* **Статистические модели (HMM/CRF).** С конца 90-х NER решается как задача **разметки последовательности** (см. предыдущий раздел): каждому слову присваивается метка O (не часть сущности) или B-/I- для начала/внутри сущностей разных типов. Классический подход – **условное случайное поле (CRF)**, обученное на размеченных текстах. Признаки для CRF включают: текущее слово (и его приведение к нижнему регистру), есть ли заглавные буквы, содержит ли цифры, входит ли в справочник имен, контекстные слова вокруг, части речи и т.д. Хорошо настроенный CRF с такими признаками **может достигать высокой точности**. Например, **Stanford NER** (2005) – модель на CRF, доступная с предобученными весами для английского, китайского и др. – долгое время считалась эталоном (F1 \~86-90% на новостных текстах для классов PER, ORG, LOC). Альтернативно применяли **максимальную энтропию (MaxEnt)** – фактически тоже логистическую модель для последовательностей (аналог MEMM), но CRF обычно превосходила ее по качеству благодаря учету глобальной последовательности. **HMM** для NER использовали реже (они уступали по гибкости), но в ранних работах тоже встречались. Сегодня CRF все еще актуален: если данных мало или нужна объяснимость, CRF-модель – хороший выбор. К тому же, она довольно быстрая и легкая, особенно по сравнению с трансформерами.

* **Нейросетевые модели малой и средней сложности.** До повсеместного внедрения BERT, лучшие NER делались на основе **BiLSTM + CRF**. Архитектура: берется *пословный эмбеддинг* (сложение, например, pre-trained word2vec или GloVe + обучаемого символного LSTM для каждого слова, чтобы учитывать морфологию), затем пропускается через двунаправленный LSTM, а выходные векторы подаются на слой CRF, который предсказывает метки. Такая модель может весить всего несколько десятков мегабайт, но показывать качество, близкое к трансформерам на многих языках. Например, библиотека **Flair** от Zalando предоставляет предобученные NER на основе BiLSTM-CRF (с использованием их stack-embeddings подхода: объединение GloVe, BERT-подобных эмбеддингов и т.д.), которые достигают **F1 \~93%** на стандартном английском наборе CoNLL-2003, что сопоставимо с BERT’ом. Важное преимущество BiLSTM-CRF – они **не требуют огромных вычислений**, их можно обучить с нуля на разметке из нескольких тысяч предложений за часы, а работать они будут в режиме реального времени.

* **Свёрточные сети (CNN)** также применяли для NER, хотя и реже. Например, spaCy изначально (до версии 3) использовал CNN для NER: несколько сверточных слоев бегали по последовательности токенов, формируя контекстные представления, а затем классифицировали токены на категории сущностей. Такая модель **работает очень быстро** (spaCy NER v2 обрабатывал тысячи токенов в секунду на CPU), но ей чуть не хватало точности до уровня BiLSTM – spaCy впоследствии перешел на трансформеры. Однако, CNN-подходи остаются хороши, если нужна **скорость без GPU**.

Современные NER-системы часто комбинируют подходы. Например, решение может использовать предобученный небольшой трансформер (типа MiniLM) для эмбеддингов слов, затем CRF для меток, а поверх – правила для постобработки. Или другой вариант: **“NER по щелчку”** (zero-shot или few-shot), когда у вас нет обученной модели на определенный тип сущностей, но вы хотите извлечь, скажем, названия болезней из медтекстов. Можно воспользоваться словарем (списком болезней) для первичного извлечения, а затем натренировать небольшой классификатор, который отличает реальное упоминание болезни от случайного совпадения (например, слово “медуза” может быть частью названия синдрома, а может быть просто животным). Такой двухшаговый pipeline сочетает *легкость словарного поиска* и *адаптивность обучения*.

**Вывод**: Без LLM NER успешно решается методами статистики и небольших нейросетей. Грамотно настроенный CRF или BiLSTM на доменных данных может дать отличные результаты. Преимущество такого решения – его можно разбирать и улучшать: добавить новое правило или новые обучающие примеры. В то время как LLM (например, использовать ChatGPT для NER) – это менее контролируемый вариант, и для надежных корпоративных приложений часто предпочитают старые, проверенные методы.

### Частеречная разметка (POS-tagging) и синтаксический анализ

**POS-tagging** – присвоение каждой словоформе в тексте ее части речи и грамматических характеристик (например, “ранний” – прилагательное, “бежит” – глагол, 3 лицо, ед. ч.). **Синтаксический анализ** – построение древовидной структуры предложения, определение связей между словами (подлежащее, сказуемое, объект, определение и т.д.).

До глубокого обучения эти задачи решались в рамках статистической лингвистики:

* POS-теггинг: скрытые марковские модели (например, **теггер Брауна** 1992 г.), условные случайные поля (теггер **Stanford TnT**), Perceptron-теггеры (алгоритм Коллинза). В русском языке известен **теггер Mystem** (Яндекс) – комбинация словаря и статистики.
* Синтаксический парсинг: широко применялись **контекстно-свободные грамматики с вероятностями (PCFG)** и их расширения. Например, **парсер Collins** (1997) и **парсер Charniak** (2000) – знаменитые статистические парсеры английского. Они обучались на размеченных древьями корпусах (Treebank) и могли с некоторой точностью прогнозировать вероятностную структуру нового предложения. Также развивался **депенденс-парсинг**: алгоритмы, которые строят не древо разбора по фразам, а прямые зависимости между словами. Здесь успех имели жадные и глобальные модели на основе SVM, перцептрона (парсер Мартиноса, парсер Макдональда).

Пример: *Статистический парсер Stanford (Klein & Manning, 2003)* – основан на PCFG, дообученной хитрыми факторами (так называемый unlexicalized PCFG, использующий грамматические категории). Он был довольно быстрый, но точность около 85% по метрике точного совпадения деревьев.

**Нейронные сети** улучшили парсинг в 2010-х: сначала вводом **эмбеддингов слов** (каждое слово – как вектор), затем полносвязными нейросетями для оценки вероятностей правил. Перед самым появлением трансформеров *state-of-the-art* парсеры были на основе BiLSTM: т.н. **parsing as sequence labeling** или **stack-LSTM** (Д. Бэррет, 2016). Например, широко известен **UDPipe** (парсер универсальных зависимостей) – он использует BiLSTM для одновременно POS-теггинга и построения зависимостей, и показывает очень высокую точность на десятках языков.

Сейчас, если нужно построить синтаксическое дерево без LLM, у разработчика много опций:

* Взять готовый инструмент: **Stanford CoreNLP**, **spaCy**, **Stanza**, **NLTK** – они имеют предобученные POS-теггеры и парсеры. Например, **Stanza** (Stanford NLP 2020) – чисто нейросетевой пайплайн (BiLSTM с вниманием), покрывающий 60+ языков. Он совсем небольшой: модель для каждого языка \~30МБ, зато обладает точностью на уровне лучших результатов в своих категориях. **spaCy** предлагает быстрый теггер (CNN) и парсер (на основе transition-based парсинга с нейросетевой моделью). **NLTK** включает более старые модели, например, теггер на счётчиках или UnigramTagger (простейший, выбирает для слова наиболее частотный тег из тренировочных данных).
* Обучить свою модель: если у вас есть размеченные данные (например, собственные тексты с тегами), можно обучить CRF-теггер или даже маленькую нейросеть. Библиотеки вроде **flair** или **scikit-learn-crfsuite** позволяют быстро настраивать такие модели.
* Использовать **правила**: для простых случаев, POS-теги могут определяться примитивными правилами по окончанию (например, слова на “-ly” в английском обычно наречия, на “-ed” – вероятно причастия/глаголы прош. времени). В 60-е годы, когда размеченных данных не было, POS-разметку делали *правилами вероятностей*, но сейчас это скорее учебный эксперимент. Для парсинга существуют **ручные грамматики** (например, грамматики зависимостей для чат-бота, где явно описывается, какие конструкции допускать), но они требуют лингвистической экспертизы и много усилий.

**Применимость**: POS-теггинг часто используется как вспомогательный этап – например, для лемматизации (разные части речи имеют разные леммы), для фильтрации слов (можно убрать все артикли/предлоги по тегам), для подсчета статистики (сколько в тексте существительных vs глаголов – признак стиля). Парсинг применяется в извлечении информации: зная синтаксис предложения, легче извлечь кто что сделал с кем (отношение субъект–объект), построить knowledge graph или преобразовать вопрос в логический запрос.

Стоит отметить: **в эпоху LLM** люди меньше пользуются отдельными парсерами, так как большие модели сами *имплицитно* понимают структуру. Но если LLM не доступен или нужна четкая структура, парсер – лучший выбор. Например, для языков с богатой морфологией (русский, польский) качественные POS и парсинг (например, Udpipe) – must have, чтобы дальше проводить семантический анализ.

### Извлечение отношений и информации

**Извлечение отношений (relation extraction)** – определение семантических связей между сущностями в тексте. Например, из предложения *“Илон Маск основал компанию SpaceX”* нужно извлечь отношение Founder(Илон Маск, SpaceX). Это часть широкой задачи **Information Extraction (IE)** – автоматического извлечения структурированных фактов из неструктурированного текста.

Классические методы:

* **Rule-based (шаблоны):** Ранние системы IE, такие как FASTUS (1995), были почти полностью основаны на шаблонах и регулярках. Например, для отношения “основал(PERSON, ORG)” можно прописать шаблон “X основал Y” или “основатель Y – X” и т.д. Такие шаблоны можно с помощью регэкспов или поверх результатов парсера. **Плюсы**: высокая точность, если шаблон сработал, и понятность. **Минусы**: покрытие (нужно очень много шаблонов для всех вариантов изложения факта), трудоемкость разработки. Тем не менее, в конкретных нишах (например, извлечение “Пациент страдает болезнью X” из медицинских заключений) шаблоны работают превосходно, поскольку язык там стандартизирован.

* **Супервизенное обучение:** Подход «как классификацию пары сущностей». Если у вас есть размеченный корпус, где для каждого упоминания пары сущностей отмечено, есть между ними определенное отношение или нет, можно обучить модель (SVM, решающее дерево, нейросеть) предсказывать отношение по **признакам контекста**. Признаки обычно берут из синтаксического парсера: слова между сущностями, путь по дереву зависимостей между ними, их POS-теги, типы сущностей и т.д. Например, для “Маск–основал–SpaceX” парсер дает связь nsubj(основал, Маск), obj(основал, SpaceX), из этого можно составить шаблон зависимостей который явный для отношения основатель. Классификатор научится отличать его от, скажем, “Маск купил акции SpaceX” – другой тип отношения. В 2000-х много работало **SVM с хитрыми ядрами по деревьям зависимостей** для relation extraction. Это позволяло достичь неплохих результатов (F1 60-70% на открытых задачах типа семинари TAC). Однако требовало разметки, которую нелегко получить в больших объемах.

* **Unsupervised/OpenIE:** Интересный парадигма – **Open Information Extraction** (OpenIE), где система пытается извлечь *все возможные* (обозначенные словесно) отношения без заранее заданных типов, работая фактически без обучения на примерах (или с минимальным обучением). Алгоритмы OpenIE (например, TextRunner, Ollie) выдают триплеты вида (“Маск”; “основал”; “компанию SpaceX”). Они полагаются на правила грамматической разбивки и простые классификаторы, чтобы отличать валидные отношения от невалидных. OpenIE полезен, когда нужен быстрый сбор фактов из корпуса, неважно каких типов. Но выход таких систем трудно использовать напрямую, там много дублирования, разные формулировки одного отношения и шум.

Современные **не-LLM** решения для relation extraction:

* **Небольшие нейросети:** BiLSTM или CNN, которые по конкатенации эмбеддингов сущностей и контекста предсказывают тип отношения. Например, конкатенация: \[эмбеддінг сущности1; эмбеддинг сущности2; эмбеддинг всех слов между ними] → полносвязный слой → softmax класса отношений. Такие модели обучаются быстро и показывают OK-результат, особенно если использовать pre-trained эмбеддинги. Отличный пример – **кодовая база NeuralRelationExtraction** от Стэнфорда (2018).
* **Distant supervision:** Чтобы не размечивать всё вручную, используют существующие базы фактов (типа Wikidata). Берут текст, ищут в нем известные пары сущностей из базы и предполагают, что предложение, где они встретились, выражает отношение, известное в базе. Это дает автоматически размеченные данные, правда с шумом. На этом обучают модель (SVM или нейросеть). Фреймворк distant supervision был популярен и без LLM, но страдал от ложных срабатываний (не каждое появление двух сущностей означает нужное отношение).
* **Комбинированные системы:** Фактически IE часто строят pipeline-ом: NER → классификация отношений. И здесь можно на каждом этапе применять лучшие методы: например, NER – BERT или CRF, а отношения – правила или мелкую модель. Или наоборот: NER – словари (в медицине так делают, определяя сущности лекарств/симптомов по словарю), а потом отношения – небольшой нейросетевой классификатор (определить, является ли “X вызывает Y” отношением side\_effect).

В итоге, для извлечения информации сейчас есть богатый инструментарий. Если нужен максимально точный продукт, обычно используются *гибридные методы*: **регэкспы/шаблоны + ML**. Например, компания может написать десяток надежных шаблонов для критичных отношений, а остальные дополнять ML-моделью. Это даст и precision, и recall.

Если же задача исследовательская – извлечь как можно больше фактов для заполнения базы – то могут применить комбо из NER+OpenIE+post-filtering, и потом ручная очистка.

**Пример кода (Python)**: воспользуемся библиотекой spaCy (которая, к слову, не LLM, а использует средние модели) для быстрого демо NER и зависимости, и извлечем простое отношение:

```python
import spacy
nlp = spacy.load("en_core_web_sm")  # небольшая модель spaCy (~12MB)
doc = nlp("Barack Obama was born in Hawaii.")
for ent in doc.ents:
    print(ent.text, ent.label_)
# Вывод: Barack Obama PERSON / Hawaii GPE

# Извлечение отношения "персона родилась в месте"
person = [ent for ent in doc.ents if ent.label_ == "PERSON"]
gpe = [ent for ent in doc.ents if ent.label_ == "GPE"]  # GPE = Geo-Political Entity (локация)
if person and gpe:
    for token in doc:
        if token.text.lower() == "born":
            print(f"Relation: BIRTH_PLACE({person[0].text}, {gpe[0].text})")
# Вывод: Relation: BIRTH_PLACE(Barack Obama, Hawaii)
```

Здесь мы используем spaCy для получения сущностей и зависимостей (в данном случае простым поиском слова "born"). В реальной системе мы бы анализировали синтаксическое дерево: “Barack Obama” – nsubj(pass) для "was born", а "Hawaii" – pobj (предложное дополнение). На основе этого можно формализовать правило.

### Машинный перевод без LLM

**Машинный перевод (MT)** – одна из старейших задач NLP (первые эксперименты еще в 1950-х). Эволюция MT прошла этапы:

* **Правил\_based перевод (RBMT)** – использование вручную созданных двуязычных словарей и грамматических правил. Например, система SYSTRAN (разрабатывалась с 1960-х) – большое количество если-то правил: если английское подлежащее стоит перед сказуемым, а во французском наоборот – переставлять; если слово такое-то – подставить соответствующее из словаря. Правил\_based перевод в теории может давать безупречный результат в ограниченной области, но требует титанического труда лингвистов для каждой языковой пары. И все равно, при встрече новой фразы система может дать смешной результат, если правило не предусмотрено. К 90-м RBMT был основой, но качество редко удовлетворяло.

* **Статистический машинный перевод (SMT)** – 2000-е годы. Здесь подход уже *обучаемый*: система выравнивает большие параллельные корпуса (оригинал–перевод), из них извлекает *переводные фразы* и оценивает их вероятности. Классическая модель – **привязка Фраз + языковая модель**. Например, для перевода с русского на английский: система хранит, что фраза “большой дом” чаще всего соответствует “big house”, а не “large home” (статистика фразового словаря), а также использует языковую модель английского, чтобы выбрать наиболее гладкий вариант (например, предпочтет "big house" vs "house big"). Алгоритмы типа **Moses** (phrasal SMT) были стандартом до 2016 года. **Достоинства SMT**: учится на данных – не нужно вручную кодировать, достаточно параллельных текстов; результаты лучше RBMT во многих случаях, особенно на распространенных языках; есть интерпретация (таблицы перевода, которые можно править). **Недостатки**: страдает на длинных дальнозависимых конструкциях, часто перевод “дословный”, могут теряться части предложения при плохом выравнивании, требуется достаточно большой параллельный корпус для приемлемого качества. Тем не менее, SMT позволил качественно улучшить перевод онлайн-сервисов: Google Translate изначально работал на SMT (с середины 2000-х до 2016).

* **Нейронный машинный перевод (NMT)** – появился около 2014–2015 с seq2seq моделями. Первые успешные – на основе LSTM + attention (как мы обсуждали). Их главные плюсы: более плавный, корректный перевод, особенно грамматически; лучше схватывают контекст, меньше переводят слово-в-слово, могут перефразировать для сохранения смысла. К 2016 году Google, Microsoft перешли на NMT, дав резкий скачок качества. Архитектура уже описана: **энкодер-декодер с вниманием**. Без трансформеров применялись многослойные LSTM, иногда BiLSTM. Например, Google’s NMT (2016) – 8-слойный LSTM, attention, субсловные единицы (byte-pair encoding). Размер модели – примерно 200M параметров, обучена на миллиардах примеров, но строго говоря, это еще не "LLM" по современным меркам, а специализированная модель перевода.

В 2023 году практически *все крупные переводчики используют трансформеры* (модели типа MarianMT, mBART, etc.). Но что, если нужно свой перевод без LLM?

* Можно воспользоваться открытыми нейросетевыми переводчиками. Например, **Helsinki-NLP/Opus-MT** – набор открытых моделей перевода (поддерживается на Hugging Face). Они относительно небольшие (60М параметров, трансформер), обучены на данных OPUS. Их можно запустить локально без интернета. Качество у них неплохое для распространенных языков. Считается, что *маленькие NMT модели* уже превзошли классические SMT почти во всех случаях.
* Если хотим совсем без нейросетей: существуют реализации **Apertium** – open-source RBMT, которые годятся для близких языков (например, перевод испанский–каталанский). Или **Moses SMT** – можно собрать свой статистический переводчик, если есть параллельные данные. Но это сложновато и качество будет скромным.
* Гибрид: использовать правила для того, что нейронка явно ошибается. Например, небольшие модели иногда путают именованные сущности с общеупотребимыми словами (переводят фамилию как существительное и т.п.). Решение: подключить модуль, который узнает имена (NER) и оставляет их транслитерированными.
* **Автоматический пост-редакшн**: отдельная тема – обучить модель исправлять ошибки машинного перевода. Это можно сделать маленькой моделькой, натренировав ее на разнице между переводами системы и эталоном. Такой подход + glossary (словарь терминов, которые нельзя переводить дословно) + NMT дает очень надежный результат в ограниченном домене.

В итоге, даже не прибегая к гигантам, качественный перевод возможен. Порог входа тоже снизился: если раньше нужно было быть спецом, чтобы поднять Moses или написать грамматику, то сейчас можно скачать готовую модель типа **MarianMT** (это seq2seq трансформер \~300M параметров) и использовать ее – она воспроизведет уровень гугл-переводчика 2017 года. MarianMT – это open-source, некоммерческая модель, не LLM, а просто двуязычный трансформер, поэтому можно считать ее “не-LLM” подходом.

**Пример кода** (используя библиотеки):

```python
from transformers import MarianMTModel, MarianTokenizer
src_text = "Привет, мир! Это тест перевода."
model_name = "Helsinki-NLP/opus-mt-ru-en"  # рус -> англ модель
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)
batch = tokenizer([src_text], return_tensors="pt")
gen = model.generate(**batch)
print(tokenizer.decode(gen[0], skip_special_tokens=True))
# Вывод: "Hello, world! This is a translation test."
```

Здесь мы использовали предобученную модель перевода ru->en из семейства OPUS-MT. Она небольшая (289МБ с токенизатором), работает на CPU и дает вполне корректный перевод простого текста.

Конечно, для лучшего качества обычно задействуют большие (например, NLLB от Meta, 1.3 миллиарда параметров), но мы обещали держаться подальше от very-large.

### Чат-боты и диалоговые системы (без LLM)

Сегодня чат-боты ассоциируются с ChatGPT, но простые диалоговые системы существуют уже давно:

* **Rule-based dialog (скриптовые боты)**: до недавнего времени большинство корпоративных чат-ботов – это *дерево решений*: пользователю задают вопросы с вариантами или ключевыми словами, по ответу переходят на ветку сценария. Примитивно, но в техподдержке работало: *“Здравствуйте, у вас вопрос по счёту или техническая проблема?”* – и дальше по сценарию. Реализация – с помощью конечных автоматов или платформ типа DialogFlow (где дизайнер прописывает интенты и реплики).
* **Retrieval-based chatbots**: бот не генерирует ответа, а выбирает наиболее подходящий из базы готовых реплик. Например, у вас есть FAQ из 100 вопросов-ответов; пользователь пишет что-то – система с помощью *semantic search* находит близкий вопрос и выдает соответствующий ответ. Технологии: tf-idf или векторные эмбеддинги для измерения близости запроса и вопросов базы. Такой бот может покрыть намного больше вариантов, чем скриптовый, и при этом он не скажет ничего “лишнего” – только то, что есть в базе. До LLM это был основной способ FAQ-чатботов.
* **Generative chatbots (узкие)**: были попытки обучить seq2seq модель на диалогах (например, на переписках поддержки или на фильмах). До трансформеров такие модели (на RNN) получались слабенькими: часто скатывались в повторение “Понимаю... чем я могу помочь?” или несли несвязное. Поэтому их редко использовали в коммерции (слишком непредсказуемо). Однако в исследованиях был знаменитый бот **Mitsuku (Kuki)** – победитель многих соревнований чат-ботов до эпохи GPT. Он был гибридным: огромная база шаблонных ответов (на всякие общие фразы типа “How are you?”) плюс немного генерации.

**Современный подход без LLM** для узконаправленных ботов обычно комбинирует:

* **NLU-модель для понимания интента и сущностей.** Пользовательское сообщение обрабатывается: классификация *намерения* (intent) – что он хочет (например, заказать товар, узнать статус заказа, пожаловаться и т.д.), и извлечение *ключевых сущностей* (слот-филлинг) – например, в фразе “забронируй столик на четверых завтра” нужно извлечь дату = завтра, кол-во людей = 4. Таких NLU-компонент множество: **Snips NLU**, **Rasa**, **DialogFlow ES** – все они под капотом используют не трансформеры, а как раз логистическую регрессию или малые сети для интентов и CRF для сущностей. **Snips**, например, показывает очень высокую точность с минимумом образцов, работая полностью *офлайн*. Это достигается за счет разумных фичей и небольших моделей, специально заточенных под короткие реплики.
* **Диалоговый менеджер**, который на основе определенного интента и слотов принимает решение, какую реплику выдать или к какому действию перейти. Здесь часто применяют *логический код* (if intent = ORDER\_PIZZA then ...).
* **Ответы** – могут быть шаблонными с подстановкой (например, “Ваш заказ номер {order\_id} принят”) или выбираться из набора. Можно немного разнообразить синонимами или шаблонами, но без генеративного ИИ бот обычно выдаёт заготовленный текст.

В итоге получаем систему, которая *понимает ограниченный набор запросов очень надежно* и *отвечает шаблонно, но по делу*. Для многих применений (справочные боты, заказы) этого достаточно. Пример: **банковский бот** – он понимает “заблокировать карту” или “узнать баланс” с вероятностью >95% даже классическим алгоритмом, потому что эти фразы отличаются четко. И он строго следует скрипту: сначала идентифицирует пользователя, потом выполняет нужное действие.

**Инструменты**: Rasa (open-source платформа ботов) – в ней по умолчанию NLU основано на *CountVectors + TensorFlowEmbed + DIET classifier*, что является небольшой нейросетью. Она может быть заменена на `LanguageModelFeaturizer` (например, BERT), но многие предпочитают лёгкий вариант, чтобы бот работал быстрее. **Snips NLU** – библиотека на Python, которая в режиме офлайн тренирует слоты и интенты с помощью CRF и SVM. Её цель – приватность (не слать данные на сервер) и быстрота, и она успешно с этим справлялась. К сожалению, Snips как компания была поглощена, но открытый код остался.

**Простой пример**: сделаем игрушечного бота на правилах и регулярках:

```python
import re

def chatbot_reply(user_input):
    # Простая классификация интентов по ключевым словам
    text = user_input.lower()
    if re.search(r'заказ.*пицц', text):
        return "Какую пиццу вы хотите заказать?"
    elif re.search(r'баланс|счет', text):
        return "Ваш баланс составляет 5 000 руб."
    else:
        return "Извините, я вас не понял. Попробуйте сформулировать иначе."
        
print(chatbot_reply("Хочу сделать заказ пиццы"))  # -> "Какую пиццу вы хотите заказать?"
print(chatbot_reply("Проверить баланс счета"))    # -> "Ваш баланс составляет 5 000 руб."
```

Это примитив, но иллюстрирует идею: мы ищем слова "заказ" и "пицц" для интента заказа пиццы, слова "баланс" или "счет" для интента проверить баланс. Реальный бот бы имел десятки таких правил или обученную модель вместо if-ов, но логика схожа. Ответы у нас статичные.

Конечно, у такого бота ограниченность: **отклонения от сценария он не обработает**. Именно это и решают LLM: гибко реагируют на что угодно. Однако не всякий сервис готов позволить боту импровизировать – зачастую важнее четко следовать процедурам.

Поэтому **в 2025 году существуют гибридные боты**: если запрос вне заложенных интентов, они передают его внешней модели (например, GPT через API) – так пользователь получит какой-никакой ответ. Но основные функции (навигация по меню, ответы на частые вопросы) бот покрывает сам простыми методами.

### Семантический поиск и анализ текстового сходства

**Семантический поиск** – это поиск документов не по ключевому слову точь-в-точь, а по *смысловой близости*. Например, запрос “как обучить нейросеть” может найти документ “методы тренировки глубоких моделей” даже без общих слов. Сейчас в моде делать такое с BERT, но раньше решали другими методами:

* **LSA (Latent Semantic Analysis)** – метод 90-х: берется матрица "документ-слово" (tf-idf), выполняется SVD-разложение, получается векторное представление слов и документов в уменьшенном пространстве, где снята избыточность. В этом пространстве похожие по смыслу слова имеют близкие координаты, и документ представляет собой "тематический" вектор. LSA мог ловить синонимию до некоторой степени: если слово “авто” и “машина” часто встречаются в похожих контекстах, их координаты сблизятся, и запрос “авто” будет подбирать документы с “машина”. Это была ранняя форма дистрибутивной семантики. До сих пор концепция LSA используется (в обучении языку, например, чтобы измерять сходство смыслов ответов студентов и материалов лекций).
* **Эмбеддинги слов и документов**: С 2013 популярно стало учить **word2vec** (преобразует слова в векторы, улавливающие семантику). Для сравнения текстов можно было усреднять word2vec слов – уже лучше, чем сырые tf-idf для коротких текстов. Далее появился **doc2vec** (Le & Mikolov, 2014) – аналог word2vec, но сразу для целого документа: обучается вектор параграфа параллельно с векторами слов, чтобы предсказывать слова. Он умел помещать схожие по теме тексты рядом в пространстве. Doc2vec какое-то время активно применялся, хотя настройка его нетривиальна.
* **BM25 и прочие улучшения TF-IDF**: Для поиска технической инфы или справочной классические подходы TF-IDF с некоторыми доработками (например, BM25 – формула ранжирования, учитывающая насыщение частот и длину документа) остаются базовым решением. Они очень быстрые и эффективны для точных совпадений. Проблема только синонимов и перефраз – их они не ловят.
* **Комбинированные индексы**: Многие поисковые движки (Solr, ElasticSearch) поддерживают *синонимичные словари*. Можно задать, что при поиске "автомобиль" он также ищет "машина", "авто". Это ручной способ добавить семантики. Также популярна была идея *расширения запроса*: взять WordNet или другую базу, добавить к запросу синонимы и гиперонимы. Частично помогает, но были и проблемы (увеличение шума).

В современном "без LLM" контексте:

* **Sentence-BERT** и подобные (это трансформеры, но небольшие) – уже очень распространились для семантического поиска. Есть модели вроде MiniLM, MPNet (\~100M параметров) от HuggingFace, которые специально натренированы для эмбеддингов предложений. Ими можно пользоваться офлайн и быстро. Можно спорить, считать ли их “LLM” – скорее нет, они не способны генерировать текст, просто делают векторы. Но они глубоко обучены, конечно, на больших данных.
* **Центроидные методы**: можно взять word embeddings (Glove, word2vec) и посчитать, скажем, *WE\_AVG* – средний вектор по всем словам запроса, и такой же для документов, потом косинусное сходство. Не идеал, но уже дает понимание. Или, улучшение: **SIF (smooth inverse frequency)** – метод усреднения эмбеддингов с понижением веса частых слов. Он, показано, выводит векторы предложений, сравнимые по качеству с первыми версиями BERT (без fine-tune).
* **Use knowledge base**: В специфических областях можно построить поиск по *онтологиям*. Например, запрос “лечение кашля” преобразовать в запрос по медицинской базе, связанной с симптомом “кашель” – и выдать найденные там документы. Это уходит больше в IR (information retrieval), но это вариант.

**Пример**: Возьмем два предложения и оценим их близость без BERT:

```python
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
sents = ["машинное обучение и глубокие нейронные сети", 
         "алгоритмы глубинного обучения для нейросетей"]
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(sents)
cos_sim = (X[0] * X[1].T).A[0][0]
print(cos_sim)  # косинус через скалярное, для нормированных tfidf он равен cos
```

Выход: он даст какое-то значение типа 0.5+. Это простой способ: TF-IDF векторы, косинус. Он покажет, что схожесть есть, так как слова пересекаются (“обучение”, “нейронные сети”). Если бы фразы были синонимичны без общих слов, tf-idf провалился бы. Тогда можно попробовать:

```python
import gensim.downloader as api
model = api.load("glove-wiki-gigaword-100")  # 100-мерный ГлоВе с Википедии
def avg_vector(text):
    vectors = [model[w] for w in text.lower().split() if w in model]
    return np.mean(vectors, axis=0)
v1 = avg_vector("машинное обучение и глубокие нейронные сети")
v2 = avg_vector("алгоритмы глубинного обучения для нейросетей")
cos_sim2 = np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))
print(cos_sim2)
```

(Тут, правда, на русском GloVe нет, но принцип ясен – для английского такое прошло бы).

В реальности, *семантический поиск* сейчас почти всегда = **Dense Vector Search**: любую модель (не обязательно BERT, может, fastText, doc2vec) для получения плотного вектора текста -> затем поиск ближайших по расстоянию (косинус/евклид). Специальные библиотеки (FAISS, Annoy) ускоряют kNN поиск по векторам. Эти методы появились до LLM и активно используются. В частности, **Pinecone** (векторная СУБД) дает сравнение подходов: TF-IDF vs BM25 vs word2vec vs BERT. Отмечается, что TF-IDF/BM25 – это “sparse vectors”, word2vec/doc2vec – первые “dense vectors”, BERT/USE – более новые “dense vectors” с контекстом. Промежуточное звено – **Universal Sentence Encoder (USE)** от Google (2018) – это по сути небольшой 4-слойный трансформер (или даже CNN вариант есть), выдающий 512-мерный вектор предложения. Он тоже обходится без огромных параметров, но дает отличные результаты в сходстве.

**Применение**: семантический поиск нужен в чат-ботах (сопоставить пользовательскую реплику с известными Q\&A), в рекомендательных системах (находить похожие товары по описанию), в кластеризации документов по теме. Без LLM всё это решается через эмбеддинги или тематическое моделирование (LDA – порождающая модель тем, тоже стоит упомянуть: **LDA (Latent Dirichlet Allocation)** – алгоритм кластеризации слов в темы; позволяет представлять документ как вектор распределения по темам. Если темы понятны, можно по ним мерить близость документов).

**Вывод**: Для семантического поиска уже есть зрелые подходы, и LLM просто дали еще лучше (но ценой). Если ресурсы лимитированы, можно применить более легкие: или даже комбинацию (сначала по ключевым словам сузить кандидаты, потом ранжировать по векторной близости – так делают гибридные поисковые движки).

### Другие задачи (обобщенно)

Сюда включим кратко:

* **Анализ тональности (sentiment analysis):** Классика – наивный Байес, SVM на BOW, или словарный метод (подсчет полярностей слов по готовому словарю). Есть готовые лексиконы (например, RuSentiLex для русского). Сейчас часто используют небольшие CNN или BiLSTM. Библиотека **TextBlob** даёт sentiment без глубокого обучения, опираясь на словарик и правило усреднения – для общего текста работает сносно.
* **Summarization (реферирование):** До нейросетей – экстрактивное: выбрать важные предложения. Методы: ранжирование предложений по весу (алгоритм TextRank – строит граф предложений по схожести, применяет алгоритм PageRank для важности). Или частотный: считать tf-idf слов для документа, каждому предложению сумму весов, взять топ-3 – это и будет короткое резюме. Работает иногда удивительно приемлемо. Нейросеточное абстрактивное суммаризацию (писать свое) без LLM – возможно seq2seq на LSTM (ранние работы пытались), но качество было так себе. Экстрактивное – все еще используется, когда нужен контроль (например, в юридических документах, где важно дословно брать предложения).
* **Разрешение анафор (coreference resolution):** Классически – набор правил (например, Hobbs algorithm) + некоторые классификаторы. Есть система **AllenNLP Coref** (2018), использующая относительно небольшую сеть с вниманием, она дает хорошее разрешение (размер модели \~30М).
* **Обнаружение фейковых новостей / определение тематики / спам-фильтры:** Это все вариации текстовой классификации. Очень успешно и сейчас используются классические модели: например, **спам-фильтр** в почте часто дообучается вариант наивного Байеса или логрегрессии с тысячами признаков (включая слова, наличие ссылок, метаданные и т.д.). К тому же, по требованиям приватности, почтовым сервисам может быть нельзя слать данные в LLM, поэтому встроенный спам-фильтр – старый, добрый (и обновляемый итеративно).

Подытожим: **Практически для любой задачи NLP существует решение, не завязанное на гигантские модели**, и зачастую не единственное. Многое доступно в виде библиотек и предобученных моделей, которые весят от мегабайт до нескольких сотен мегабайт, а не десятков гигабайт, как LLM.

## Современные библиотеки и инструменты

Перечислим несколько популярных библиотек для NLP, предоставляющих готовые методы и модели (кроме сугубо трансформерных вроде `transformers`, которое заточено на большие модели):

* **NLTK (Natural Language Toolkit):** одна из старейших (разработка начата в 2001). Это скорее учебно-вспомогательная библиотека для Python, предоставляющая: токенизацию, стемминг/лемматизацию (например, стеммер Портера), POS-теггер (несколько алгоритмов, включая униграммный, биграммный, HMM-теггер), парсер (рекурсивный спуск, ProbabilisticEarley и т.п.), и большой корпус текстов и словарей для исследований. NLTK очень удобен, чтобы быстро попробовать что-то, но в продакшн используют реже из-за скорости.

* **spaCy:** упомянутая индустриальная библиотека NLP на Python, оптимизированная на Сython под высокую скорость. Включает: токенизацию (правила под каждый язык), POS-теггинг и лемматизацию (встроенные статистические модели), синтаксический парсинг зависимостей (модель на основе transition-based парсера), NER (CNN-модель в старой версии, трансформер – в новой), и многие утилиты. spaCy примечательна своей производительностью – обработка тысяч токенов в секунду. Их модели `xx_core_web_sm` бесплатны и довольно точны (но, конечно, чуть уступают глубоким подходам). spaCy удобна, если нужно сделать целый NLP-конвейер **офлайн**: например, извлечь все именованные сущности и все зависимости из миллиона документов – она справится.

* **Stanza (Stanford NLP):** это Python-враппер вокруг CoreNLP от Stanford, но теперь с нейросетевыми моделями. Он особенно полезен для многих языков: их модель обучена на Universal Dependencies и покрывает >60 языков (включая рус/англ/кит и редкие). Stanza делает токенизацию, POS, морфологию, зависимости, NER, категоризацию чувств (sentiment) – одним вызовом. Точность высокая, но скорость ниже, чем spaCy (BiLSTM не так быстр). Если важнее качество и многоязычность – Stanza.

* **Flair:** библиотека от Zalando (на Python), менее известна широкой публике, но популярна в сообществе. Она построена вокруг концепции *объединения эмбеддингов*: предоставляет различные pre-trained embeddings (Glove, FastText, BytePair, ELMo, BERT и др.) и умеет их комбинировать. Содержит реализованные модели: последовательный теггер (BiLSTM-CRF), текстовый классификатор (BiLSTM или pooling), NER, POS, и др. С Flair можно достигать SOTA результатов классическими методами – авторы показывали, что объединяя, например, character-level embedding + GloVe + pooled BERT, их BiLSTM-CRF побил просто BERT-CRF. Конечно, сложность больше, но интересен сам подход. Flair также имеет набор готовых моделей (напр., `flair/ner-english`).

* **Gensim:** библиотека для тематического моделирования и семантического анализа. Основное: реализация word2vec, doc2vec, fastText; LSI/LSA; LDA (через MALLET или собственной реализации). Удобна для получения эмбеддингов, кластеризации тем. Сейчас несколько отошла на второй план, но для обучения своих эмбеддингов на текстах она хороша.

* **scikit-learn:** хотя не специализируется на NLP, но имеет инструменты, необходимые для многих описанных методов: `CountVectorizer`, `TfidfVectorizer` – создание признаков из текста; реализации наивного Байеса (`MultinomialNB`), SVM (`LinearSVC`), деревьев и лесов – всё, что нужно для построения классификатора текстов. Есть также `CalibratedClassifierCV` для вероятностных оценок, много метрик и пр. Для задач разметки последовательностей scikit-learn не имеет прямого решения (там надо извращаться), но есть надстройка **sklearn-crfsuite** – обертка над CRFsuite (C++ библиотека CRF).

* **Snips NLU:** упомянутая библиотека для слотов/интентов, хороша для небольших встроенных решений, где нет тяжёлых моделей. Она легкая и работающая offline. Однако её поддержка прекратилась, но форки существуют.

* **OpenNLP (Java):** если нужно на Java – Apache OpenNLP предлагает токенизацию, POS, NER, парсинг – с вероятностными моделями (максент и пр.), поддерживает обучение на своих данных. Не суперточно, но самостоятельно и довольно быстро.

* **AllenNLP:** фреймворк от AI2 (на Pytorch) – много готовых моделей, в т.ч. coreference resolution, constituency parsing, NER, QA. Но он больше исследовательский, требует хорошего GPU для работы (часто модели встроены на BERT).

* **HuggingFace Transformers (небольшие модели):** хотя это про трансформеры, там можно найти и небольшие модели, обученные для специфических целей. Например, у них есть раздел **sentence-transformers** – куча моделей для семантического поиска. Или модели для summarization, обученные на маленьких seq2seq (T5-small). Воспользоваться ими можно даже на CPU приемлемо.

* **Stanford CoreNLP:** сервер/библиотека на Java, классика. Обеспечивает полный конвейер: от токенов до coref resolution. Можно запускать как сервер и стучаться из любого языка. Внутри – комбинация правил и моделей (некоторые уже нейросетевые). CoreNLP славится надежностью (в смысле стабильности работы) и тем, что покрывает многое – даже извлечение временных выражений. Однако, новейших достижений в нем может не быть (Stanford скорее переключились на Stanza).

**Интеграция и экосистема:** Часто используют несколько библиотек вместе. Например, NLTK + scikit-learn: NLTK токенизирует, scikit обучает классификатор. Или spaCy + custom model: spaCy выделил сущности, дальше вы обрабатываете их с помощью своего кода. Rasa использует внутри spacy или tensor2tensor. Flair и HF Transformers могут совместно применяться.

Нужно отметить **DeepLearning библиотеки**: PyTorch, TensorFlow – это низкоуровневые вещи. На них реализуют кастомные модели, если нужно экспериментировать. Например, можно самому написать BiLSTM-CRF под задачу, если стандартные не подходят. Сейчас, однако, редко пишут с нуля – обычно берут готовые реализации и адаптируют.

## Примеры кода для базовых задач

Мы уже показали несколько небольших примеров в тексте. Здесь сведем их кратко, чтобы проиллюстрировать конкретные техники:

**1. Мешок слов и классификация наивным Байесом (Python, scikit-learn):**

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

texts = ["Этот товар отличный", "Ужасное качество, не покупайте", "очень довольна покупкой"]  # три отзыва
y = [1, 0, 1]  # 1 = позитив, 0 = негатив

vect = CountVectorizer()  
X = vect.fit_transform(texts)  # матрица "текст-слово"
model = MultinomialNB()
model.fit(X, y)
test = ["товар просто ужас"] 
X_test = vect.transform(test)
print(model.predict(X_test))  # вывод: [0] - негатив
```

Здесь мы построили BoW (модель учитывает частоты слов) и обучили наивный Байес определять тональность. Подаем фразу “товар просто ужас” – она содержит слово “ужас” встреченное в негативном примере, классификатор выдает 0 (негатив). Простой, но часто действенный подход.

**2. Использование регулярных выражений для извлечения информации:**

```python
import re
text = "Компания \"Ромашка\" была основана Петром Ивановым в 1995 году."
# Найдем шаблон: Название в кавычках
match = re.search(r'\"(.+?)\"', text)
if match:
    org_name = match.group(1)
# Найдем основателя: ищем "основан(а) * Именем Фамилией"
match2 = re.search(r'основан\w*\s+([А-ЯЁ][а-яё]+\s+[А-ЯЁ][а-яё]+)', text)
if match2:
    founder = match2.group(1)
print(org_name, founder)  # -> Ромашка Петром Ивановым
```

Регэксп здесь ищет кавычки, и шаблон для имени: слово "основан", за которым две слова с заглавных букв (предполагаем имя фамилию). Это хрупко, но демонстрирует extraction.

**3. CRF для последовательности (с использованием sklearn\_crfsuite):**

```python
!pip install sklearn-crfsuite
from sklearn_crfsuite import CRF
# Пример размеченных данных: последовательность слов и тегов (неполный псевдокод)
train_sentences = [
    [("Barack", "B-PER"), ("Obama", "I-PER"), ("lives", "O"), ("in", "O"), ("Washington", "B-LOC")],
    ...
]
# Преобразуем данные в признаки для CRF
def sent2features(sent):
    # формируем список диктов признаков для каждого токена
    return [ {"word": word.lower(), "is_capital": word.istitle(), "suffix3": word[-3:]} for word, tag in sent ]
def sent2labels(sent):
    return [tag for word, tag in sent]

X_train = [sent2features(s) for s in train_sentences]
y_train = [sent2labels(s) for s in train_sentences]
crf = CRF()
crf.fit(X_train, y_train)
# Теперь crf.predict([sent2features(test_sent)]) выдаст последовательность меток
```

Это упрощенно (настоящий код потребует настроить параметры). Мы определили признаки: само слово в нижнем регистре, флаг, что слово с большой буквы, суффикс длины 3. CRF обучится связывать эти признаки с метками типа B-PER, I-PER и O (outside). Если подать ему новое предложение с похожим шаблоном, должен выделить персону и локацию.

**4. Использование WordNet (через NLTK) для поиска синонимов:**

```python
from nltk.corpus import wordnet as wn
syns = wn.synsets("car")
print([syn.lemmas()[0].name() for syn in syns][:5])  
# Вывод: ['car', 'car', 'cable_car', 'car', 'gondola'] – разные значения "car"
# Выберем первое значение:
print(syns[0].definition())  # -> "a motor vehicle with four wheels; usually propelled by an internal combustion engine"
# Найдем гипероним (родовое понятие) для car:
hyper = syns[0].hypernyms()[0]
print(hyper.lemmas()[0].name())  # -> "motor_vehicle"
```

Этот код через NLTK WordNet найдет синонимы слова *car*, выведет определение первого значения и его гипероним "motor\_vehicle". Это демонстрация, как можно использовать лексическую базу: например, расширить запрос добавлением гиперонима или синонима.

**5. Семантическое сравнение текста без тяжелых моделей (используем Gensim LSI):**

```python
from gensim import corpora, models
texts = [["машинное", "обучение", "важно"], ["алгоритмы", "обучения", "нейронных", "сетей"]]
# Создаем словарь и корпус
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
# Строим модель LSI на 2 скрытых темах
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
# Векторное представление документов в пространстве тем:
vec0 = lsi[corpus[0]]
vec1 = lsi[corpus[1]]
print(vec0, vec1)
```

Модель LSI выявит 2 “темы” и спроецирует каждый документ на эти темы. Если они похожи, их координаты будут похожи. В реале мы бы посчитали косинус между vec0 и vec1. LSI позволяет улавливать скрытые синонимы (если бы они были – в таком коротком примере вряд ли, но на большом корпусе – да).

## Заключение: Тренды и перспективы “не-LLM” NLP

Большие языковые модели произвели революцию в NLP, но **классические и малые методы остаются крайне востребованными**. Они часто проще, прозрачнее, требуют меньше ресурсов и могут быть применены там, где использование LLM невозможно (из-за приватности, отсутствия интернета, ограничений оборудования или просто избыточности LLM для задачи).

### Текущие тренды в развитии “не-LLM” NLP:

* **Интеграция с миром знаний:** Добавление к классическим NLP возможностей доступа к базам знаний, поисковым системам, внешним источникам. Это идет в ногу с трендом объяснимого и достоверного ИИ. Даже большие модели, как мы видим, пытаются подкрепить выдачу фактами (см. Bing Chat, который делает поиск и цитирует источники). В мире малых моделей – это естественным образом делалось и раньше (pipelines IE, FAQ-боты). Теперь упор на *универсальные интерфейсы*: например, язык программирования. Вместо генерации длинного текста, система может сгенерировать запрос или программу, которая извлечет нужные данные. Так сочетается логика и язык.

* **Neuro-Symbolic AI:** Уже обсуждали – объединение символических правил и нейросетей. Исследования в этой области (IBM, MIT) активно ведутся. Практическое выражение – появление фреймворков, где можно задать ограничение на вывод модели. Например, библиотека **Constraint-Based Decoding** для генерации – можно задать, что модель *должна* упомянуть определенные факты или следовать шаблону. Это поможет гарантировать, что вывод не нарушает правил.

* **Sustainability (устойчивость):** Общество начинает осознавать, что не всегда оправдано тратить огромные вычислительные ресурсы. Маленькие модели “зеленее”. Будет спрос на решения, которые экономят энергию. Это мотивирует разработки в области эффективных алгоритмов. Например, возрождение интереса к алгоритмам типа **Frank-Wolfe для оптимизации** – они могут обучать модели более точно на меньших данных. Возможно, снова станут популярны *компримированные представления*, как LSA, но на новом уровне (например, с использованием квантовых вычислений – об этом тоже говорят).

* **LLM-вдохновленные улучшения классики:** Пример – взять тот же механизм внимания, но применить его в небольшой модели или в аналитическом алгоритме. Или использовать знания, извлеченные из LLM, чтобы улучшить правило. Например, можно “спросить” у ChatGPT список синонимов для заданного слова и затем использовать их в обычном поиске – это уже гибрид большого и малого. То есть LLM могут служить *инструментом разработки*: сгенерировать тренировочные данные (data augmentation), предложить новые признаки. Но финальная модель может оставаться простой.

* **AutoML и мало данных:** Автоматизация процесса подбора модели – сейчас доступны AutoML-инструменты, которые перебирают разные алгоритмы (в т.ч. простые) и находят оптимальный. Для бизнеса важно получить приемлемое решение быстро и с минимальными данными. И часто автоML выберет не нейросеть, а что-то вроде ансамбля деревьев или логрегрессии с особыми признаками, если данных мало. С развитием few-shot/multi-task методов, возможно, появятся сервисы типа “GPT, обучи мне небольшой классификатор на этих примерах” – и GPT выдаст код логрегрессии с коэффициентами. Это переведет знания больших моделей в small scale.

* **Мультимодальность:** В интеграции текстовых методов с другими: изображениями, аудио. Например, распознавание речи – классически HMM+акустические модели (GMM), потом LSTM, теперь трансформеры. Но для embeded устройств старые методы ASR (Automatic Speech Recognition) до сих пор используются (Kaldi – там много серьезной математики без трансформеров). В компьютерном зрении – обратный пример: там нейросети задоминировали еще раньше NLP, но и там (скажем, для распознавания простых QR-кодов) нужны не гиганты, а детерминированные алгоритмы. Интеграция – например, анализ видео с субтитрами: можно делать детекцию сцен нейросетью, а по субтитрам – классику NLP, и объединять.

**Направления развития:**
В научном плане, вероятно, продолжится поиск **альтернатив трансформерам**: уже исследуются модели на основе продолженных CNN (так называемые FNet, gMLP – пытаются заменить self-attention на другие механизмы). Если что-то удастся, это приведет к новым типам моделей, возможно, более простым.
Также, **методы обучения с учителем** могут вернуться на новые круге: LLM впечатляют, но они в некотором смысле отступили от четкого supervision (они учатся предсказывать следующее слово, а не явные метки). Есть мнение, что *лучшее будущее – комбинация огромных pretrained + точного дообучения на специальных задачах*. Уже происходит: например, медицинский вопрос–ответ – берут большую модель и доучивают на экспертах. Но я говорю о том, что **архитектуры малых моделей могут вписаться внутрь больших**: допустим, LLM отвечает на вопрос и параллельно запускает маленький модуль логического вывода или поиска. Со стороны пользователя это один инструмент, но “под капотом” – совместная работа.

Наконец, **образование и объяснимость**: малые модели проще объяснить. Мы можем сказать: “вот, модель определяет тональность, потому что слову ‘хорошо’ присвоен положительный вес +2.0, а ‘плохо’ – -3.1”. С LLM сложно так сделать. Поэтому в чувствительных сферах (юриспруденция, медицина) могут предпочесть **интерпретируемые решения**, даже если они немного менее точные. Правила + деревья решений + человеческая проверка – могут дать требуемое доверие.

### Заключительная мысль:

Обработка естественного языка – это богатый инструментарий, накопленный десятилетиями. Большие языковые модели – мощный новый инструмент, но не панацея для всех случаев. В арсенале NLP остаются:

* Проверенные статистические методы (они просты и надежны на известных типах данных).
* Мелкие обучаемые модели (их можно обучить и запустить практически везде).
* Грамматические и семантические правила (где нужна точность и контроль).
* Лингвистические базы (они привносят знания, которые модель может не вывести сама).
* Комбинирование разных подходов для достижения лучших результатов.

Вероятно, будущее NLP – это **гармоничное сочетание** всех этих элементов. Если LLM сравнить с универсальным солдатом, то классические методы – это хорошо слаженная команда специалистов. В зависимости от миссии, иногда выгоднее послать команду, а не одного универсала. Мы видим, что обе парадигмы взаимодействуют: большие модели вдохновляют улучшения маленьких, а маленькие помогают большим быть более практичными. Как итог, можно уверенно сказать, что эра LLM не отменяет важности и развития “не-LLM” методов. Напротив, она дает им новое применение и стимул к эволюции, делая поле NLP ещё более разнообразным и увлекательным.
