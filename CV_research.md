# Компьютерное зрение 2024–2025: современные методы, достижения и тенденции

## Введение

Компьютерное зрение (Computer Vision, CV) – область ИИ, позволяющая машинам «видеть» и понимать визуальные данные (изображения, видео) подобно человеческому зрению. В 2024–2025 гг. CV продолжает быстро развиваться, внедряться в разных отраслях и показывать рекордные результаты. Объём рынка технологий CV достиг \~\$19,8 млрд в 2024 году и прогнозируется рост \~20% в год. В этой сводке мы рассмотрим **современные архитектуры** моделей CV, **новейшие методы обучения**, ключевые **области применения**, а также основные **тренды 2024–2025 гг.** Мы перечислим наиболее **значимые научные работы**, **открытые проекты и датасеты**, используемые **фреймворки**, приведём **таблицу соответствия моделей задачам**, и отметим **ведущие исследовательские группы и компании** в сфере CV. (Все утверждения подкреплены ссылками на источники.)

## Современные архитектуры в компьютерном зрении

### Сверточные нейронные сети (CNN)

**CNN** исторически доминируют в CV: с их помощью были достигнуты прорывы в классификации изображений (например, знаменитая сеть AlexNet в 2012 году и ResNet в 2015). CNN эффективно извлекают локальные признаки на изображении с помощью свёрток и слоёв pooling. Сегодня **сверточные сети** по-прежнему широко применяются – особенно в задачах детекции объектов, сегментации и др. Современные архитектуры CNN, такие как EfficientNet и ConvNeXt, достигают высоких точностей, оптимизируя количество параметров и эффективность вычислений. Тем не менее, с 2020–2021 годов появился серьёзный конкурент CNN – трансформеры, которые обеспечивают сравнимые или лучшие результаты в ряде задач.

### Vision Transformers (ViT) и трансформерные модели

**Vision Transformer (ViT)** – архитектура на базе механизма *самовнимания*, изначально разработанного для NLP, адаптированная для изображений. ViT обрабатывает изображение, разбитое на патчи, и позволяет модели учитывать глобальный контекст сцены. **Трансформеры для зрения** совершили переворот: в 2024 году они вышли на ведущие позиции, превзойдя традиционные методы на ключевых задачах. ViT и его варианты (например, Swin Transformer, DeiT) доказали высокую эффективность в классификации, а также *установили новые стандарты качества в детекции и сегментации объектов*. Трансформеры способны обрабатывать изображение целиком и учиться глобальным зависимостям, что особенно полезно для сложных сцен. Например, архитектура DETR (Facebook AI, 2020) показала, как чисто трансформерный подход может решить задачу детекции без традиционных CNN. Благодаря этому все больше современных моделей CV используют или комбинируют трансформеры, а объём рынка решений на основе ViT стремительно растёт (с \$280 млн в 2024 до прогнозируемых \$2784 млн к 2032).

### Диффузионные модели

**Диффузионные модели** (diffusion models) – класс генеративных моделей, вызвавший огромный интерес с \~2021 года. Их принцип – пошаговое добавление шума к изображениям и обучение обратному процессу восстановления, что позволяет затем генерировать новые образцы. В области *генерации изображений* диффузионные модели совершили прорыв: они **превзошли GAN по качеству и реалистичности синтезируемых изображений**. Например, модель **Stable Diffusion** (2022) продемонстрировала возможность открытого генератора изображений высокого разрешения на основе латентных диффузионных моделей. Диффузионные подходы теперь используются в **текст-	to-изображение** системах (DALL-E 2/3 от OpenAI, Imagen/Parti от Google), для *интерполяции, реставрации и раскрашивания* изображений, и даже для генерации видео. GAN-сети не исчезли, но очевидно уступают по реалистичности – отмечается, что **диффузионные модели превосходят GAN по создаваемому реализму, пусть и за счёт большего времени генерации**. Сейчас ведутся работы над ускорением диффузионных моделей и объединением их с другими подходами (например, гибрид Diffusion-GAN). Диффузионные модели стали ключевым трендом генеративного CV, всё чаще применяются для синтеза данных, что даже считается одним из трендов 2025 года.

### Мультимодальные модели (текст+изображения и др.)

Современное CV всё чаще *объединяется с обработкой языка*, что дало рождение **мультимодальным моделям** – способным понимать одновременно изображение и текст (а иногда и другие модальности, как аудио). Яркий пример – модель **CLIP** (OpenAI, 2021), обученная сопоставлять изображения и текстовые описания в общем пространстве признаков. CLIP показал, что можно выполнять классификацию изображений *без обучения под конкретные классы* – через текстовые подсказки, достигая **zero-shot производительности** на многих датасетах сравнимой с обученными моделями. Другие мультимодальные модели, такие как ALIGN (Google, 2021) и Florence (Microsoft, 2021), также обучались на парах «картинка-текст» из интернета и создали основу для широкого применения *поиска изображений по описанию, генерации описаний к картинкам (image captioning) и визуального вопрос-ответа (VQA)*.

В 2023 году тенденция усилилась: большие языковые модели (LLM) начали наделяться зрением. Например, **GPT-4** от OpenAI умеет анализировать изображения, ответить на вопрос по картинке – это стало возможным благодаря объединению CV с NLP. Появились и специализированные мультимодальные модели: **Flamingo (DeepMind, 2022)** – для визуального диалога, **BLIP-2 (Salesforce, 2023)** – для генерации описаний и диалога по картинкам, **LLaVA (2023)** – объединяющая LLM с визуальным входом. Тренд таков, что **число работ по совмещению зрения и языка на CVPR выросло вдвое за год**. Мультимодальные *генеративные* модели тоже прогрессируют: например, **DALL-E 3 (2023)** генерирует изображения по сложному описанию с беспрецедентным качеством. В целом, интеграция зрения с другими модальностями (речь, текст) – один из главных трендов CV 2024–2025, позволяющий создавать более «понятные» ИИ-системы, которые могут и видеть, и читать, и объяснять.

### Другие архитектуры и подходы

Помимо основных направлений, стоит отметить несколько специализированных архитектур CV:

* **GAN** (Generative Adversarial Networks, генеративно-состязательные сети) – до появления диффузионных моделей были основным инструментом генерации изображений. Современные GAN (StyleGAN3, GauGAN2 и др.) всё еще применяются для сверхвысокочёткой синтезии лиц, улучшения разрешения (супер-резолюшн) и других задач, где нужна мгновенная генерация без итераций. Однако их развитие замедлилось, уступая место диффузионным моделям.

* **Neural Radiance Fields (NeRF)** – архитектура для **3D-визуализации и реконструкции сцен** по набору 2D-фото. NeRF и его многочисленные улучшения (Instant-NGP, NeRFStudio и др.) с 2020 по 2023 год совершили революцию в представлении 3D-объектов: теперь по фотографиям можно строить фотореалистичные 3D-модели сцены, что нашло применение в AR/VR и робототехнике. К 2025 году NeRF стали быстрее (близко к реальному времени) и объединяются с диффузионными моделями для генерации **новых 3D-сцен** по текстовому описанию – зарождение новой области Generative 3D.

* **Графовые нейросети** (Graph Neural Networks) в CV применяются для задач, где данные имеют структуру графа: например, анализ сцены через граф объектов или моделирование взаимодействий (social graph) при детекции людей. Их применение узкое, но они дополняют CNN/ViT, позволяя моделировать связи «объект-объект».

* **Two-stage vs One-stage детекторы**: традиционно архитектуры детекции делятся на двухшаговые (Region-based, как Faster R-CNN) и одношаговые (YOLO, SSD). В 2024 году граница между ними стирается, так как трансформерные детекторы (DETR) и улучшенные одношаговые сети достигают сопоставимых результатов. Тем не менее, семейство **YOLO** продолжает активно развиваться – вплоть до YOLOv8 (2023) и дальнейших версий – сохраняя лидерство в *реальном времени* и простоте использования.

В таблице далее мы сведем основные типы моделей CV и их применение по задачам.

## Новейшие техники обучения моделей CV

### Обучение без учителя (Self-Supervised Learning)

Традиционные модели CV требовали огромных размеченных датасетов. **Self-supervised learning (SSL)** – обучение без ручной разметки, на основе скрытых свойств самих данных – стало необходимым ответом на ограниченность разметки. В CV SSL обычно реализуется через *предтекстовые задачи*: например, предсказание закрытых фрагментов изображения (masked modeling) или сопоставление разных видов представлений одного изображения (contrastive learning). С 2020 года появилось много SSL-методов, позволивших обучать **фундаментальные модели** на миллионах нелейбленных изображений: SimCLR, MoCo, BYOL, SwAV, DALL-E Encoder и др. К 2023–2024 году SSL подошло к черте, когда качество функций сравнимо с обучением на размеченных данных. Пример – модель **DINOv2** (Meta AI, 2023): первая self-supervised модель, *достигшая уровня SOTA супервизованных методов*, причём без какой-либо тонкой настройки под задачи. DINOv2 (на основе vision transformer) обучена на огромном объёме неразмеченных изображений и генерирует универсальные визуальные признаки, которые можно сразу использовать для классификации (линейным классификатором) или других задач, достигая высоких результатов.

Самосупервизируемое обучение фактически создаёт **«обобщённые модели зрения»** (foundation models) – их можно адаптировать под множество прикладных задач без полного переобучения. Это подтверждается и взрывом открытых датасетов для SSL (см. ниже про LAION-5B, Segment Anything), и практикой: например, функции, полученные SSL, применяются в медицинских изображениях, где мало разметки, или в робототехнике. **Тренд 2024–2025** – совершенствование SSL для CV, объединение его с языковыми данными (обучение на парах «изображение-текст» – как CLIP) и снижение зависимости CV-систем от ручной разметки.

### Обучение с подкреплением (Reinforcement Learning) в CV

Хотя RL более характерен для задач управления и игр, он всё теснее переплетается с компьютерным зрением. **Обучение с подкреплением** применяется, когда системе нужно принимать *последовательные решения на основе визуальных данных*. Классический пример – автономные агенты (роботы, дроны, самоуправляемые машины), которые с помощью CV «видят» окружающую среду и с помощью RL учатся действовать в ней (объезжать препятствия, манипулировать объектами и пр.). Исследования последних лет показывают перспективность RL в сочетании с CV: так, *применение глубинного RL продемонстрировано для задач самого CV* – улучшения детекции, сегментации, трекинга и даже генерации изображений. Например, агент с подкреплением может активно выбирать области изображения для детального распознавания (active object detection) или управлять камерой, чтобы захватить лучший ракурс. В робототехнике CV+RL позволяют обучать системы схватить предметы: робот видит объект камерой и через RL оттачивает стратегию хватания.

Тем не менее, RL в CV сталкивается с вызовами: высокая размерность визуальных входных данных, дороговизна симуляции. Работы 2024 года указывают на новые подходы – *иерархические* политики, *мультимодальное* RL, *domain adaptation* – чтобы повысить эффективность и обобщаемость визуальных RL-агентов. В целом, RL ещё не стал стандартным инструментом CV, но на стыке этих областей появляются интересные решения (например, «обучение с подкреплением для улучшения генеративных моделей» или для оптимизации параметров распознающих сетей). Ожидается дальнейшая интеграция RL и CV, особенно в автономных системах и роботах, где требуется замкнутый цикл «восприятие-действие».

### Трансферное обучение и тонкая настройка (Transfer Learning, Fine-Tuning)

**Transfer learning** – перенос обученных моделей на новые задачи – давно стал рабочей лошадкой CV. Почти каждый современный проект компьютерного зрения использует предварительно обученную модель (обычно на большом датасете вроде ImageNet) и затем выполняет *fine-tuning* (тонкую настройку) под свою специфическую задачу. Это позволяет обучать точные модели, имея на порядки меньше размеченных данных, экономя время и ресурсы. Например, берётся **ResNet-50, обученный на ImageNet** (размером 1,2 млн изображений), и дообучается на небольшом датасете медицинских снимков – модель быстро адаптируется к новым классам, уже «зная» общие визуальные признаки.

В 2024 году подходы трансферного обучения развились вместе с появлением *фундаментальных моделей*. Теперь часто переносят не только классификаторы, но и универсальные эмбеддинги или целые модели с поддержкой различных задач. Например, **Segment Anything Model (SAM)** от Meta (2023) – модель сегментации, обученная на огромном наборе изображений и масок – можно без обучения применять на любых изображениях для сегментации по точке или рамке, или использовать как основу и тонко настроить под сегментацию медицинских снимков. Похожим образом, самосупервизорные модели (типа DINOv2) переносятся без дообучения – признаки используются напрямую для новых задач.

Также развивается идея *few-shot* и *zero-shot* обучения в CV – крайние случаи трансферного обучения, когда новая задача решается с очень маленьким числом примеров или вообще без примеров. Например, CLIP позволяет сразу классифицировать новые категории через текстовые описания (zero-shot), а Segment Anything может выделять объекты по любому указанию пользователя без доп. обучения. Для повышения гибкости часто используют *prompt tuning* – например, добавляют адаптеры или текстовые подсказки к модели, вместо изменения всех её весов. Всё это – вариации трансферного подхода.

**Итог**: трансферное обучение остается стандартным инструментом CV, а fine-tuning крупных предобученных моделей – основным путем достижения наилучшей точности на прикладных задачах. Практики тонкой настройки все более тонки: выбирают какие слои заморозить, какие learning rate, используют техники вроде knowledge distillation при переносе знаний на облегчённые модели для внедрения. Без трансферного обучения сегодня практически не обходится ни один проект компьютерного зрения.

## Области применения компьютерного зрения

Прорывы в алгоритмах влияют на множество отраслей. Рассмотрим ключевые прикладные области, где в 2024–2025 гг. активно применяется CV:

### Медицина

Компьютерное зрение совершает революцию в диагностике по медицинским изображениям. Алгоритмы распознавания на **рентгеновских снимках, МРТ, КТ** позволяют автоматически обнаруживать патологии – порой *точнее врачей* в узких задачах. Например, компании **Zebra Medical Vision** и **Aidoc** разрабатывают ИИ, которые автоматически анализируют снимки и выявляют признаки рака груди, легочных узелков, инсульта и т.д., разгружая врачей и ускоряя диагностику. В офтальмологии AI уже помогает: алгоритм **Google Health** по снимкам сетчатки выявляет диабетическую ретинопатию на ранней стадии, предотвращая слепоту. Стартап **PathAI** использует CV для анализа гистопатологических срезов (биопсий) – обнаружения опухолевых клеток, оценки агрессивности рака.

Другие применения включают **хирургию** – CV применяется в роботизированных хирургических комплексах (например, **робот-хирург da Vinci**): камеры дают увеличенное 3D-изображение операционного поля, а алгоритмы могут подсвечивать критические структуры. В телемедицине CV помогает мониторить состояние пациентов по видео (распознавание движений, мимики боли). *Важно*, что во всех медицинских приложениях вырос запрос на **доверие и объяснимость**: поэтому разрабатываются методы XAI, поясняющие, что именно «увидел» алгоритм на снимке. Уже к 2024 году ряд исследований показал эффективность CV-систем, и они проходят клиническую апробацию (например, ИИ для маммографии, одобренный FDA). Медицинская область – одна из наиболее социально значимых для CV, и тренд нарастает.

### Автономный транспорт

**Компьютерное зрение – «глаза и мозг» самоуправляемых автомобилей**. Современные беспилотные машины оснащены множеством камер кругового обзора, и нейросети в реальном времени анализируют поток видео: распознают дорожные знаки и светофоры, детектируют автомобили, пешеходов, разметку, строят 3D-карту окружения. Система **Tesla Autopilot** – один из примеров vision-only подхода: 8 камер высокого разрешения и сетевые модели обрабатывают дорожную сцену, позволяя реализовать адаптивный круиз-контроль, удержание в полосе, предотвращение столкновений и т.д.. Другой лидер – **Waymo (Alphabet)** – комбинирует данные камер с лидарами и радарами; CV-модели Waymo решают задачи от сегментации дорожного полотна до отслеживания поведения пешеходов. Уже достигнуто, что в ограниченных условиях (например, городские такси Waymo в Фениксе) ИИ-водители работают без человека.

CV в транспорте – это не только легковые автомобили. В поездах, трамваях внедряются системы машинного зрения для обнаружения препятствий на путях. В авиации – для автоматического взлёта-посадки дронов по видео с бортовых камер. На дорогах CV помогает умным светофорам – камеры считают поток машин и пешеходов, оптимизируя фазы. В целом, прогресс CV напрямую улучшает **безопасность транспорта**: с каждым улучшением точности детекции уменьшается вероятность, что беспилотник не распознает опасность. По мере роста вычислительных мощностей, все больше обработки переносят на борт (edge AI), уменьшая задержки. В перспективе 2025+ – появление всё более автономных транспортных средств, интегрированных в **«умные города»** с повсеместным видеоконтролем трафика.

### Безопасность и наблюдение

**Системы видеонаблюдения** с поддержкой CV стали основой безопасности в общественных местах и на объектах. Алгоритмы реального времени анализируют потоки с камер, чтобы *обнаруживать угрозы и аномалии*. Например, технологии **распознавания лиц** позволяют идентифицировать разыскиваемых преступников в толпе (эта технология уже развернута в ряде аэропортов и городов). Сети для детекции действий могут автоматически заметить драку, падение человека или оставленный без присмотра предмет и сразу подать сигнал оператору. **“Умные” камеры** в общественных местах (парках, на улицах) с помощью CV считают число людей, распознают подозрительное поведение, что помогает полиции реагировать превентивно.

На пропускных пунктах CV используется для **распознавания номеров автомобилей**, контроля доступа по лицу. В коммерческих зданиях – для автоматического обнаружения нарушений (проникновение в запретную зону, отсутствие каски на производстве). Отдельное новое направление – **детекция дипфейков** и фальсификаций видео: с ростом генеративных алгоритмов появилась потребность средствами CV проверять подлинность видеоизображений (например, искать следы подмены лица). Это вызов, и 2024 год принёс много исследований по распознаванию синтетических медиа, так как для безопасности (в том числе информационной) это критично.

С другой стороны, остро стоят вопросы **этики и приватности**. Европейские регуляторы обсуждают ограничения на массовое распознавание лиц; требуются методы анонимизации (размытие лиц на видео неслежения) и **объяснимости решений CV** (почему система сочла кого-то угрозой). Тем не менее, тренд ясен: камеры становятся «умнее», и доля **AI-видеонаблюдения** растёт по всему миру (особенно в смарт-сити проектах Китая, Сингапура и т.д.). Ожидается, что к 2030 году большую часть видеоданных будут анализировать именно алгоритмы CV, а не люди.

### Ритейл (розничная торговля)

**Retail-сектор** активно внедряет CV для автоматизации и улучшения клиентского опыта. Один из самых показательных кейсов – магазины формата *Amazon Go* и аналоги: в торговом зале установлено множество камер, а CV-модели отслеживают действия покупателей – какие товары они берут с полок – позволяя отказаться от кассиров и очередей. **Смарт-камеры** фиксируют, какой товар взят, и система автоматически списывает его стоимость при выходе. Это стало возможным благодаря сочетанию детекции объектов и идентификации покупателей.

&#x20;*Пример системы CV в ритейле: камеры магазина автоматически определяют покупателей (в прямоугольниках) и их перемещение по залу, а также выделяют зоны интереса для бизнес-аналитики.*

Кроме магазинов без касс, CV решает и другие задачи ритейла: **анализ полок** – камеры проверяют выкладку товара, наличие пустых мест и автоматически создают задачи персоналу пополнить запасы. **Противодействие потерям** – системы видеонаблюдения с CV способны распознавать кражи (неоплаченный товар спрятан и выносится) или мошенничество на кассе. Например, ИИ может заметить, что на кассе пробивают один товар, а в пакете другой – и уведомить охрану. **Аналитика поведения клиентов** – тепловые карты перемещения людей по магазину, время, проведённое у витрин, подсчёт очередей – всё это делают алгоритмы зрения, помогая оптимизировать выкладку и работу персонала.

В онлайн-ритейле CV тоже незаменим: **поиск по изображению** (например, в приложении клиента можно сфотографировать вещь – и найти похожую в каталоге, как реализовано в Zalando или AliExpress). Это делается с помощью эмбеддингов изображений и поиска ближайших соседей. Также CV используется для **виртуальной примерки**: алгоритмы сегментации позволяют «надеть» одежду на фото покупателя. В маркетинге – для анализа эмоций клиентов на видео с камер в магазине (понимания, какие зоны вызывают интерес). Сочетание CV с AR дает приложениям «наведи камеру на товар – получи информацию или отзывы» в дополненной реальности.

Таким образом, от склада до торгового зала, CV делает ритейл более интеллектуальным, снижает издержки и улучшает сервис. По прогнозам, практически все крупные сети ретейла внедрят CV-решения к 2025 году для контроля запасов и бесшовных покупок.

### Дополненная и виртуальная реальность (AR/VR)

**Дополненная реальность** напрямую зависит от CV: чтобы наложить виртуальные объекты на реальный мир, устройство должно понимать, что видит. AR-очкам или смартфону нужно **отследить плоскости, глубину сцены, движения камеры** – этим занимаются алгоритмы SLAM и глубинной оценки, основанные на CV. Например, **платформа ARCore (Google)** в реальном времени распознаёт поверхности (пол, стены) и ключевые точки, чтобы правильно разместить 3D-модель поверх видео. **Niantic Lightship** – AR-платформа (создатели Pokémon GO) – продвинуло эту область, позволяя множеству пользователей видеть общие виртуальные объекты, что требует высокоточного картирования пространства с помощью CV.

В **виртуальной реальности** CV тоже находит применение: современные VR-шлемы (Oculus Quest, HTC Vive) оснащены камерами для *отслеживания положения в пространстве* – так называемый inside-out tracking. Алгоритмы CV обрабатывают видео и определяют положение и ориентацию гарнитуры без внешних датчиков, а также отслеживают контроллеры или даже руки пользователя (как реализован **hand-tracking** на Oculus – с помощью CNN, выделяющих ключевые точки рук).

CV используется и для **захвата движений** (motion capture) без маркеров: просто по видео определяется поза человека в 3D, что важно для «аватара» в VR. Кроме того, CV помогает улучшать **качество рендеринга**: например, техника foveated rendering (отрисовка высокого качества только там, куда смотрят глаза) требует отслеживать взгляд – это тоже задача CV (детекция зрачков на камере внутри гарнитуры).

В AR-приложениях на смартфоне CV отвечает за **распознавание образов** (image tracking) – например, наведя камеру на постер, приложение узнаёт его и показывает связанную анимацию. Или в промышленной AR-очках – распознавание объектов оборудования, чтобы выдать информацию по ним.

Важно отметить, что 3D-алгоритмы (глубинное зрение, стерео, оптический поток) – неотъемлемая часть AR/VR. В 2024 году большое внимание уделялось улучшению *точности и скорости глубинных нейросетей*, а также сжатию моделей для работы на мобильных чипах. CV совместно с AR/VR ведёт к появлению **«метавселенных»**, где реальное и виртуальное смешиваются в режиме реального времени. Ожидаемый в 2024–2025 выход устройств (например, **Apple Vision Pro**) с продвинутым набором камер и сенсоров поднимет планку для CV: такие устройства требуют **минимальной задержки** и максимальной надёжности компьютерного зрения для полного погружения пользователя.

### Генерация и редактирование изображений

В последние два года в массовое сознание вошёл термин **«генеративный ИИ»**, в том числе для изображений. Теперь компьютерное зрение не только распознаёт, но и *создаёт визуальный контент*. На практике это означает появление множества сервисов **генерации изображений по описанию** (Stable Diffusion, MidJourney, DALL-E и др.), **редакции изображений** (замена фона, цветизация, улучшение качества) и даже **генерации видео**. Эти приложения строятся на моделях, описанных выше – диффузионных, GAN, VQ-VAE и трансформерах.

Считается, что **Generative AI** – ключевой тренд 2023–2025 во всех сферах ИИ. Визуальные генеративные модели уже используются в **развлечениях и дизайне**: художники и дизайнеры получают эскизы от ИИ, синтезируя десятки вариантов за минуты. В **образовании и маркетинге** – для создания наглядных материалов по запросу. В индустрии моды – для генерации образов одежды на модели. Даже в **научных исследованиях** – для синтеза изображений, похожих на реальные (например, чтобы увеличить набор медицинских снимков, не беспокоясь о приватности пациентов).

Ещё одна сфера – **синтетические данные** для обучения других моделей CV: генеративные сети могут создать фотореалистичные сцены с размеченными объектами (аннотации получают «бесплатно» из движка), что помогает обучить детекторы, не собирая реальный датасет.

Важное достижение конца 2023 – **сочетание языковых и визуальных генераторов**: появились нейросети, которые могут по общему запросу создавать целые сцены или видео (*сценарий плюс визуальный ряд*). К примеру, модель **Phenaki (Google)** обещает генерировать длительные видео по тексту, а **Runway Gen-2 (2023)** уже позволяет краткие видеоролики.

Генеративное CV ставит и вызовы: отслеживание синтетического контента (упомянутая задача детекции дипфейков), этические вопросы (авторские права на картинки, DeepFake-политика). Тем не менее, прогресс не замедляется – **диффузионные модели увеличили своё присутствие в исследованиях CV в 3 раза за год**. Можно ожидать, что 2024–2025 принесут ещё более качественные, управляемые генеративные модели (например, **ControlNet (2023)** уже позволяет задавать эскиз или позу для диффузионной модели). Генерация изображений из разряда футуристической забавы перешла в утилитарный инструмент, пронизывающий многие творческие и промышленные процессы.

### Робототехника

**Роботы и дроны** полагаются на компьютерное зрение для восприятия мира. С развитием CV роботы становятся более «видящими» и способными к сложным действиям. К 2024 году мы наблюдаем сближение CV и робототехники: отмечается, что *тесная интеграция ИИ и роботов, с CV в центральной роли, приведёт к росту производительности на 20–30% во многих отраслях*.

Примеры: на производстве **робот-манипулятор с камерой** способен видеть разбросанные объекты и выбирать, какой схватить – технологии **pick-and-place** теперь включают детекцию и сегментацию в реальном времени. В агросекторе **дроны** с камерой летают над полями, используя CV для выявления сорняков, оценки всходов, поиска больных растений (точное земледелие, см. примеры John Deere и FarmWise выше). В складе **роботы-курьеры** ориентируются среди стеллажей, распознавая дорожки и ящики. Гуманоидные роботы (например, **Atlas от Boston Dynamics**) применяют CV для удержания равновесия и взаимодействия с предметами – недавно показано, как Atlas на основе зрительного образа планирует брать и переносить объекты.

В сфере обслуживания – появляются **роботы-уборщики, официанты**, которые используют камеры для навигации и обнаружения людей, чтобы объезжать их и реагировать. **Автономные дроны** благодаря CV умеют облетать препятствия в сложных средах (леса, помещения) без GPS. В армейских технологиях – CV помогает беспилотным машинам и роботам работать на пересеченной местности, распознавать цели.

Наконец, CV всё больше используется в **интерактивной робототехнике**: робот, взаимодействующий с человеком, должен считывать жесты и мимику – а это распознавание позы тела, рук, лица. Такие системы (см. проекты роботов-ассистентов) включают блоки компьютерного зрения для понимания невербальных команд.

Главные тренды CV в робототехнике – улучшение *3D-зрения* (камеры глубины, стерео, лидарные данные вкупе с CV), *обучение в симуляции* (виртуальные окружения с генеративной графикой, где роботы «учатся видеть» прежде чем выйти в реальный мир) и интеграция с *когнитивными архитектурами* (большие модели, типа PaLM-E от Google, соединяющие язык, зрение и действия робота). Все это должно сделать роботов более автономными, способными к решению разнообразных задач в непредвиденной обстановке – что и является целью на ближайшие годы.

### Прочие области

Помимо перечисленного, **компьютерное зрение проникает практически во все сферы**. В промышленности CV контролирует качество продукции на конвейере (автоматический поиск дефектов, например, трещин или неправильной сборки – внедрено на заводах Siemens, BMW и др.). В энергетике – мониторинг трубопроводов, ЛЭП с помощью дронов, распознающих повреждения. В сельском хозяйстве – мы упомянули дроны и сорняки; также CV применяется на фермах (контроль состояния скота по изображениям), в рыбных хозяйствах (подсчет рыбы подводными камерами). В спорте – системы Hawk-Eye и подобные используют CV для анализа траектории мяча, определения положения игроков на поле, подсчета статистики автоматически. В развлекательных приложениях – фильтры дополненной реальности (меняющие внешность в Snapchat, Instagram) основываются на детекции лиц и наложении эффектов.

Даже в таких областях, как **экология и климат**, CV находит применение: анализ спутниковых и аэроснимков для слежения за вырубкой лесов, таянием льдов, подсчета популяции животных. В сфере **безбарьерной среды** – приложения для слепых, где CV описывает голосом окружающую сцену, распознаёт текст и знаки (см. приложения Microsoft Seeing AI). Можно перечислять еще долго – CV уже настолько универсален, что входит в топ-3 технологий ИИ, драйвящих изменения. Ожидается, что с развитием аппаратного обеспечения (доступные камеры, чипы для нейросетей) и появлением открытых моделей, всё больше узкоспециализированных задач будут решаться средствами компьютерного зрения.

## Основные достижения и тренды 2024–2025 годов

Рассмотрев техники и области применения, выделим **ключевые тенденции** и **важнейшие достижения** в CV за рассматриваемый период:

* **Распространение генеративного CV**. 2023 год стал годом популяризации *генеративных моделей для изображений*, и эта волна продолжает расти в 2024–2025. Инструменты на базе Stable Diffusion и др. стали массовыми (например, встроены в графические редакторы, как Adobe Firefly). Большой прорыв – **диффузионные модели** вытеснили прошлые методы генерации, установив новый стандарт качества (см. работу Dhariwal & Nichol *«Diffusion Models Beat GANs»*). Сгенерированные изображения достигли такой фотореалистичности, что порой их сложно отличить от реальных – в связи с чем усилились работы по детекции дипфейков. Тренд на *синтез данных* тоже важен: многие компании начали использовать генерацию для увеличения обучающих выборок там, где мало данных (особенно в медицине, автоматизации). В 2024 вышли усовершенствования диффузионных моделей – например, **ControlNet (2023)** для управляемой генерации, **Consistent Diffusion** для ускорения вывода, **SDXL (Stable Diffusion XL, 2023)** для изображений сверхвысокого разрешения. Ожидается, что к 2025 появятся более легковесные и быстрые генеративные модели, возможно, объединенные с LLM, чтобы генерировать целые мультимодальные сцены (текст+визуал).

* **Доминирование трансформеров в CV**. Как обсуждалось, **Vision Transformers** стали основным выбором для многих задач, особенно где важен глобальный контекст. Если в 2019–2020 гг. CNN были безальтернативны, то к 2024 на ряде бенчмарков трансформеры лидируют. Например, в **ImageNet-классификации** лучшие результат достигают модели на основе трансформеров (включая *CoCa (2022)* – мультимодальную модель от Google). В **детекции** – архитектуры на базе самовнимания (DETR и производные, Swin-Transformer в Mask R-CNN и др.) догнали и превзошли класcические (RetinaNet, Faster R-CNN). В **сегментации** – появились специализированные трансформерные сегментаторы (MaskFormer, Segmenter). При этом CNN не исчезают: интересный тренд – *конвергенция архитектур*. Например, архитектура **ConvNeXt (2022)** показала, что модифицировав CNN (более глубокие, больший receptive field, упрощение стем-слоя) можно достичь качества трансформеров. Таким образом, лучшие модели 2024 года часто используют *гибридные подходы* – берут лучшее от CNN (эффективность сверток) и от ViT (внимание на глобальные патчи).

* **Универсальные фундаментальные модели зрения**. Следуя за NLP (GPT-3, BERT и пр.), CV-сообщество пришло к концепции **Foundation Models** – больших предобученных моделей, применимых к множеству задач. В 2023–2024 появились такие модели: **SAM (Segment Anything)** от Meta – универсальный сегментатор (датасет SA-1B из *1,1 млрд масок* стал крупнейшим для обучения сегментации); **DINOv2** – уже упомянутый self-supervised ViT от Meta, порождающий многоцелевые эмбеддинги; **CLIP/OpenCLIP** – дающий мультимодальные представления для поиска и классификации; **Florence** (Microsoft, 2021) – 900-млн параметр. модель, обученная на 900 млн пар изображение-текст; **SegGPT (2023)** – модель от Huawei, способная сегментировать все и вся как GPT в тексте. Тренд в том, что CV-модели растут в размере и обучаются на огромных наборах данных без строгой привязки к одной задаче, после чего *тонко настраиваются* или *подсказываются* (prompt) под задачу. Это привело к улучшению показателей на множестве бенчмарков сразу. Например, **Segment Anything** смог выделять объекты, которых не было в обучающей выборке – т.е. выполняет *open-set сегментацию*. **DINOv2** показал, что полностью самонаблюдаемая модель может достичь *98% точности определителя глубины* и высоких результатов в классификации признаков без единой метки. Ожидаемый тренд – появление кросс-модальных универсальных моделей (как **PaLM-E (Google, 2023)** – сочетание текста, зрения и команд роботу в одной модели).

* **Рост мультимодальности и интеграции с LLM**. Отдельно подчеркнём тренд соединения CV с большими языковыми моделями. В 2024 мы увидели первые сильные результаты: **GPT-4** смог снимать информацию с изображений; открытые модели типа **LLaVA** (Large Language and Vision Assistant) объединяют CLIP-визуальный энкодер с LLM, позволяя проводить диалоги по картинкам. Число работ по Vision+Language на ведущих конференциях CV удвоилось. Появился термин **Vision-Language Models (VLM)** и даже **VLMo** (универсальная мультимодальная основа). Это указывает на тренд: CV-модели больше не изолированы – они становятся частью комплексных ИИ-систем, умеющих и видеть, и читать, и генерировать текст. Практически это выражается в продуктах: поисковики начали поддерживать визуальный поиск, голосовые ассистенты – описывать картинку с камеры, и т.п. В научном плане – формируется направление **Vision-LLM**, где пытаются «научить GPT думать о зрительных данных». Это, по сути, шаг к более **общему ИИ**, сочетающему разные виды восприятия.

* **Edge AI и оптимизация моделей**. Параллельно с ростом размеров моделей идёт и обратный тренд – делать CV-модели лёгкими и пригодными для *встраиваемых устройств*. Многие приложения (дроны, камеры наблюдения, смартфоны) требуют выполнения CV-алгоритмов локально, с минимальной задержкой и без отправки данных в облако (ради приватности). Поэтому 2024 год принёс много работ по *квантованию* нейросетей (до INT8 и ниже), по сжатым архитектурам (MobileNetv3, EfficientDet) и специализированным ускорителям (NVIDIA Jetson Orin, Google Coral, Apple Neural Engine). **Edge Computing** вообще – важный тренд: вычисления переносятся ближе к источнику данных, что в CV означает анализ прямо на камере или локальном сервере. Например, производители камер (Hikvision, Dahua) стали встраивать чипы с нейросетями в сами камеры, чтобы те сразу могли детектировать движение или людей без постоянного видеопотока. Это снижает требование к полосе пропускания и повышает приватность (передаются уже события, а не сырое видео). Прогресс фреймворков (OpenVINO, TensorRT, CoreML) позволяет запускать сложные модели (даже трансформеры) на edge-девайсах в реальном времени. **Реальный пример 2025**: умная камера в магазине на чипе Hailo может в 30 FPS определять 80 классов объектов по модели, натренированной на COCO, и передавать только распознанные метаданные.

* **Explainable AI и этика**. С ростом применения CV в критических областях (медицина, автономное вождение, правоприменение) усилился запрос на **интерпретируемость**. 2024 год ознаменован увеличением инвестиций в XAI для CV: разрабатываются методы визуализации внимания моделей, генерации текстовых объяснений тому, что распознала сеть (например, зачатки этого – модели, которые могут не только классифицировать изображение, но и обосновать классификацию текстом). Также компании и регуляторы фокусируются на **этике CV**: избегании **смещения и дискриминации** (известно, что модели распознавания лиц могут работать хуже на некоторых расовых группах – это активно исследуется и исправляется в новых датасетах); защите данных (появляются датасеты с “нейтральной” лицензией, оповещения об использовании данных из интернета и т.д.); приватности (методы деперсонализации изображений). Европейский союз готовит **AI Act**, который затронет и применение CV (например, возможно, потребуется сертификация для систем, принимающих значимые решения на основе видео). В ответ на это, компании внедряют *Privacy AI* – например, удалённое хранение биометрических данных, шифрование на уровне камеры и т.п. Этот тренд более социальный, но он влияет и на техническое развитие: так, одним из драйверов self-supervised learning стало желание меньше зависеть от вручную размеченных человеком данных (которые могут содержать предвзятости).

* **Открытость исследований и данных**. Сообщество CV традиционно открытое (выпуски OpenCV, общедоступные бенчмарки). 2024 год усилил эту тенденцию: Meta, Google, Microsoft, NVIDIA и др. выложили в открытый доступ ряд мощных моделей и *гигантских датасетов*. Например, **LAION-5B** – набор из **5,85 млрд пар «изображение-текст»** из интернета, крупнейший открытый мультимодальный датасет; **SA-1B** (Segment Anything Dataset) – 11 млн изображений и 1,1 млрд масок – беспрецедентный по объёму для сегментации; **Open Images V6** от Google – \~9 млн изображений с аннотациями (больше и богаче, чем COCO). Это привело к тому, что независимые исследователи и небольшие команды получили доступ к материалам для обучения своих моделей, а значит – ускорилось развитие по всему миру, а не только в крупных компаниях. Также появляются *платформы для совместной работы* над CV: например, **Hugging Face Hub** стал популярным местом публикации моделей CV (ViT, DETR, Stable Diffusion и тысячи других доступны для скачивания). В 2024 г. Hugging Face совместно с CVPR даже выпустили архив популярных моделей.

* **Стремительный рост сообщества CV**. Конференции CVPR, ICCV бьют рекорды по числу отправленных и принятых статей. CVPR 2024 приняла 2719 статей (23,6% из 11532 заявок) – на 25% больше, чем годом ранее. В эту область приходят всё новые исследователи, часто – с бэкграундом в смежных областях ИИ, принося новые идеи. География тоже расширяется: по данным CVPR 2024, почти 70% публикаций приходятся на организации из США и Китая, но на втором плане усиливаются Германия, Сингапур, Южная Корея, Великобритания, Швейцария и др.. Растёт и междисциплинарность: CV всё чаще сочетается с роботикой, медиа, дизайном, что порождает новые направления (например, workshops по **Vision+Graphics**, **Vision+Language** и т.д.).

Подводя итог, **основные тренды 2024–2025** в CV: генеративные модели и мультимодальность, господство трансформеров и крупных предобученных моделей, продолжение улучшения классических задач (детекция/сегментация) за счёт данных и вычислений, и расширение применения CV на устройствах и в ответственных приложениях. Сообщество активно решает новые вызовы, и ближайшие годы обещают ещё более интегрированные и мощные системы зрения для ИИ.

## Заметные научные статьи, модели, проекты и датасеты

Ниже перечислим некоторые наиболее влиятельные работы, открытые проекты и наборы данных, появившиеся или получившие развитие в последнее время:

* **Segment Anything Model (SAM)** – большая модель сегментации от Meta AI (2023) и связанный датасет **SA-1B**. *Статья*: Kirillov et al., *Segment Anything* (arXiv 2023) – представляет новую задачу и модель для универсальной сегментации объектов без обучения на конкретные классы. SAM умеет по простому prompt’у (точка или рамка) выделять объект на любом изображении. *Код и модель* открыты (GitHub: `facebookresearch/segment-anything`), датасет из 1 млрд масок – открытый. SAM стала базой для множества приложений (от интерактивного редактирования изображений до 3D-сегментации с NeRF).

* **DINOv2** – self-supervised Vision Transformer от Meta (2023). *Статья*: Oquab et al., *DINOv2: Learning Robust Visual Features without Supervision* (arXiv 2023). Декларируется, что **DINOv2 – первый метод, использующий самонаблюдаемое обучение для достижения результатов, сравнимых с лучшими супервизованными моделями CV**. DINOv2 модели (от маленьких до гигантских ViT-G) доступны открыто, их признаки показали SOTA в ряде задач без дообучения. Проект открыт на GitHub (`facebookresearch/dinov2`). Это значимый шаг к foundation models в CV.

* **Vision Transformer (ViT)** – хотя оригинальная статья Google (Dosovitskiy et al.) вышла в 2020, в 2024 ViT и ее производные остаются крайне цитируемыми. За эти годы появилось множество улучшений ViT: *DeiT (2021)* – показала, что ViT можно обучить на доступных данных (ImageNet-1k) с результатом, превзошедшим ResNet; *Swin Transformer (Liu et al. 2021)* – ввёл идею сдвигаемого окна, улучшив ViT для детекции/сегментации; *ConvNeXt (2022)* – переразмыслил ResNet в стиле трансформеров. Эту серию работ можно считать определяющими для архитектур CV. К 2025 рынок архитектур движется в русле, заложенном этими статьями.

* **YOLO family** – серия одношаговых детекторов, начатая работой Redmon et al. (2015). Последние заметные релизы: *YOLOv7* (Wang C. et al., 2022) – тогда SOTA в скорости и точности детекции; *YOLOv8* (Ultralytics, 2023) – поддерживает не только детекцию, но и сегментацию, позу, классификацию в одном пакете. YOLO — также открытый проект (GitHub: ultralytics/YOLO). В 2025 анонсирована дальнейшая версия (условно YOLO11), показывающая ещё более высокую точность на COCO при меньшем числе параметров. YOLO сыграл огромную роль в практическом внедрении CV, поэтому его эволюция – одно из важных явлений.

* **Detectron2 и OpenMMLab** – популярные открытые экосистемы для CV. Detectron2 (Facebook AI) – открытая в 2019 платформа на PyTorch для детекции и сегментации, включающая реализации Mask R-CNN, RetinaNet, Faster R-CNN, паноптической сегментации и др. OpenMMLab (от Shanghai AI Lab) – серия библиотек (MMDetection, MMSegmentation, MMTracking и др.), также предоставляющих огромное число моделей CV. Обе экосистемы активно обновляются под новые SOTA модели и широко используются исследователями и инженерами.

* **OpenCV library** – классическая C++/Python библиотека, развиваемая с 2000 года. Хотя она не о глубоком обучении, OpenCV остаётся востребованной: для базовой обработки изображений, реализации традиционных алгоритмов (ORB, SIFT, оптический поток и т.д.). В 2024 году вышли версии OpenCV 4.8–4.9 с улучшенной поддержкой DNN (модели ONNX, backend OpenVINO и др.), а к 2025 ожидается релиз OpenCV 5. OpenCV – основа для быстрой разработки CV-прототипов, и новые версии учитывают тренды (например, поддержка 0D/1D тензоров для CV задач типа аудио).

* **LAION-5B** – уже упомянутый *гигантский датасет* для обучения мультимодальных моделей, выпущенный некоммерческой организацией LAION (2022). Это **5,85 млрд пар изображение-текст**, добытых из Common Crawl, отфильтрованных моделью CLIP. *Статья*: Schuhmann et al., *LAION-5B: An open large-scale dataset for training next generation multimodal models* (NeurIPS 2022) – обосновывает создание LAION-5B для демократизации исследований мультимодальных моделей. LAION-5B свободно доступен для скачивания (570 ТБ данных). Он уже стал основой для обучения множества открытых моделей: Stable Diffusion обучен на подмножестве LAION, BloomClip и OpenCLIP – тоже. Это событие укрепило открытое CV-сообщество.

* **COCO** – *Common Objects in Context*, ключевой датасет для детекции и сегментации. Хоть он опубликован еще в 2014 (Microsoft), он до сих пор стандарт де-факто для оценки алгоритмов. COCO содержит \~330 000 изображений (из них \~200 000 размечены) с **1,5 млн объектов, 80 категорий** и аннотации для задач: детекция объектов, сегментация **каждого экземпляра**, распознавание *«статических»* объектов (stuff), позы людей и описание изображений. В 2020-х на базе COCO проводятся ежегодные соревнования (Kaggle, CVPR challenges), и SOTA результаты не стоят на месте. Например, на детекции (mAP) лучший результат 2023 \~65 mAP (модель DetectGPT от Naver), что впечатляет по сравнению с \~40 mAP у Mask R-CNN в 2017. COCO-денди (лидерборды) стимулирует развитие новых моделей и остается актуальным.

* **ImageNet** – знаменитый датасет классификации (Deng et al., 2009) – 14 млн изображений, 1000 классов (подмножество). Он послужил «Олимпом» для сверточных сетей и до сих пор используется для pre-training. Сейчас точность топ-1 на ImageNet достигла \~90%+, превосходя человеческий уровень \~85%. Многие считают задачу решённой; тем не менее, *ImageNet-21k* (полная версия на 21 841 класс) и связанный новый датасет **JFT-3B** (Google, 2022 – 3 млрд изображений, недоступен публике) прокладывают путь к ещё более универсальным моделям. Из открытых аналогов – **Open Images (Google)**: \~9 миллионов изображений, 600 классов объектов, аннотации детекции, сегментации и визуальных отношений. Также упомянем специализированные датасеты: **Cityscapes** (5k уличных фото с пиксельной сегментацией города), **KITTI** и **nuScenes** (автономное вождение), **MPII Human Pose** (для позы человека), **VisDrone** (аэровидео), **ADE20K** (сегментация различных сцен), **LVIS** (расширение COCO с 1000+ классами). Все они продвигают определённые направления CV.

* **Hugging Face Transformers/Diffusers** – открытые инструменты, облегчающие работу с моделями CV. Библиотека 🤗Transformers изначально для NLP, теперь включает и модели CV: Vision Transformer, Swin, BEiT, CLIP, DETR, SegmentAnything и многие другие доступны в несколько строк кода. Также библиотека 🤗Diffusers содержит реализации десятков диффузионных моделей (StableDiffusion, Glide, DALLE2 decoder и т.д.) и удобные pipeline для их использования и дообучения. Эти проекты стали популярны благодаря активному сообществу и модели распространения через **Model Hub** – репозиторий, где можно найти тысячи предобученных моделей CV. Например, на Hub выложены весы Stable Diffusion, Segment Anything, DETR, StyleGAN и многое другое. Это значительно ускоряет воспроизведение результатов и применение их в реальных проектах.

* **PyTorch и TensorFlow** – два ведущих фреймворка DL, которые продолжают развиваться. *PyTorch* (Meta) в 2022–2023 выпустил версии 1.13–2.0, внедрив компиляцию моделей (TorchCompile) и улучшения производительности. *TensorFlow* (Google) хоть и потерял доминирование в исследовательском CV, остаётся широко используем в продакшене, особенно с экосистемой TFX, TensorRT и TF-Lite для оптимизации и деплоя. Отмечается, что **в академическом сообществе PyTorch стал предпочтительным за счёт простоты и гибкости**, и большинство свежих CV-моделей публикуются с PyTorch-кодом. Однако TensorFlow 2 с Eager execution сблизился в удобстве, а для мобайла и веба (TensorFlow Lite, TF.js) у него преимуществ больше. Оба фреймворка – с открытым исходным кодом, и конкуренция между ними стимулирует развитие. Также набирает популярность *JAX* (Google) – особенно в исследованиях, требующих быстрой компиляции на TPU, но порог входа выше.

* **Прочие открытые проекты**:

  * **OpenCLIP** – открытая реплика модели CLIP, обученная LAION, позволяет всем желающим получить мультимодальные эмбеддинги без доступа к проприетарному OpenAI CLIP.
  * **MidJourney/StableDiffusion WebUI** – сообщество вокруг генерации изображений создало множество удобных интерфейсов, расширений (например, Automatic1111 WebUI для StableDiffusion, имеющий сотни вкладок от ControlNet до текстурирования 3D).
  * **MediaPipe** (Google) – фреймворк с готовыми CV-пайплайнами реального времени (детекция рук, лица, позы), доступный разработчикам мобильных приложений.
  * **Roboflow** – платформа для управления своими датасетами CV, аннотаций и даже тренировки моделей в облаке, популярна у инженеров.
  * **Ultralytics HUB** – платформа от создателей YOLO, упрощающая обучение и развертывание моделей детекции (например, той же YOLOv8) через веб-интерфейс.
  * **NVIDIA Maxine и Omniverse** – SDK от NVIDIA, связанные с CV: Maxine – для видео-конференций (фильтрация видео, отслеживание взгляда, аватары), Omniverse – для симуляций и рендеринга (с применением CV для доменадаптации и синтеза данных).
  * **Academic Toolkits**: timm (PyTorch Image Models) – библиотека со множеством готовых архитеткур CV; mmcv – утилиты от OpenMMLab; Kornia – дифференцируемая библиотека CV на PyTorch (для классических трансформаций, фильтров).
  * **Deep Augmentation**: Albumentations – открытая библиотека фото-augmentations, широко используемая для подготовки данных, поддерживает основные и продвинутые методы (MixUp, GridDistortion и т.п.).

Конечно, перечислить все значимые проекты невозможно – экосистема CV чрезвычайно богата. Мы отметили лишь некоторые, получившие широкое распространение или представляющие новый шаг в развитии области.

## Сравнение моделей и задач компьютерного зрения

Различные архитектуры моделей лучше подходят для разных задач CV. В следующей таблице суммировано, **какие типы моделей успешно применяются для ключевых задач**:

| **Тип модели**                        | **Классификация**                                                                                                                                                | **Детекция объектов**                                                                                                                                                                 | **Сегментация**                                                                                                                                                             | **Генерация изображений**                                                                                                                                                                                                |
| ------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **CNN (сверточные сети)**             | ✔️ Основа с 2012 г. (AlexNet, VGG, ResNet). SOTA до 2020 г., сейчас – база многих решений.                                                                       | ✔️ Да (используются в двухшаговых и одношаговых детекторах: Fast/Faster R-CNN, YOLO, SSD и пр.)                                                                                       | ✔️ Да (FCN, U-Net, Mask R-CNN используют CNN-энкодер; сегментация хорошо решается CNN с skip-слоями)                                                                        | ✔️ Частично (Генеративные сети GAN строятся на CNN-архитектурах; например, StyleGAN – чистый CNN)                                                                                                                        |
| **Vision Transformers (ViT)**         | ✔️ Да (ViT, Swin Transformer достигли SOTA на ImageNet; крупные ViT превосходят ResNet)                                                                          | ✔️ Да (Transfomer-детекторы: DETR, Deformable DETR; либо гибриды – Swin-Transformer + Region Head, показывают топ-результаты)                                                         | ✔️ Да (Transformer-энкодеры в Segmenter, Mask2Former обеспечивают высокую точность сегментации)                                                                             | ⚬ Возможно (Для чистой генерации трансформеры реже используются, но есть примеры: VQGAN+Transformer как декадер в DALL-E; также текущее поколение diffusion-генераторов имеет трансформерные компоненты)                 |
| **Диффузионные модели**               | ❌ Нет (не применяются для классификации напрямую; слишком медленные, их область – генеративные задачи)                                                           | ❌ Нет (для детекции не используются)                                                                                                                                                  | ⚬ Эксперименты (иногда применяются для генерации сегментационных масок, но не основной метод)                                                                               | ✔️ **Да, основной метод** (Diffusion – SOTA подход для синтеза изображений; примеры: Stable Diffusion, Imagen, DALL-E 2 decoder)                                                                                         |
| **Генеративные модели (GAN, VAE)**    | ❌ Нет (не используются для классификации)                                                                                                                        | ❌ Нет                                                                                                                                                                                 | ❌ Нет                                                                                                                                                                       | ✔️ Да (GAN ранее были SOTA для генерации лиц, вещей; VAEs для контрольной генерации. Сейчас уступают диффузионным, но всё еще применяются для видео, 3D и др.)                                                           |
| **Мультимодальные (Vision-Language)** | ⚬ Частично (Модели типа CLIP могут выполнять *zero-shot* классификацию: по подписи определить класс изображения, хотя напрямую не обучены под конкретные классы) | ⚬ Да, в открытой постановке (CLIP+Region может находить по текстовому описанию объекты – *open-vocabulary detection*; есть модели типа GLIP, OWL-ViT для совмещения языка и детекции) | ⚬ Да, опосредованно (Напрямую мультимод. модель не выдает маску, но возможно сегментировать через текстовые подсказки: например, SAM + CLIPSEG для подписанных сегментаций) | ✔️ **Да** (Современные генераторы изображений – мультимодальные: принимают текст + генерируют картинку, как DALL-E, Stable Diffusion. Кроме того, есть модели генерирующие текстовые описания по изображению и наоборот) |
| **Специализированные (пример)**       | **Capsule Networks** (2000-е) – непопулярны сейчас                                                                                                               | **Point Cloud Networks** (детекция 3D LiDAR-точек, напр. PointNet)                                                                                                                    | **Графовые сети** (реконструкция сцен, Scene Graphs)                                                                                                                        | **NeRF** (генерация новых видов 3D-сцены по 2D)                                                                                                                                                                          |

<small>✔️ – модель является основой или показывает выдающиеся результаты в данной задаче; ⚬ – возможно применение или вспомогательная роль; ❌ – обычно не применяется.</small>

Несмотря на условность такой схемы, видно, что для *дискриминативных задач* (классификация, детекция, сегментация) основную роль играют CNN и Transformers, тогда как для *генеративных задач* – диффузионные модели и GAN. Мультимодальные модели выступают как универсальные, решая новые постановки (zero-shot, open-set задачи), но часто состоят из комбинаций вышеперечисленных архитектур (например, CLIP включает ViT + текстовый трансформер). В реальных системах нередко используются гибридные подходы: например, **Mask R-CNN** объединяет CNN-экстрактор признаков для детекции *и* сегментации; современные детекторы могут включать CNN-«хвост» на трансформерном «теле» и т.п.

Важно: **выбор модели зависит не только от точности, но и от требований по скорости и вычислителям**. Встроенные системы часто берут облегченную CNN, а не тяжёлый трансформер. Если нужна максимальная точность на изображениях высокого разрешения – применяют большие ViT. Для сжатия моделей и вывода на мобильных устройствах используют прунинг, квантование – это затрагивает все типы архитектур.

## Ведущие исследовательские группы и компании в CV

Развитие компьютерного зрения – заслуга и академических групп, и индустриальных лабораторий по всему миру. Отметим некоторых лидеров:

* **Технологические гиганты (R\&D)**:

  * **Google / Google DeepMind** – один из безусловных лидеров. Google представила многие революционные идеи: архитектуры (Inception, ViT), алгоритмы (RCNN-семейство частично выходцы из Google/DeepMind), огромные датасеты (Open Images), приложения (автономное вождение Waymo). По статистике CVPR 2024, Google – ведущая компания с >50 статей. После слияния с DeepMind в 2023, их объединённая команда фокусируется на фундаментальных моделях (пример: публикации по Parti, Imagen, Muse для генерации изображений, MetNet для метеорологии и т.д.). **Google AI** также известна продуктами: Google Lens (распознавание всего через камеру смартфона), ARCore, и использованием CV в YouTube (контроль контента) и поиске.
  * **Meta (Facebook) AI** – команда FAIR известна вкладом в CV: **ResNet**, Mask R-CNN, Detectron, Segment Anything – все это создали или соавторствовали исследователи Meta. В 2024 Meta активно работает над сегментацией и self-supervised (DINOv2), multimodal (вообще, название “Язык и Зрение” – часть их исследований, например, FLAVA). По публикациям Meta – одна из топ (35 статей на CVPR’24). Кроме исследований, Meta применяет CV внутри своих сервисов: от модерации контента в Facebook/Instagram (поиск запрещённых изображений) до функций в Portal, VR (Oculus) и AR-очках (RayBan Stories).
  * **Microsoft Research** – исторически сильно в CV (еще в 2015 их ResNet выиграл ImageNet). MSR, особенно отделение Asia, публиковало такие работы, как DualGAN, Swin Transformer, BEiT, ZeroShot-Det. Microsoft в 2023 анонсировала **Kosmos-1**, **Florence** – большие мультимодальные модели. Кроме того, Microsoft вложилась в OpenAI, интегрируя CV-возможности (например, GPT-4 Vision) в свои продукты (новый Bing с поиском по картинкам, Azure Cognitive Services с готовыми CV-модулями). В индустрии Microsoft также предлагает **HoloLens** (AR-очки) – продукт, требующий передовых CV (смотрят в реальном времени на жесты, пространственную карту).
  * **OpenAI** – фокусируется не только на тексте: их работа **CLIP (2021)** стала поворотной для CV, а **DALL-E** (2021) – для генерации изображений. В 2023 OpenAI представила мультимодальный GPT-4. Хотя OpenAI – относительно небольшая по числу CV-публикаций организация (их подход скорее создавать закрытые большие модели), влияние её разработок на область огромное. CLIP породил целое направление zero-shot CV, DALL-E показал потенциал трансформеров для картинки.
  * **NVIDIA** – помимо выпуска GPU, имеет сильную исследовательскую группу (NVIDIA Research), специализирующуюся на графических алгоритмах и CV. **StyleGAN (2018-2020)** – серия их работ, задавшая уровень фотореализма для GAN. Они же развивают **Omniverse** платформу (для синтетических данных и симуляций), **Maxine** (коммуникации). В 2024 NVIDIA активно исследует NeRF (Instant NeRF в 2022), диффузионные модели ускоренные (работа по “Diffusion distillation” для SDXL). NVIDIA также помогает разработчикам CV: выпуская библиотеки (CUDA, cuDNN – ускорители под капотом PyTorch/TF; TensorRT – оптимизация), инструменты как **TAO Toolkit** (Train, Adapt, Optimize – для обучения CV-моделей без глубоких знаний).
  * **Apple** – менее публична, но значима. Apple разрабатывает CV для своих устройств: FaceID (глубинная камера + CV для лица), системы двух камер (портретный режим – разделение переднего/заднего плана), ARKit (фреймворк дополненной реальности на iOS), приложение «Лупа» для слабовидящих. В 2024 Apple анонсировала **Vision Pro** – XR-гарнитуру с множеством камер, где CV отвечает за жестовое управление и смешанную реальность. Исследования Apple часто не публикуются открыто, но известно о работах над приватным обучением в CV (чтобы модели можно было обучать на устройстве) и о сильной команде, нанявшей многих известных учёных CV (включая авторов RCNN, Transfuser и др.).
  * **Другие**: **Tesla** – важна как компания, продвигающая *vision-only* подход в автопилоте, они создали суперкомпьютер Dojo для обучения сетей на видео. **Amazon** – использует CV в логистике (склады Amazon Robotics), в ритейле (упомянутые Amazon Go), также AWS предоставляет услуги CV (Rekognition). **IBM, Intel, Adobe** – также имеют исследовательские подразделения: IBM Research (реже про CV, больше про AI вообще), Intel (поддерживает OpenCV, выпустила множество статей по 3D-сканированию, SLAM и пр.), Adobe (сфокусирована на CV для творчества: алгоритмы контент-аварного заполнения, умной обрезки, их проект Adobe Sensei включает ИИ-функции в Photoshop/Premiere).

* **Компании, специализирующиеся на CV**:

  * **SenseTime** – китайская компания, один из крупнейших AI-стартапов, фокус CV: системы наблюдения, распознавание лиц, городские системы безопасности. Активно публикуется: от них вышли работы по детекции (например, модели ExtremeNet), по видеоразметке, они также делали крупнейший в мире фейс-датасет MS1M.
  * **Megvii (Face++)** – тоже из Китая, известна технологиями face recognition, победами в соревнованиях на лицах (MegaFace). Публиковала работу **ShuffleNet** (мобильная CNN).
  * **Clearview AI** – американская спорная компания, собравшая 3 млрд фото из интернета для распознавания лиц, используется полицией. Хоть не публикуется, но известна как символ применения CV в безопасности.
  * **OpenCV.org** – организация, поддерживающая одноименную библиотеку, проводит курсы, конкурсы (OpenCV AI Competition), разрабатывает недорогое железо для CV (OpenCV AI Kit – камеры DepthAI). Своей R\&D как у BigTech нет, но огромная роль в экосистеме.
  * **Широкий круг стартапов**: их очень много – от систем для медицины (**Caption Health** – УЗИ с AI, **Zebra Medical** – рентген) до автотранспорта (кроме Tesla – **Cruise, Mobileye, Pony.ai, Comma.ai**), ритейла (**Standard Cognition, Trigo** – магазины без касс), промышленности (**Landing AI** – Эндрю Ын, CV для производства), агро (**John Deere** купила стартап Blue River Tech для CV-систем прополки), и т.д. Есть даже стартапы по CV для моды (анализ трендов по фото из соцсетей), для спорта (анализ матчей).
  * **Компании-агрегаторы**: которые не разрабатывают CV сами, но предоставляют сервисы – напр. **SuperAnnotate, LabelBox** (помощь в разметке данных CV), **Scale AI** (аутсорс разметки), **Roboflow** (уже упоминалось – платформа управления датасетами).

* **Академические группы и университеты**:

  * **Университеты Китая** сейчас выходят на лидирующие позиции по публикациям. В 2024 году рекорд по числу статей на CVPR принадлежит **Университету Цинхуа** – 88 принятых статей. Также сильны **Пекинский университет**, **Шанхайский AI Lab**, **CAS (Академия наук КНР)**. Это результат государственной поддержки: крупные центры наподобие Beijing AI Academy ведут много прикладных CV-проектов.
  * **США и Канада**: исторические лидеры – **Stanford (Stanford Vision Lab)** под руководством Фэй-Фэй Ли – создатели ImageNet, продолжают работать над 3D-сценами, робототехникой (совм. с NVIDIA). **MIT CSAIL** – известные профессора Антонио Торралба, Билл Фримен, используют CV в смежных с человеческим зрением исследованиях (компьютерная фотометрия, когнитивные науки). **UC Berkeley (BAIR)** – один из мировых центров CV, чьи представители (Тревор Дэррелл, Джитендра Малик) внесли огромный вклад: работы по сегментации (Mask R-CNN – К. Хе ученик Малика), детекции, синтезу. **CMU (Carnegie Mellon)** – сильна в стереозрении, 3D, трекинге, и особенно в сочетании с робототехникой (там же находился и Навлаб для автономных авто). **UC San Diego, University of Washington, Columbia, Princeton, Georgia Tech** – все имеют активные CV-группы. **Toronto, Montreal** – в Канаде сочетание с DL (Geoff Hinton), хотя больше по концептуальным DL, но есть и прикладной CV (например, работа Capsule Networks из Toronto). **NYU** – лаборатория Яна ЛеКуна, откуда вышли первые CNN для распознавания рукописных цифр, продолжает исследования (сейчас фокус на более общем AI, energy-based models и т.д.).
  * **Европа**: **Oxford (Visual Geometry Group)** – авторы VGGNet, и множества работ по распознаванию, один из сильнейших центров CV в Европе. **INRIA, École Polytechnique, ETH Zurich, MPI Tübingen, University of Amsterdam, TU Munich** – известные группы, которые лидируют в темах: мультиобъектный трекинг (ETH, INRIA), 3D-видение (MPI, ETH), действие-видео (Университет Цюриха – Давиде Скарамуцца, дроны). **Российские** группы также участвуют: **Skoltech, МФТИ, ИИЦ МГУ** публикуют на CVPR/ICCV, хотя не столь многочисленно. Индийские и корейские университеты тоже наращивают присутствие (KAIST, IIT).

* **Коллаборации индустрии и академии**: Стоит отметить, что много исследований – плоды совместной работы университетов и компаний. Например, **ResNet** разработан учёными MSR Asia совместно с Университетом Цинхуа. **Detr** – Facebook + INRIA. **CoAtNet** – Google + Berkeley. Такие коллаборации позволяют объединять теоретический и вычислительный ресурсы. По статистике \~27,6% статей CVPR 2024 имели совместных авторов из академии и индустрии, что говорит о крепкой связке.

Summing up, компьютерное зрение – очень динамичная область, за развитием которой стоит коллективный труд огромного числа групп. Лидеры задают ориентиры: Google и Meta активно продвигают open-source и публикуют сильные результаты, китайские организации вкладываются в масштаб исследований, академия приносит новые идеи и талантливые кадры. Конкуренция и сотрудничество одновременно ускоряют прогресс. Мы наблюдаем, что **наука и реализация в CV идут рука об руку**, и прорывы быстро находят применение (как трансформеры из статьи стали через год появляться в продуктах).

На 2025 год можно ожидать дальнейшего роста влияния крупных «foundation» моделей, ещё большего слияния с языковыми технологиями, появление новых больших открытых датасетов (говорят об открытом аналоге ImageNet-21k) и, вероятно, достижение новых высот на классических проблемах (например, близкие к 100% решения в ограниченных сценариях). Компьютерное зрение становится базовым компонентом практически каждого интеллектуального устройства, и понимание его актуальных возможностей и трендов – ключ к инновациям во многих отраслях.

**Источники:** Мы старались опираться на актуальные данные 2024–2025 гг. (см. ссылки). Для углубления рекомендуем обзоры: *“Computer Vision Trends 2025”*, аналитика CVPR 2023/24, и открытые отчёты компаний-лидеров.
