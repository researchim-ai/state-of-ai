# **Применение искусственного интеллекта в формальной математике и доказательстве теорем**

Формальная математика – это направление, где математические утверждения и доказательства записываются на формальных языках, позволяющих их **полностью проверить с помощью компьютера**. Ещё в 1950-х годах появились первые попытки автоматического доказательства теорем на компьютерах. Однако лишь в последние годы, благодаря прорывам в области искусственного интеллекта (особенно *machine learning* и больших языковых моделей, LLM), интерес к применению ИИ в формальной математике резко вырос. Число научных работ на стыке глубинного обучения и доказательства теорем увеличилось с буквально нескольких в 2016 году до десятков в 2023-м, и исследования в этой области активно развиваются по многим направлениям. [1](https://arxiv.org/html/2404.09939v1#:~:text=Proving%20theorems%20is%20a%20cornerstone,these%20advancements%2C%20later%20research%20extended) [2](https://arxiv.org/html/2404.09939v1#:~:text=Exploring%20learning,of%20progress%20and%20indicate%20the) [2](https://arxiv.org/html/2404.09939v1#:~:text=Exploring%20learning,of%20progress%20and%20indicate%20the)

Главная цель использования ИИ в математике – **помочь преодолеть сложность современных доказательств**. Многие важные теоремы имеют чрезвычайно громоздкие доказательства, где не исключены человеческие ошибки. Формальная проверка шаг за шагом гарантирует, что доказательство корректно, но процесс формализации и поиска доказательства часто требует огромных усилий. Здесь на помощь приходят ИИ-системы, которые могут автоматизировать рутинные шаги, проверять корректность и даже **предлагать новые идеи**. Уже есть примеры, когда нейросети помогли открыть *новые математические результаты*, незамеченные ранее людьми. В данном обзоре рассмотрены основные области, где ИИ применяется в формальной математике и доказательстве теорем, современные системы-доказчики (Lean, Coq, Isabelle, HOL Light и др.), а также последние достижения (особенно 2023–2025 годов) – от проектов Google DeepMind (AlphaTensor, AlphaCode, FunSearch, AlphaProof и др.) до инициатив OpenAI и академических групп (MIT, Stanford, Эдинбург и др.). В завершение представлена сводная таблица ключевых проектов с их задачами и ссылками на оригинальные статьи и репозитории. [3](https://deepmind.google/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways/#:~:text=between%20different%20areas%20of%20mathematics,that%20will%20be%20submitted%20to) [4](https://deepmind.google/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways/#:~:text=knot%20tell%20us%20about%20the,By%20using%20attribution)

## **Основные направления применения ИИ**

**Автоматическое доказательство теорем.** Одна из классических задач – *полностью автоматическое* нахождение доказательства заданного формального утверждения. Ещё с 1960-х созданы алгоритмические доказчики (например, системы резолюций для первой порядка логики, такие как **E-Prover** и **Vampire**). Современные ИИ-методы улучшают эти инструменты, применяя обучение для выбора нужных аксиом и шагов. Например, в среде Isabelle механизм *Sledgehammer* интегрирует внешние автоматические доказчики (E, Vampire и др.) и использует машинное обучение для выбора релевантных фактов (*рэфилер MaSh*). В последнее десятилетие появились *нейросетевые доказчики*: системы на основе нейронных сетей генерируют шаги доказательства и ищут доказательство через обучение. Так, модель OpenAI *GPT-f* впервые применяла трансформеры для генерации тактических шагов доказательства, а последующие работы комбинировали языковые модели с *поиском по дереву* или *подкреплением* для повышения эффективности. В 2023 году достигнут заметный прогресс: например, система **AlphaProof** от DeepMind с помощью методов подкрепления научилась решать задачи международной математической олимпиады на уровне призёра – без участия человека находя формальные доказательства сложных задач. [5](https://arxiv.org/html/2404.09939v1#:~:text=ATP%20aims%20to%20verify%20formal,a%20proof%20or%20refutation%20is) [6](https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)#:~:text=Though%20interactive%2C%20Isabelle%20features%20efficient,6) [7](https://arxiv.org/html/2404.09939v1#:~:text=GPT,repair%20the%20whole%20proof%20at) [8](https://arxiv.org/html/2404.09939v1#:~:text=match%20at%20L363%20first%C2%A0,first) [9](https://arxiv.org/html/2404.09939v1#:~:text=match%20at%20L511%202019%3B%20Polu,%282022%29%20further) [10](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/) [11](https://arxiv.org/abs/2504.11354)

Формализация математики. Другая сфера – перевод обычной (неформальной) математики в строгую формальную форму. Здесь ИИ призван помочь математикам в трудоёмком процессе записи существующих теорем и доказательств на языке формальных систем. Задача автоформализации пока далека от решённой: например, перевести произвольную страницу учебника или статьи в код Lean или Coq крайне сложно. Однако начаты перспективные исследования. Одни подходы пытаются использовать LLM, обученные на математических текстах и примерах формальных доказательств, чтобы генерировать формулы и доказательства по описанию на естественном языке [12](https://www.emergentmind.com/topics/autoformalization). Другие работы предлагают разбивать задачу на этапы: сначала распознать структуру доказательства, затем подобрать в библиотеке формальные аналоги утверждений, и наконец сгенерировать проверяемый код доказательства [13](https://arxiv.org/abs/2310.07957) [14](https://openreview.net/forum?id=hUb2At2DsQ). Уже появились прототипы, способные автоматически формализовать простые задачи – например, перевести школьную олимпиадную задачу в формальное утверждение и доказать её с небольшой корректировкой человека. Ожидается, что в ближайшие годы автоформализация будет стремительно развиваться, учитывая успехи больших языковых моделей в понимании математического текста.

Верификация доказательств. Одно из основных преимуществ формальной математики машинная проверка корректности. В традиционной ("бумажной") математике даже опубликованные доказательства иногда содержат ошибки или пробелы. Формальная проверка устраняет человеческий фактор: специальное программное ядро (kernel) подтверждает каждое Роль преобразование. ИИ здесь скорее вспомогательная масштабируемость и удобство проверки. Например, при формализации громоздких доказательств (таких как классификация простых групп или доказательство гипотезы Кеплера) можно использовать автоматические тактики и доказатели для проверки рутинных шагов. Такие инструменты, как **CoqHammer** для Coq, способны сами доказать часть тривиальных лем, переводя их в задачу для внешних автоматических решателей и возвращая проверенные решения [15](https://discourse.rocq-prover.org/t/coqhammer-1-1-1-for-coq-8-9/327) [16](https://popl18.sigplan.org/details/CoqPL-2018/2/CoqHammer-Strong-Automation-for-Program-Verification). Верификация доказательств с помощью ИИ означает также использование машинного обучения для обнаружения потенциальных ошибок или недостающих частей: например, анализ непротиворечивости больших теорий или альтернативная проверка результатов с помощью разных систем (двойная верификация). В итоге, сочетание формальных методов и ИИ обеспечивает беспрецедентный уровень доверия к математическим результатам – любое утверждение, прошедшее формальную проверку, считается доказанным с абсолютной строгостью.

Коллаборация человека и ИИ. На практике наибольших успехов достигает тандем "математик + ИИ", где компьютерные системы помогают, но не заменяют человека. Такие системы называются интерактивными доказателями (proof assistants) – они позволяют пользователю вводить рассуждения на понятном языке, а затем автоматически проверяют и дополняют детали. Современные proof assistant'ы (Lean, Coq, Isabelle и др.) уже включают много встроенной автоматизации, а с помощью ИИ их возможности расширяются ещё больше. Примером успешной коллаборации стала работа DeepMind с университетскими математиками: нейросети были обучены на данных о узлах (объектах в топологии) и подсказали неожиданную гипотезу о связи между их геометрией и алгебраическим инвариантом (подписью узла), которую затем люди доказали традиционными методами [17](https://deepmind.google/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways/). По отзыву специалистов, это первый значимый математический результат, полученный с участием машинного обучения [3](https://deepmind.google/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways/). В других случаях ИИ выполняет роль генератора идей: например, система FunSearch генерирует новые конструкции (программы), которые могут привести к открытию решения открытой задачи, а человек уже анализирует и формализует это решение [18](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/). В среде формальных доказательства, поиска подходящих ранее подсказок следующего шага, автодополнения кода доказательства, поиска подходящих ранее

доказанных лем. В сообществе Lean уже используются большие языковые модели: они помогают находить нужные утверждения в библиотеке mathlib или даже автоматически заполнять пропущенные фрагменты доказательства. Таким образом, ИИ становится своего рода *"соавтором"* при доказательстве: рутинная проверка и перебор вариантов делегируется машине, тогда как интуиция и общий план остаются за человеком. Многие эксперты считают, что именно в тесном сотрудничестве человека и ИИ кроется путь к решению самых трудных проблем – машина обеспечивает надёжность и скорость, а человек – глубину понимания и творческий вклад.

## **Обзор систем формальной математики**

Чтобы понять, как ИИ применяется в доказательстве теорем, важно познакомиться с ключевыми системами формальной математики – программными комплексами, в которых ведётся формализация и проверка доказательств. Ниже рассмотрены наиболее популярные системы: **Lean**, **Coq**, **Isabelle/HOL** и **HOL Light**. Каждая из них имеет сообщество пользователей, обширную библиотеку формализованных знаний и уже послужила платформой для внедрения ИИинструментов.

#### **Lean (Lean 4 и mathlib)**

Lean – сравнительно *молодой* интерактивный доказатель, разработка которого началась Леонардо де Мурой в Microsoft Research в 2013 году [19](https://lean-lang.org/doc/reference/latest/Introduction/#:~:text=Leonardo%20de%20Moura%20launched%20the,minimal%20and%20independent%20implementations%20exist). Lean сочетает в себе функциональный язык программирования и средство для написания формальных доказательств, опираясь на строгую теоретико-типовую основу (вариант *Calculus of Constructions*). Особенностью Lean является сильная ориентация на **саморасширяемость**: пользователи могут определять собственные тактики, нотации и автоматические процедуры на самом языке Lean [20](https://lean-lang.org/doc/reference/latest/Introduction/#:~:text=with%20a%20dedicated%20front,to%20the%20diverse%20skills%20required). Это позволило сообществу создать обширную математическую библиотеку **mathlib** – единый репозиторий формализованной математики. Проект mathlib стартовал в 2017 году с целью формализовать как можно больше разделов чистой математики [21](https://en.wikipedia.org/wiki/Lean_(proof_assistant)#:~:text=In%202017%2C%20a%20community,theorems%20and%20100%2C000%20definitions%20in). К 2023 году библиотека mathlib была **портирована на Lean 4** (новую версию системы) и выросла до *свыше 1,5 млн строк* кода доказательств [22](https://lean-lang.org/doc/reference/latest/Introduction/#:~:text=is%20faster%20and%20scales%20to,3%20could%20check%20its%20smaller), содержащего более 210 000 теорем и 100 000 определений [21](https://en.wikipedia.org/wiki/Lean_(proof_assistant)#:~:text=In%202017%2C%20a%20community,theorems%20and%20100%2C000%20definitions%20in). Lean 4, официально выпущенный в сентябре 2023 года, значительно повысил производительность и гибкость системы – теперь сам доказатель на 90% написан на языке Lean, что позволяет быстро развивать его возможности [23](https://lean-lang.org/doc/reference/latest/Introduction/#:~:text=Development%20of%20Lean%204%20began,Even%20though%20Mathlib). Lean завоевал популярность среди математиков: в нём формализованы современные достижения (например, эксперимент Шольца по формализации теории жидких тензоров) и ведутся активные исследования по интеграции ИИ. В 2023 году вышла работа **LeanDojo** – открытая платформа для обучения языковых моделей на корпусе Lean, включающая данные из mathlib и инструментарий для автоматического доказательства теорем в Lean [24](https://arxiv.org/abs/2306.15626#:~:text=introducing%20LeanDojo%3A%20an%20open,premises%20and%20hard%20negative%20examples). LeanDojo предоставляет датасет из ~98 734 теорем и доказательств Lean с разметкой ~130 000 вспомогательных фактов [25](https://arxiv.org/html/2404.09939v1#:~:text=steps,Datasets%20for%20other%20proof), а также реализует *LLM-провер ReProver* с механизмом вытягивания релевантных лем из огромной библиотеки mathlib [24](https://arxiv.org/abs/2306.15626#:~:text=introducing%20LeanDojo%3A%20an%20open,premises%20and%20hard%20negative%20examples). Благодаря этому Lean становится одной из основных площадок для экспериментов по обучению ИИ-доказчиков.

#### **Coq**

Coq – один из *старейших и наиболее развитых* proof assistant'ов, разрабатываемый с 1980-х годов во французском INRIA. Coq основан на мощной теории типов (Calculus of Inductive Constructions) и предоставляет строго формализованный язык для записи математических определений,

утверждений и доказательств [26](https://zenodo.org/records/11551307). Coq прославился тем, что на нём были формализованы ключевые результаты: теорема о четырех красках, громоздкая теорема Фейта–Томпсона о простых группах (полная формализация заняла несколько лет), а также разработаны основы новой теории гомотопических типов (HoTT) [27](https://zenodo.org/records/11551307). Coq активно применяется и в информатике – для верификации программ и оборудования (яркий пример – компилятор CompCert, формально доказанный правильным [28](https://zenodo.org/records/11551307)). С точки зрения ИИ, Coq имеет богатые средства автоматизации: язык тактик Ltac, решатели уравнений, арифметические процедуры и т.д. Для подключения внешних ИИ-решателей создан **CoqHammer** – "молот" (hammer) интеграции с автоматическими доказателями [15](https://discourse.rocq-prover.org/t/coqhammer-1-1-1-for-coq-8-9/327). CoqHammer может автоматически попытаться доказать текущую цель в Coq, используя базу известных лем и внешние системы первого порядка, а затем вернуть доказательство, проверенное Coq-ядром [16](https://popl18.sigplan.org/details/CoqPL-2018/2/CoqHammer-Strong-Automation-for-Program-Verification). В последние годы появились и нейросетевые надстройки: например, проект *CoqGym* собирает данные о шагах доказательств в Coq для обучения моделей, а исследователи экспериментируют с LLM, генерирующими тактики Coq на естественном языке. Таким образом, Coq остаётся флагманским инструментом формальной проверки, соединяя десятилетия накопленного опыта в автоматизации с новыми подходами машинного обучения.

#### Isabelle/HOL

Isabelle – универсальная платформа для формальных доказательств, разработанная Л. Полсоном и коллегами (начиная с 1986 г.) [29](https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)). Isabelle примечательна тем, что является generic proof assistant: в ней реализован метаязык и небольшой логический kernel, на базе которого можно определять различные формальные логики (т. н. object logics). Наиболее популярным является вариант Isabelle/ **HOL** - интерактивный доказатель для классической высшей логики (Higher-Order Logic). Isabelle/HOL широко используется и обладает огромным архивом формальных доказательств (Isabelle AFP), где собраны тысячи формализованных теорем из разных областей математики и компьютерных наук [30](https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)) [31](https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)). С практической стороны Isabelle славится мощной встроенной автоматизацией. Помимо традиционных средств (перебора, переписывания и т.д.), Isabelle включает интерфейс Sledgehammer, который по запросу пользователя пытается автоматически доказать текущую цель, обращаясь к внешним решателям - как классическим (SMT-солвер CVC4, автоматы первого порядка E, SPASS, Vampire), так и специализированным методам внутри Isabelle <sup>6</sup> . Sledgehammer сам отбирает подходящие факты из контекста (используя в том числе и машинное обучение, например компонент MaSh), отправляет задачу внешнему доказателю и затем реконструирует найденное решение в средстве Isabelle (через инструмент Metis) <sup>6</sup> . Этот *полуавтоматический* режим значительно ускоряет работу, позволяя пользователю доверять рутинные шаги ИИ-инструменту. Сообщество Isabelle также активно исследует применение нейросетей – от отбора фактов до предложения тактик. Например, разрабатываются графовые нейросети для предсказания полезных лем, улучшенные версии hammer-инструментов с учётом эмбеддингов и т.д. Благодаря открытой архитектуре Isabelle, новые ИИ-модули можно встраивать, не нарушая надёжности проверки (ядро Isabelle остаётся минимальным и проверяет каждый шаг). Isabelle/HOL вместе с Coq и Lean формирует основу современного *ПР*-ландшафта, и все три системы являются целевыми для различных ИИ-инициатив.

#### **HOL Light**

**HOL Light** – облегчённая система для классической высшей логики, созданная Джоном Харрисоном в 1990-х как упрощение более раннего HOL. HOL Light написана на OCaml и характеризуется экстремально малым ядром и простыми принципами построения теорий [32](https://en.wikipedia.org/wiki/HOL_Light). Несмотря на кажущуюся

минималистичность, HOL Light продемонстрировала способность формализовывать очень сложные результаты. Сам Харрисон с её помощью формализовал основную теорему анализа (о существовании и единственности решений уравнений), теорему Дирихле о простых числах в арифметических прогрессиях, а совместно с Т. Хейлзом – проверил доказательство гипотезы Кеплера (проект Flyspeck). HOL Light стала и полигоном для исследований ИИ: на её базе Google DeepMind создал обучающую среду **HOList**, представляющую из себя набор из ~30 тысяч теорем и доказательств в формате, удобном для reinforcement learning-алгоритмов [33](https://openreview.net/pdf?id=Ozp1WrgtF5_). В рамках HOList были опробованы подходы с глубоким обучением для автоматического доказательства, и получены первые успехи: нейросети научились доказывать значительную долю задач из библиотеки HOL Light. Вдобавок, существовал датасет **HolStep** (2017) для обучения тактик HOL по логам доказательств [34](https://arxiv.org/html/2404.09939v2). Хотя HOL Light сейчас используется реже, идеи, опробованные на ней, переносятся на другие системы. Её ценят за прозрачность: каждый вывод легко проследить до аксиом через цепочку простых правил. Для ИИ это удобная площадка – минимизировать сложность логики и фокусироваться на поиске доказательства. В перспективе, наработки HOList и других проектов на HOL Light могут интегрироваться в более мощные системы вроде Isabelle и Lean.

(Помимо указанных, существуют и другие системы: например, Mizar – один из первых доказателей (1970-е), вдохновивший многие последующие проекты, а также Metamath – экстремально простой формализм, использованный в экспериментах OpenAI GPT-f. Однако основные тенденции сегодня связаны с развитием Lean, Coq, Isabelle и HOL.)

## Последние достижения и научные публикации (2023–2025)

В период 2023–2025 гг. в сфере ИИ-доказательства теорем произошёл качественный скачок. Появились **прорывные работы**, показавшие, что современные модели способны решать ранее не поддающиеся автоматизации математические задачи. Ниже мы рассмотрим ключевые достижения этого периода, распределённые по ведущим исследовательским группам и проектам.

Google DeepMind: Эта компания (ныне подразделение Google) явно стала лидером по применению ИИ в математике. В 2022-2023 гг. DeepMind представила несколько знаковых систем. Во-первых, AlphaTensor – первая ИИ-система, сумевшая *открыть новый алгоритм* для фундаментальной задачи умножения матриц [35](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/). AlphaTensor формулирует поиск алгоритма как игру для агента (на основе архитектуры AlphaZero) и с помощью обучения с подкреплением обнаружила способ перемножать матрицы  $4 \times 5$  и  $5 \times 5$  за 76 умножений вместо 80 – быстрее, чем лучшие человеческие разработки [36](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/). Это решает давнюю открытую проблему оптимизации матричного умножения [35](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/) [37](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/). Более того, AlphaTensor нашёл целое *семейство новых алгоритмов*, некоторые из которых на 10–20% эффективнее при реализации на современных процессорах и GPU [38](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/). Во-вторых, AlphaCode модель для генерации исходного кода, нацеленная на задачи спортивного программирования. В опубликованной работе (Science, 2022) показано, что AlphaCode достиг уровня среднего участника соревнований Codeforces, успешно решая ~30% сложных алгоритмических задач [39](https://arxiv.org/abs/2203.07814#:~:text=example%2C%20competitive%20programming%20problems%20which,to). Это означает, что ИИ научился понимать условие, разрабатывать и кодировать алгоритм решения – по сути, приближаясь к навыкам соревнующихся программистов [40](https://arxiv.org/abs/2203.07814#:~:text=this%20gap%2C%20we%20introduce%20AlphaCode%2C,on%20program%20behavior%20to%20a) . AlphaCode добивается этого сочетанием большой обученной модели и обширного поиска решений с последующей верификацией результатов [41](https://arxiv.org/abs/2203.07814#:~:text=evaluations%20on%20recent%20programming%20competitions,a%20small%20set%20of%20submissions).

В 2023 году DeepMind совершила ещё один шаг: в журнале Nature был представлен метод **FunSearch**, предназначенный для **автоматического открытия математических результатов** с помощью LLM [42](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/). FunSearch (от function search) сочетает крупную языковую модель (для генерации идей в виде программного кода) и специального оценщика, который отсеивает некорректные решения, предотвращая «галлюцинации» нейросети [42](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/). В процессе итеративного эволюционного поиска система генерирует и улучшает набор программ-кандидатов. Применив FunSearch, авторам удалось впервые с помощью ИИ решить открытые математические задачи: в частности, найти новые решения знаменитой задачи Cap Set (о максимальном множестве без трёхточечной арифметической прогрессии) [18](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/). Также система сама вывела улучшенные алгоритмы для NP-трудной задачи упаковки бинарных объектов (bin packing) [18](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/). Эти результаты получили широкий резонанс: фактически, ИИ начал самостоятельно делать вклад в математику, предлагая полностью проверяемые (поскольку представлены в виде кода) открытия. Важное преимущество FunSearch – выходные данные представлены в воспроизводимой форме (программа), что облегчает проверку и понимание человеком [43](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/).

Последним громким успехом DeepMind стала связка **AlphaProof** и **AlphaGeometry**. В июле 2024 г. объявлено, что их объединённая система решила *4 из 6 задач* Международной математической олимпиады 2024, набрав 28 баллов из 42 – уровень **серебряной медали** IMO [10](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/#:~:text=See%20our%20system%27s%20IMO%202024,solutions) [44](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/). AlphaProof – это новая RL-модель, тренированная для формального доказательства задач в популярных системах (возможно Lean или Isabelle), а AlphaGeometry 2 – усовершенствованный модуль для геометрических задач [10](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/#:~:text=See%20our%20system%27s%20IMO%202024,solutions). Стоит отметить, что *формулировку задач Олимпиады в формальном виде выполняли люди* (эксперты перевели текст задач в формальные цели) [45](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/), однако все дальнейшие шаги – поиск решения и запись полного доказательства – система выполнила сама. Особенно впечатляет, что AlphaProof справилась с самой трудной алгебраической задачей IMO-2024, которую решили лишь 5 участников-людей [46](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/). Решения ИИ были проверены официальными судьями (включая лауреата Филдсовской премии Т. Гауэрса) и получили максимально возможные баллы [47](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/) [11](https://arxiv.org/abs/2504.11354#:~:text=with%20minimal%20sampling%20,Prover). Это достижение приближает реализацию так называемого **IMO Grand Challenge** – задачи, поставленной сообществом, создать ИИ, способный завоевать золото на Международной олимпиаде по математике в формате «формулировка-доказательство» (F2F) [48](https://openreview.net/references/pdf?id=s--ynnG4f8#:~:text=the%20content%20and%20name%20of,checkable%29%20proof%20for%20that%20problem). Теперь серебро уже взято, и цель золота (решить хотя бы 5 из 6 задач) становится обозримой перспективой.

Меta (FAIR) и другие промышленно-академические проекты: В 2023 г. команда Меta AI представила LeanDojo, о котором упоминалось выше. LeanDojo знаменует важную веху: это полноценный набор инструментов и данных для исследования ИИ-доказателей на базе Lean [24](https://arxiv.org/abs/2306.15626#:~:text=introducing%20LeanDojo%3A%20an%20open,premises%20and%20hard%20negative%20examples). Он устраняет барьеры воспроизводимости предыдущих работ, предлагая открытые датасеты, обученные модели и интеграцию с Lean. В частности, модель ReProver из LeanDojo реализует подход retrieval-augmented (доказательство с подсказками из базы): при каждом шаге она с помощью специального pempuвера выбирает из mathlib нужные леммы, благодаря чему на сложных теоремах превосходит даже GPT-4 [49](https://arxiv.org/abs/2306.15626) [50](https://arxiv.org/abs/2306.15626). LeanDojo уже вдохновил последующие исследования – например, работы по улучшению выбора предпосылок с помощью графовых нейросетей [51](https://arxiv.org/html/2404.09939v1#:~:text=achieves%2027.6,2024%29%2C%20outperforms%20a%20symbolic) и новые бенчмарки для Lean. Также в Меta в 2022–2023 разрабатывался проект Galina (GuideAlpha? – информации мало, возможно внутренняя инициатива по автоформализации), и велись эксперименты с Code LLM (Codex) применительно к Coq и Lean. Хотя Мeta больше известна по успешным LLM (Llama 2 и др.), вклад в формальную математику через LeanDojo отмечен в сообществе как значимый шаг к доступности исследований в этой области.

**OpenAI:** Компания OpenAI одной из первых еще в 2020 г. продемонстрировала потенциал больших языковых моделей для доказательства теорем. Их система **GPT-f** (2020) использовала модифицированную GPT-3 для генерации шагов доказательства в системе Metamath [7](https://arxiv.org/html/2404.09939v1#:~:text=GPT,repair%20the%20whole%20proof%20at). При помощи обучения на корпусе формальных доказательств GPT-f научилась автоматически доказывать простые утверждения, предсказывая какую следующую тактику или правило применения нужно использовать [7](https://arxiv.org/html/2404.09939v1#:~:text=GPT,repair%20the%20whole%20proof%20at). Хотя процент решённых задач тогда был скромным, эта работа показала принципиальную возможность «научить» трансформер логическим рассуждениям. В 2022 г. команда (частично связанная с OpenAI) опубликовала бенчмарк **miniF2F**, уже упоминавшийся выше [52](https://openreview.net/references/pdf?id=s--ynnG4f8#:~:text=statements%20intended%20to%20provide%20a,advances%20in%20neural%20theorem%20proving) . MiniF2F предоставляет 488 формализованных задач олимпіадного уровня на трёх системах (Lean, Isabelle, Metamath) и служит открытым тестом для сравнения разных ИИ-доказчиков [52](https://openreview.net/references/pdf?id=s--ynnG4f8#:~:text=statements%20intended%20to%20provide%20a,advances%20in%20neural%20theorem%20proving). На нём проверялись как GPT-f, так и другие подходы, стимулируя прогресс (к 2023 г. лучшие модели решают ~30–40% задач из miniF2F). OpenAI продолжает исследования: в 2023 появились работы по *curriculum* learning (постепенному обучению моделей на все более сложных формальных задачах) [53](https://arxiv.org/html/2404.09939v1#:~:text=first%C2%A0,first) и эксперименты с GPT-4, способным в интерактивном режиме помогать пользователю формализовывать доказательство. Хотя у OpenAI нет отдельного «Alpha»-проекта, сопоставимого с DeepMind, их общие достижения в LLM (ChatGPT/GPT-4) оказывают большое влияние и на эту сферу. Уже есть энтузиасты, использующие ChatGPT для помощи при доказательствах в Lean: модель предлагает следующий шаг или находит подходящую лемму из документации. Можно ожидать, что будущие версии GPT будут ещё лучше понимать формальные языки и синтезировать доказательства, особенно если их обучить на данных из Coq/Lean.

Академические группы: Университеты и исследовательские институты по всему миру активно вовлечены в эту тематику. В 2023 г. прошли специальные семинары и воркшопы по ИИ в математических изысканиях (например, мероприятие Национальной академии наук США) - в их материалах отмечено, что ИИ способен ускорить открытия, выявляя скрытые структуры в массивах математических данных [54](https://www.nationalacademies.org/projects/DEPS-BMSA-23-01#:~:text=2023) [55](https://www.nationalacademies.org/projects/DEPS-BMSA-23-01#:~:text=A%20National%20Academies%20of%20Sciences%2C,strengthen%20collaboration%20among%20research%20communities). Группы в МІТ, Стэнфорде, Оксфорде, Эдинбурге, Праге и др. занимаются как теорией (например, исследования по автоформализации на базе лингвистики), так и практикой (создание бенчмарков, улучшение алгоритмов поиска доказательств). Так, университет Эдинбурга исторически связан с темой автоматического доказательства (там работал создатель первой системы ЛЦД-решеателя Робин Милнер), и нынешние учёные вносят вклад, работая над обучаемыми решателями для Isabelle/HOL. В Стэнфорде и Беркли ведутся работы по нейронному выбору лемм и по объединению символьных методов с нейросетями (гибридные доказчики). В 2023 опубликован обширный **обзор по deep learning для theorem proving** <sup>56</sup> , обобщающий более 170 работ – он также подготовлен силами академического консорциума (Торонто, Калтех и др.). Все эти инициативы, вместе взятые, формируют интенсивно растущее поле исследований. Ниже приведена таблица, суммирующая основные проекты последних лет, их цели и ссылки на исходные статьи или код.

#### Ключевые проекты 2022-2025

| Проект | Описание и достижения | Источник |
|--------|----------------------|----------|
| **AlphaTensor** (DeepMind, 2022) | Первая система для автоматического открытия новых алгоритмов. AlphaTensor открыл более эффективные способы умножения матриц, решив 50-летнюю проблему (например, 4×4 матрицы за 47 умножений). Нашёл сотни алгоритмов с рекордной сложностью и оптимизировал их под разное железо. | [Nature 2022](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/); [GitHub](https://github.com/deepmind/alphatensor) |
| **AlphaCode** (DeepMind, 2022) | Модель генерации кода, достигшая уровня среднего человека в соревнованиях по программированию. Решала сложные алгоритмические задачи на Codeforces, заняв место около топ-54% участников. Генерирует множество решений и автоматически фильтрует по успешному выполнению тестов. | [Science 2022](https://arxiv.org/abs/2203.07814); [arXiv:2203.07814](https://arxiv.org/abs/2203.07814) |
| **FunSearch** (DeepMind, 2023) | Метод эволюционного поиска программ с использованием LLM. Сгенерировал новые математические открытия: нашёл решения комбинаторной задачи Cap Set и улучшенные алгоритмы для bin-packing. Первая демонстрация, что LLM может привести к решению открытого математического вопроса. | [Nature 2023](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/); GitHub |
| **LeanDojo** (2023) | Открытая платформа для исследований ИИ-доказательства на Lean. Включает инструменты выгрузки датасетов из библиотеки mathlib (≈98k теорем, 130k лемм), обученные модели и бенчмарки. Реализует доказчик ReProver с механизмом выбора релевантных фактов из большой библиотеки. Свободно доступен (MIT License). | [NeurIPS 2023](https://arxiv.org/abs/2306.15626); [arXiv:2306.15626](https://arxiv.org/abs/2306.15626); [Сайт LeanDojo](https://leandojo.org/) |
| **AlphaProof** (DeepMind, 2024) | Система на основе RL и нейросетей для формального доказательства сложных задач. Вместе с AlphaGeometry 2 решила 4/6 задач IMO 2024, набрав серебро. AlphaProof генерирует полноценные формальные доказательства в интерактивной системе, справляясь даже с задачами, непосильными большинству участников Олимпиады. | [DeepMind Blog 2024](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/); Nature 2025 (в печати) |
| **GPT-f / miniF2F** (OpenAI, 2020–22) | GPT-f – первый трансформер, обученный доказывать формальные теоремы (Metamath). Предсказывал следующий шаг доказательства, генерируя тактики на основе состояния цели. miniF2F – межсистемный набор задач (488 теорем) для оценки нейродоказчиков на Lean, Isabelle и др. Стал стандартным тестом в исследованиях. | [arXiv 2020](https://arxiv.org/html/2404.09939v1); [ICLR 2022](https://openreview.net/references/pdf?id=s--ynnG4f8); [GitHub (miniF2F)](https://github.com/openai/miniF2F) |

*(Примечание: ссылки на arXiv и репозитории приведены по возможности для свободного доступа к материалам.)*

### **Выводы и перспективы**

Применение ИИ в формальной математике из стадии экспериментов стремительно переходит к стадии реальных результатов. Уже сегодня **нейросети способны решать математические задачи соревновательного уровня**, генерировать новые алгоритмы и помогать в проверке грандиозных доказательств. Ключевым достижением можно считать демонстрацию того, что ИИ может действовать *на пределе современных знаний*: например, AlphaProof фактически выступил на Олимпиаде за несколько дней и занял место рядом с сильнейшими молодыми математиками мира. При этом, человек остаётся неотъемлемой частью процесса – формулируя задачи, направляя поиск, интерпретируя открытия. Вероятно, такой симбиоз сохранится и в будущем: ИИ станет мощным «ускорителем» исследований, освобождая время для творческих инсайтoв. [44](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/#:~:text=See%20our%20system%27s%20IMO%202024,solutions)

Важный показатель прогресса – **рост сообщества и инфраструктуры**. Появление открытых библиотек (mathlib, AFP Isabelle и др.) и платформ вроде LeanDojo делает входной барьер для исследований ниже, привлекает новых специалистов. Выпуск обзоров и проведение международных семинаров говорит о формировании *междисциплинарного поля* на стыке математики, ИИ и компьютерных наук. Большие игроки (Big Tech и топ-университеты) инвестируют ресурсы, видя перспективы автоматизации математического труда. [57](https://www.nationalacademies.org/projects/DEPS-BMSA-23-01#:~:text=Artificial%20intelligence%20,Sponsored%20by%20the%20National%20S) [58](https://www.nationalacademies.org/projects/DEPS-BMSA-23-01#:~:text=This%20workshop%20will%20bring%20together,of%20AI%20for%20mathematical%20reasoning)

Что ждет впереди в 2025–2030 годах? Эксперты прогнозируют прорывы в **автоформализации** – возможно, ИИ-системы научатся переводить текст учебника или черновика математической статьи в проверяемый код, устраняя главную «узкость» процесса. Мы вероятно увидим первое **полностью автоматическое доказательство** новой нетривиальной теоремы, выполненное связкой нейросети и формального доказчика без подсказок человека. Цель завоевать золото на IMO или решить задачу из списка проблем тысячелетия может перестать быть фантастикой. В то же время, появятся и **новые вызовы**: как проверить не только корректность, но и значимость открытого ИИ результата? как сделать так, чтобы машинные доказательства были понятны людям? Эти вопросы потребуют сотрудничества математиков и специалистов по ИИ.

Можно с уверенностью сказать, что **ИИ не заменит математика, но изменит стиль его работы**. Рутинная проверка и поиск будут автоматизированы, тогда как человеческая интуиция сфокусируется на постановке проблем и интерпретации решений. Подобно тому, как вычислительные средства расширили возможности науки в XX веке, интеллектуальные системы расширят границы познания в XXI. Формальная математика, долго считавшаяся нишевой, благодаря ИИ может стать повсеместным стандартом обоснования результатов – от чистой теории до прикладных областей. Перспектива весьма вдохновляющая: возможно, уже вскоре мы станем свидетелями того, как человек и искусственный интеллект вместе докажут теорему, которую по отдельности ни один из них доказать не мог.

#### Дополнительные ссылки (первая часть документа):

- [3](https://deepmind.google/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways/#:~:text=between%20different%20areas%20of%20mathematics,that%20will%20be%20submitted%20to) [4](https://deepmind.google/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways/#:~:text=knot%20tell%20us%20about%20the,By%20using%20attribution) [17](https://deepmind.google/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways/) Exploring the beauty of pure mathematics in novel ways — Google DeepMind

- [6](https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)#:~:text=Though%20interactive%2C%20Isabelle%20features%20efficient,6) [29](https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)) [30](https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)) [31](https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)) Isabelle (proof assistant) — Wikipedia

- [10](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/#:~:text=See%20our%20system%27s%20IMO%202024,solutions) [11](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/) [44](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/) [45](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/) [46](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/) [47](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/) AI achieves silver-medal standard solving International Mathematical Olympiad problems — Google DeepMind

- [12](https://www.emergentmind.com/topics/autoformalization) Autoformalization: Bridging Informal and Formal Math — Emergent Mind

- [13](https://arxiv.org/abs/2310.07957) A New Approach Towards Autoformalization — arXiv

- [14](https://openreview.net/forum?id=hUb2At2DsQ) Rethinking and Improving Autoformalization: Towards a Faithful — OpenReview

- [15](https://discourse.rocq-prover.org/t/coqhammer-1-1-1-for-coq-8-9/327) [16](https://popl18.sigplan.org/details/CoqPL-2018/2/CoqHammer-Strong-Automation-for-Program-Verification) CoqHammer: Strong Automation for Program Verification — Rocq Prover / CoqPL

- [18](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/) [42](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/) [43](https://deepmind.google/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/) FunSearch: Making new discoveries in mathematical sciences using Large Language Models — Google DeepMind

- [19](https://lean-lang.org/doc/reference/latest/Introduction/) [20](https://lean-lang.org/doc/reference/latest/Introduction/) [22](https://lean-lang.org/doc/reference/latest/Introduction/) [23](https://lean-lang.org/doc/reference/latest/Introduction/) Introduction — Lean

- [21](https://en.wikipedia.org/wiki/Lean_(proof_assistant)) Lean (proof assistant) — Wikipedia

- [24](https://arxiv.org/abs/2306.15626) [49](https://arxiv.org/abs/2306.15626) [50](https://arxiv.org/abs/2306.15626) LeanDojo: Theorem Proving with Retrieval-Augmented Language Models — arXiv

- [26](https://zenodo.org/records/11551307) [27](https://zenodo.org/records/11551307) [28](https://zenodo.org/records/11551307) The Coq Proof Assistant — Zenodo

- [32](https://en.wikipedia.org/wiki/HOL_Light) HOL Light — Wikipedia

- [33](https://openreview.net/pdf?id=Ozp1WrgtF5_) Reasoning with Transformer-based Models: Deep Learning — OpenReview

- [34](https://arxiv.org/html/2404.09939v2) A Survey on Deep Learning for Theorem Proving — arXiv

- [35](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/) [36](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/) [37](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/) [38](https://deepmind.google/blog/discovering-novel-algorithms-with-alphatensor/) Discovering novel algorithms with AlphaTensor — Google DeepMind

- [39](https://arxiv.org/abs/2203.07814#:~:text=example%2C%20competitive%20programming%20problems%20which,to) [40](https://arxiv.org/abs/2203.07814#:~:text=this%20gap%2C%20we%20introduce%20AlphaCode%2C,on%20program%20behavior%20to%20a) [41](https://arxiv.org/abs/2203.07814#:~:text=evaluations%20on%20recent%20programming%20competitions,a%20small%20set%20of%20submissions) Competition-Level Code Generation with AlphaCode — arXiv

- [48](https://openreview.net/references/pdf?id=s--ynnG4f8#:~:text=the%20content%20and%20name%20of,checkable%29%20proof%20for%20that%20problem) [52](https://openreview.net/references/pdf?id=s--ynnG4f8#:~:text=statements%20intended%20to%20provide%20a,advances%20in%20neural%20theorem%20proving) MiniF2F — OpenReview

- [54](https://www.nationalacademies.org/projects/DEPS-BMSA-23-01#:~:text=2023) [55](https://www.nationalacademies.org/projects/DEPS-BMSA-23-01#:~:text=A%20National%20Academies%20of%20Sciences%2C,strengthen%20collaboration%20among%20research%20communities) [57](https://www.nationalacademies.org/projects/DEPS-BMSA-23-01#:~:text=Artificial%20intelligence%20,Sponsored%20by%20the%20National%20S) [58](https://www.nationalacademies.org/projects/DEPS-BMSA-23-01#:~:text=This%20workshop%20will%20bring%20together,of%20AI%20for%20mathematical%20reasoning) AI to Assist Mathematical Reasoning: A Workshop — National Academies



# **Искусственный интеллект в формальной математике: обзор работ за 2025 год**

## **Автоматическое доказательство теорем и достижения на соревнованиях**

- **AlphaProof и AlphaGeometry (DeepMind)** связка формальных доказчиков на основе RL, которая в 2024 году впервые достигла уровня серебряной медали на Международной математической олимпиаде (IMO). Система AlphaProof (для алгебры/комбинаторики/ теории чисел) обучалась методом AlphaZero на миллионах формализованных задач, а AlphaGeometry решала задачи геометрии; вместе они решили 4 из 6 задач IMO 2024, включая самую сложную, набрав 28 баллов (уровень серебра). Это был прорыв: ИИ впервые продемонстрировал способность соревноваться с лучшими школьниками в формальном формате. • [1](https://www.nature.com/articles/s41586-025-09833-y?error=cookies_not_supported&code=d91a137e-8e07-4f43-8884-4efd4a223f83#:~:text=provides%20a%20mechanism%20for%20learning,3%7D%2C%20this%20performance%2C%20achieved%20with) [2](https://www.nature.com/articles/s41586-025-09833-y?error=cookies_not_supported&code=d91a137e-8e07-4f43-8884-4efd4a223f83#:~:text=on%20historical%20mathematics%20competition%20problems,solving) [3](https://www.nature.com/articles/s41586-025-09833-y?error=cookies_not_supported&code=d91a137e-8e07-4f43-8884-4efd4a223f83#:~:text=millions%20of%20related%20problem%20variants,learning%20at%20scale%20from%20grounded) [4](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/#:~:text=Last%20year%2C%20Google%20DeepMind%E2%80%99s%20combined,approach%20elite%20human%20mathematical%20reasoning)
- **Gemini с режимом Deep Think (Google DeepMind)** в 2025 году новый большой язык модели Gemini преодолел достижения AlphaProof. Благодаря усовершенствованному режиму многовариантного рассуждения *Deep Think* и обучению с усилением на математических задачах, Gemini решила 5 из 6 задач IMO 2025 и набрала 35 баллов, что соответствует уровню золотой медали. В отличие от прошлогоднего подхода, Gemini работала **end-to-end** на естественном языке: она напрямую генерировала строго проверяемые решения из текстов задач (без ручной формализации) и уложилась в отведённые 4,5 часа соревнования. Организаторы IMO подтвердили, что решения были корректны и понятны, тем самым впервые ИИ достиг *«золотого»* уровня в решении олимпиадных задач. • [5](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/#:~:text=Breakthrough%20Performance%20at%20IMO%202025,with%20Gemini%20Deep%20Think) [6](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/#:~:text=IMO%20President%20Prof,Dolinar) [7](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/#:~:text=This%20achievement%20is%20a%20significant,hour%20competition%20time%20limit)
- **Kimina-Prover** экспериментальный крупный формальный доказчик (модель 72B), обученный с помощью **RL** от модели Qwen 2.5. В работе *"Kimina-Prover Preview"* (апрель 2025) представлено, что эта модель осваивает «формальный стиль рассуждений», эмулирующий человеческую стратегию: поэтапно строит доказательство, получая обратную связь от проверяющего ядра Lean. Kimina-Prover установила новый рекорд на бенчмарке MiniF2F – **80.7%** решённых теорем (при большом лимите выборок). Важно, что модель показывает высокую эффективность даже при минимальном числе попыток (pass@1) благодаря RL-подходу и масштабируется с ростом размера. Авторы открыли упрощённые версии Kimina (1.5B и 7B параметров) для сообщества. • [8](https://arxiv.org/abs/2504.11354#:~:text=%3E%20Abstract%3AWe%20introduce%20Kimina,with%20pass%408192.%20Beyond) [9](https://arxiv.org/abs/2504.11354#:~:text=of,verification%20and%20informal%20mathematical%20intuition) [10](https://arxiv.org/abs/2504.11354#:~:text=proving%2C%20as%20showcased%20in%20this,efficiency%2C%20delivering%20strong%20results%20even) [9](https://arxiv.org/abs/2504.11354#:~:text=of,verification%20and%20informal%20mathematical%20intuition) [11](https://arxiv.org/abs/2504.11354#:~:text=with%20minimal%20sampling%20,Prover) [11](https://arxiv.org/abs/2504.11354#:~:text=with%20minimal%20sampling%20,Prover)
- **DeepSeek-Prover-V2** открытый формальный **LLM-доказчик** для Lean4, представленный командой DeepSeek (Ren et al., 2025). Модель огромного размера (оценочно ~671 млрд параметров) обучена с нуля на основе стратегии *декомпозиции целей*: более мощный модуль DeepSeek-V3 рекурсивно разбивает сложные задачи на подзадачи и генерирует черновые доказательства, которые затем используются для RL-тюнинга финального провера. В результате DeepSeek-Prover-V2 достиг **88.9%** успеха на тестовом наборе MiniF2F и решил 49 из • [12](https://arxiv.org/abs/2504.21801#:~:text=%3E%20Abstract%3AWe%20introduce%20DeepSeek,671B) [13](https://arxiv.org/abs/2504.21801#:~:text=collected%20through%20a%20recursive%20theorem,9)

658 задач PutnamBench – это один из лучших показателей среди нейросетевых доказчиков [14](https://arxiv.org/abs/2504.21801#:~:text=reasoning%20into%20a%20unified%20model,between%20formal%20and%20informal%20mathematical). Кроме того, авторы представили новый набор задач **ProverBench** (325 формализованных задач, включая 15 недавних задач AIME) для расширенной оценки [15](https://arxiv.org/abs/2504.21801#:~:text=pass%20ratio%20on%20the%20MiniF2F,language%20models%20is%20substantially%20narrowing). Модель DeepSeek-Prover-V2 в открытом доступе сокращает разрыв между неформальным решателем (DeepSeek-V3) и формальным – на 15 задачах AIME она решила 6, в то время как мощный неформальный GPT-рещатель решил 8, что свидетельствует о сближении возможностей [16](https://arxiv.org/abs/2504.21801#:~:text=collection%20of%20325%20formalized%20problems%2C,language%20models%20is%20substantially%20narrowing).

- **LeanAbell-Prover-V2** относительно компактная модель-доказчик (7 млрд параметров) от исследователей из Tencent (Ji et al., 2025), демонстрирующая интеграцию верификатора в процесс вывода. Используя Reinforcement Learning с обратной связью от ядра Lean4, модель динамически узнаёт о правильности своих шагов: при ошибке компиляции она получает сигнал и учится корректировать ход доказательства [17](https://arxiv.org/abs/2507.08649#:~:text=Reinforcement%20Learning%20,available%20at%3A%20this%20https%20URL). Такой self-aware подход позволил улучшить точность вывода: LeanAbell-Prover-V2 превзошёл базовые модели аналогичного размера на ~2–3% (pass@128) на MiniF2F [18](https://arxiv.org/abs/2507.08649#:~:text=directly%20optimizes%20LLM%20reasoning%20trajectories,available%20at%3A%20this%20https%20URL), например, дал +3.2% к результату distilled-7В версии Kimina и +2.0% к DeepSeek-7В. Код, данные и сами модели LeanAbell v2 опубликованы в открытом доступе [19](https://arxiv.org/abs/2507.08649#:~:text=simple%20reward%20strategy,available%20at%3A%20this%20https%20URL), что способствует воспроизводимости.

- **Gödel-Prover** и **Gödel-Prover-V2** (Princeton et al.) открытая серия LLM для автоматического доказательства в Lean. Первая версия (Gödel-Prover, 2025) была представлена как "фронтирная" модель с открытым кодом, обученная на уникальном синтезированном наборе 1.64 млн формальных утверждений, полученных через автоформализацию [20](https://yangky11.github.io/#:~:text=We%20introduce%20Goedel,million%20formal%20statements%20through%20autoformalization). Улучшенная Gödel-Prover-V2 (Lin et al., 2025) добилась заметного скачка производительности: на сложном наборе PutnamBench её показатель успеха (Pass@32) оказался вдвое выше предыдущего SOTA, при том что размер модели меньше в 20 раз [21](https://yangky11.github.io/#:~:text=Our%20Goedel,with%20a%2020x%20smaller%20model). Такой прогресс при меньших ресурсах подчёркивает эффективность подходов команды Gödel (сочетание синтеза данных и самокоррекции) в сравнении с более громоздкими системами.

- **APOLLO** агентно-ориентированная система взаимодействия LLM с Lean, предложенная в 2025 году. *APOLLO* (**Automated Proof Repair via LLM and Lean collaboration**) действует как мета-алгоритм, который автоматически обрабатывает доказательства, генерируемые LLM, исправляя их и доводя до верного результата [22](https://arxiv.org/html/2505.05758v4#:~:text=remains%20a%20formidable%20task,LLM%20on%20each%20remaining%20goal) [23](https://arxiv.org/html/2505.05758v4#:~:text=and%20sampling%20budgets,raises%20the%20state%E2%80%91of%E2%80%91the%E2%80%91art%20accuracy%20for). Pipeline APOLLO включает ряд агентов: один модуль исправляет синтаксические ошибки с помощью Lean-компилятора, другой выделяет место ошибки в доказательстве, разбивает цель на подлеммы, подключает автоматические решатели и при необходимости вызывает LLM для незакрытых подцелей [24](https://arxiv.org/html/2505.05758v4#:~:text=agentic%20framework%20that%20combines%20the,among%20sub). После исправлений кусочки доказательства собираются и проверяются заново, итеративно до достижения корректности. Этот подход резко повысил эффективность: для моделей ~8 млрд параметров APOLLO достиг **84.9%** решения задач MiniF2F (на 8B-моделях, по состоянию на август 2025) при ограниченном бюджете попыток [25](https://arxiv.org/html/2505.05758v4#:~:text=with%20a%20low%20top%E2%80%91%20budget,efficiency%20and%20correctness%2C%20suggesting%20a). Даже менее специализированные модели (например, небольшие OpenAI GPT) под управлением APOLLO подняли успешность решения с 3–7% до >40% [26](https://arxiv.org/html/2505.05758v4#:~:text=benchmark%2C%20we%20establish%20a%20new,ospanov%2FAPOLLO). Код APOLLO открыт на GitHub [27](https://arxiv.org/html/2505.05758v4#:~:text=hundred,ospanov%2FAPOLLO), предлагая перспективный парадигм объединения LLM и формального компилятора для масштабируемого доказательства теорем.

### Автоформализация математических задач

• **FormaRL** – метод усиленного обучения для автоформализации без размеченных данных (Huang *et al.*, 2025). Эта работа направлена на преодоление дефицита пар «неформальное утверждение – формальный код», который сдерживает прогресс в автоматизации доказательств. **FormaRL** обучает модель-формализатор, используя лишь небольшой объем сырого (неразмеченного) текста: в цикле RL модель получает *награду* за вывод за счёт двух инструментов – синтаксического чекера Lean4 и LLM-модуля проверки эквивалентности [28](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=efficient%20reinforcement%20learning%20framework%20for,on) [29](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=accuracy%20of%20Qwen2.5,MT%2FFormaRL). Авторы собрали набор задач *uProof* (≈5000 утверждений из университетских курсов математики) и показали, что их подход повышает точность автоформализации в разы. Например, для базовой модели Qwen2.5-7B pass@1 на наборе ProofNet вырос с 4.04% до **26.15%**, а на более сложных задачах uProof – с 2.4% до 9.6% [30](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=formalizer,and%20pass%4016%20ac%02curacy) [29](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=accuracy%20of%20Qwen2.5,MT%2FFormaRL) , используя всего 859 неразмеченных примеров. Также FormaRL улучшила обобщающую способность на задачах *поверх* существующих SOTA-формализаторов. Код и обученные модели опубликованы открыто [31](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=And%20on%20uproof%20our%20method,MT%2FFormaRL) (GitHub **THUNLP-MT/FormaRL** [32](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=autoformalizers%20on%20both%20pass%401%20accuracy,MT%2FFormaRL) ), что позволяет воспроизвести результаты.

- **StepFun-Formalizer** семейство больших моделей (7В и 32В), предложенное Wu et al. (2025) для перевода текстовых задач в строго формальные утверждения Lean4. Авторы подчеркнули, что успешная автоформализация требует двух компонентов: (1) глубокого знания формального домена (библиотеки определений, теорем и т.д.) и (2) сильных умений рассуждать на естественном языке, чтобы правильно сопоставить неформальную задачу с формальным выражением [33](https://arxiv.org/abs/2508.04440#:~:text=,To) [34](https://arxiv.org/abs/2508.04440#:~:text=interpret%20real,5). StepFun реализует стратегию обучения, которая укрепляет оба навыка. Во-первых, они автоматически дистиллировали большой датасет формальных фактов (для пополнения знаний модели) и синтезировали цепочки рассуждений «неформальный вопрос → формальное решение» по шаблонам (для обучения навыку выстраивания соответствия) [35](https://arxiv.org/abs/2508.04440#:~:text=understanding%20and%20informal,resulting%207B%20and%2032B%20models). Затем модель дообучается комбинацией методов: Supervised Fine-Tuning и RL с верификацией и рефином (RLVR). Итоговые модели **StepFun-Formalizer-7B/32B** обладают и обширными знаниями, и умением интерпретировать задачу. Модель 32В достигла новых рекордов: например, **BEq@1 = 40.5%** на наборе FormalMATH-Lite и 26.7% на ProverBench, превосходя всех предыдущих универсальных и специализированных автоформализаторов [36](https://arxiv.org/abs/2508.04440#:~:text=further%20fuse%20and%20refine%20the,purpose%20and%20specialized%20models) . Интересно, что данная работа принята Oral на AAAI 2026, свидетельствуя о важности темы [37](https://arxiv.org/abs/2508.04440#:~:text=reasoning.%20Notably%2C%20StepFun,purpose%20and%20specialized%20models) . Код моделей StepFun доступен на Hugging Face [38](https://huggingface.co/stepfun-ai/StepFun-Formalizer-32B#:~:text=stepfun,Models%20%C2%B7%20Usage%20%C2%B7%20License) и GitHub [39](https://github.com/stepfun-ai/StepFun-Formalizer#:~:text=StepFun,into%20formal%20statements%20in) , облегчая их использование.

- **Autoformalizer with Tool Feedback (ATF)** ещё один значимый шаг (Guo et al., 2025) к надёжной автоформализации. АТГ интегрирует инструменты в цикл генерации: модельформализатор во время вывода автоматически обращается к Lean4-компилятору для исправления синтаксиса и использует несколько LLM-"судей" для проверки смысловой консистентности перевода [40](https://arxiv.org/html/2510.06857v1#:~:text=models%20to%20training%20an%20end,start%20phase%20on) [41](https://arxiv.org/html/2510.06857v1#:~:text=Feedback%20,markedly%20outperforms%20a%20range%20of). Таким образом, если первоначальный вывод модели содержит ошибку (например, не компилируется в Lean или меняет смысл задачи), инструменты выявляют проблему и возвращают сигнал модели, побуждая её скорректировать результат. ATF обучается в несколько этапов: cold-start на синтетических данных с вызовами инструментов, затем итеративное обучение с экспертом (expert iteration) и оптимизация для стабильности исправлений [42](https://arxiv.org/html/2510.06857v1#:~:text=Feedback%20,superior%20performance%20further%20validated%20by) [43](https://arxiv.org/html/2510.06857v1#:~:text=synthetic%20tool,Feedback). Результат – превосходство над базовыми моделями: АТГ заметно превзошёл лучшие на тот момент формализаторы (включая Goedel-Formalizer-v2) по качеству формальных переводов [44](https://arxiv.org/html/2510.06857v1#:~:text=,judge%20approach) [43](https://arxiv.org/html/2510.06857v1#:~:text=synthetic%20tool,Feedback). Более того, исследователи открыли большой синтетический датасет **Numina-ATF** (750 тыс. утверждений) сгенерированных формальных для дальнейших исследований автоформализации [45](https://arxiv.org/html/2510.06857v1#:~:text=revisions,Feedback) . Код и данные доступны на GitHub [46](https://arxiv.org/html/2510.06857v1#:~:text=human%20evaluations,Feedback) , что делает ATF ценным ресурсом для сообщества.

#### Новые наборы задач и бенчмарки

- **FormalMATH** крупнейший на сегодня бенчмарк по формальным доказательствам (Yu et al., 2025). Он содержит 5 560 задач, формализованных в Lean4, которые охватывают широкий спектр: от олимпиадных задач (ІМО, национальные олимпиады) до университетских теорем, по темам от алгебры и геометрии до анализа и дискретной математики [47](https://arxiv.org/abs/2505.02735#:~:text=artificial%20intelligence%2C%20hindered%20by%20limitations,based%20disproof%20filtering) [48](https://arxiv.org/abs/2505.02735#:~:text=benchmark%20comprising%205%2C560%20formally%20verified,of%20statements%20before%20manual%20verification). Ключевое достижение FormalMATH – масштаб получен без полного ручного ввода: авторы разработали human-in-the-loop конвейер автоформализации. Сначала специализированные LLM переводят задачи в Lean-формат, затем несколько LLM проверяют эквивалентность и **отсекают** ошибочные переводы с помощью опровержений (генерируя противоположные утверждения и проверяя их недоказуемость) [49](https://arxiv.org/abs/2505.02735#:~:text=%28e,based%20theorem%20provers%20reveals%20significant) [50](https://arxiv.org/abs/2505.02735#:~:text=a%20novel%20human,excelling). Только наиболее перспективные формализации (pprox72% от исходных) отправляются эксперту для финальной правки, что сильно экономит трудозатраты [50](https://arxiv.org/abs/2505.02735#:~:text=a%20novel%20human,excelling). FormalMATH позволил провести всестороннюю оценку современных доказчиков: даже лучшие из них решили лишь ~16.5% задач при ограниченном бюджете попыток [51](https://arxiv.org/abs/2505.02735#:~:text=evaluation%20of%20state,benchmark%20for%20benchmarking%20formal%20mathematical). Также выявлен дисбаланс: модели заметно лучше справляются, скажем, с алгеброй, чем с анализом, и чрезмерно полагаются на встроенные тактики вместо «творчества» [51](https://arxiv.org/abs/2505.02735#:~:text=evaluation%20of%20state,benchmark%20for%20benchmarking%20formal%20mathematical). Неожиданным открытием стало и то, žе добавление человеку написанных неформальных подсказок *снижает* успех доказательства – видимо, из-за шума и неоднозначности живой речи, что мешает LLM в строгом режиме [52](https://arxiv.org/abs/2505.02735#:~:text=practical%20sampling%20budgets%2C%20exhibiting%20pronounced,for%20benchmarking%20formal%20mathematical%20reasoning). FormalMATH (технический отчёт, 33 стр.) призван стать **надежным стресс-тестом** для новых моделей и стимулировать прогресс в АІ-доказательствах [53](https://arxiv.org/abs/2505.02735#:~:text=reasoning%20scenarios%2C%20suggesting%20that%20human,for%20benchmarking%20formal%20mathematical%20reasoning).

- **IndiMathBench** свежий набор соревнованийных задач, собранный при участии Microsoft Research (октябрь 2025). IndiMathBench включает 312 формально записанных теорем Lean4, взятых из задач Индийских математических олимпиад (INMO, RMO и др.) [54](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=manual%20curation%20and%20validation%20of,Multiple%20such) [55](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=for%20formalizing%20natural%20language%20problems,We%20analyze%20the%20performance). Для его создания применён *AI-поддерживаемый* человеко-машинный цикл: несколько LLM генерировали кандидаты формализаций, проверяя их через Lean и отладочный цикл, после чего удобный интерфейс позволял человеческому эксперту быстро просмотреть и исправить предложения [56](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=math%02ematical%20theorem%20proving%2C%20curated%20using,These%20formulas%20are%20presented%20with) [57](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=problem%20statements%2C%20sourced%20from%20Indian,INDIMATHBENCH%20is%20available%20at). Такой подход существенно ускорил формализацию новых, ранее отсутствовавших в датасетах задач. Итоговый бенчмарк представляет интерес тем, что дополняет существующие (MiniF2F, PutnamBench) задачами из других региональных олимпиад, увеличивая разнообразие [55](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=for%20formalizing%20natural%20language%20problems,We%20analyze%20the%20performance) [58](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=efficient%20validation%20and%20repair%20by,com%2Fprmbiy%2FIndiMathBench). Инструменты для работы с IndiMathBench (например, расширение VS Code для Lean) также разработаны в рамках проекта. Набор и код открыты на GitHub [59](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=are%20generated%20using%20an%20ensemble,com%2Fprmbiy%2FIndiMathBench).

- **PutnamBench** набор задач из знаменитого студенческого конкурса Putnam, представленный в конце 2024 года (Tsoukalas et al., NeurIPS 2024) и активно используемый в 2025. PutnamBench содержит ~500 формализованных задач (1938–2019 годы) из соревнования William Lowell Putnam, охватывающих разные разделы математики [60](https://arxiv.org/html/2510.06857v1#:~:text=,11354%2C%202025). Этот бенчмарк стал одним из стандартных тестов для новых AI-доказчиков: многие работы 2025 года (DeepSeek-Prover, Gödel-Prover-v2 и др.) отчитываются о количестве решённых задач именно на PutnamBench. Например, DeepSeek-Prover-V2 решила 49 задач этого набора [14](https://arxiv.org/abs/2504.21801#:~:text=reasoning%20into%20a%20unified%20model,between%20formal%20and%20informal%20mathematical), а ProofOptimizer научилась сокращать длинные доказательства решений Putnam-задач более чем наполовину [61](https://arxiv.org/abs/2510.15700#:~:text=reducing%20proof%20length,training%20data%20for%20supervised%20finetuning). Наличие PutnamBench позволяет сравнивать модели на едином сложном наборе задач, требующих глубокого понимания и изящных доказательств.

- **MiniF2F (Mini-***Fermat to Friedman***)** кросс-системный бенчмарк олимпиадного уровня, созданный ещё в 2021 году, но в 2025 остаётся важным индикатором прогресса. MiniF2F включает ~500 задач из IMO, AIME, AMC, формализованных в нескольких ИТП (Lean, Isabelle, Coq). Все ключевые системы 2025 года измеряют успех и на MiniF2F: так, Kimina достигла ~80.7% (при очень большом числе выборок), DeepSeek-Prover-V2 – 88.9% (pass ratio), а LeanAbell-v2 и другие – ~70–78% при ограниченных 128 попытках. Progress по MiniF2F свидетельствует о росте возможностей моделей, хотя до 100% пока далеко – многие задачи (особенно по геометрии) остаются трудными даже для лучших ИИ. • [62](https://arxiv.org/html/2510.06857v1#:~:text=FormalMATH,mathematical%20statements%20synthesized%20by%20LLMs) [62](https://arxiv.org/html/2510.06857v1#:~:text=FormalMATH,mathematical%20statements%20synthesized%20by%20LLMs) [9](https://arxiv.org/abs/2504.11354#:~:text=of,verification%20and%20informal%20mathematical%20intuition) [14](https://arxiv.org/abs/2504.21801#:~:text=reasoning%20into%20a%20unified%20model,between%20formal%20and%20informal%20mathematical) [63](https://www.emergentmind.com/topics/bfs-prover-v2#:~:text=Mind%20www,V2%3A%20Automated%20Lean%20Theorem) [64](https://www.alphaxiv.org/overview/2507.08649v1#:~:text=Leanabell,correct%20and)
- **CombiBench** специализированный бенчмарк для **комбинаторной математики** (Liu *et al.*, 2025). Он содержит 100 тщательно отобранных задач по комбинаторике из различных источников (IMO, USAMO, APMO, учебники и др.), предназначенных для оценки возможностей LLM решать сложные комбинаторные проблемы. CombiBench не столько про формальные доказательства, сколько про проверку *reasoning* модели в комбинаторных головоломках, требующих нестандартных шагов. В 2025 этот набор позволил выявить узкие места больших моделей – например, базовые GPT-4/Claude нередко ошибаются в таких задачах, показывая необходимость дальнейшего обучения с упором на структурированные рассуждения. CombiBench был представлен как submission на ICLR 2026 и привлёк внимание тем, что оценивает ИИ в области, близкой к олимпиадной математике, но вне чисто формального контекста. Он дополняет формальные бенчмарки, расширяя проверку AI на математическую смекалку. • [65](https://twitter.com/gm8xx8/status/1916942820671115744#:~:text=%F0%9D%9A%90m%F0%9D%9F%BE%F0%9D%9A%A1%F0%9D%9A%A1%F0%9D%9F%BE%20on%20X%3A%20,IMO%2C%20USAMO%2C%20APMO%2C%20textbooks) [66](https://twitter.com/gm8xx8/status/1916942820671115744#:~:text=CombiBench%3A%20Benchmarking%20LLM%20Capability%20for,IMO%2C%20USAMO%2C%20APMO%2C%20textbooks) [67](https://twitter.com/gm8xx8/status/1916942820671115744#:~:text=CombiBench%3A%20Benchmarking%20LLM%20Capability%20for,IMO%2C%20USAMO%2C%20APMO%2C%20textbooks)
- **IMO-Bench** новый комплекс бенчмарков от DeepMind (Luong, Lockhart *et al.*, 2025) для всесторонней оценки математических возможностей ИИ на уровне IMO. Согласно анонсу, **IMO-Bench** состоит из трёх частей, которые проверяют модели по различным аспектам: решение задач, генерация проверяемых доказательств и устойчивость к вариациям формулировок. Задачи прошли экспертизу международных спецалистов по олимпиадной математике, а сами бенчмарки были использованы при разработке Gemini (для отслеживания прогресса модели). Предварительный технический отчёт (*"Towards Robust Mathematical Reasoning"*, EMNLP 2025) показывает оценку разных доступных моделей (Anthropic Claude, DeepSeek-V3, etc.) на IMO-Bench. Появление IMO-Bench важно тем, что акцент смещается на **проверку надёжности** и гибкости ИИ-решателей: задачи намеренно сформулированы разнообразно, чтобы выявить умение модели адаптироваться. Также включена проверка на *строгие формальные доказательства*, а не только на получение правильного ответа . Таким образом, IMO-Bench призван стать стандартом для будущих исследований на стыке неформального и формального ИИ-разума в математике. • [68](https://www.reddit.com/r/mlscaling/comments/1opmv4y/google_deepmind_introducing_imobench_google/#:~:text=Reddit%20www,that%20judge%20models%20on) [69](https://www.alphaxiv.org/overview/2511.01846v1#:~:text=Towards%20Robust%20Mathematical%20Reasoning%20,solving) [70](https://www.alphaxiv.org/overview/2511.01846v1#:~:text=Towards%20Robust%20Mathematical%20Reasoning%20,solving) [71](https://arxiv.org/html/2511.01846v1#:~:text=Towards%20Robust%20Mathematical%20Reasoning%20,%2C%20DeepSeek%20V3) [72](https://arxiv.org/html/2511.01846v1#:~:text=We%20evaluate%20IMO,%2C%20DeepSeek%20V3) [69](https://www.alphaxiv.org/overview/2511.01846v1#:~:text=Towards%20Robust%20Mathematical%20Reasoning%20,solving) [73](https://paperverse.io/paper/6c722f2a-c627-4601-b60f-e368a0213d2a#:~:text=...%20paperverse.io%20%20IMO,getting)

## **Инструменты и проекты 2025 года для формальной математики**

**Lean Copilot** – фреймворк, превращающий большие языковые модели в **ассистентов** внутри системы Lean. Разработанный в Caltech (Song, Yang, Anandkumar, 2025), Lean Copilot позволяет запускать выводы LLM *непосредственно* в среде Lean, что даёт возможность создать интерактивные инструменты поддержки доказательств. По сути, Lean Copilot соединяет Python-обёртку для LLM (локально или через API) с интерфейсом Lean: пользователи могут запрашивать у модели подсказки следующего шага, автозавершение доказательства для • [74](https://ar5iv.org/abs/2404.12534#:~:text=be%20critical,the%20effectiveness%20of%20our%20method) [75](https://ar5iv.org/abs/2404.12534#:~:text=inference%20in%20Lean,all%20codes%20under%20a%20permissive) 
текущей цели или подбор нужной теоремы из библиотеки (premise selection) [76](https://ar5iv.org/abs/2404.12534#:~:text=inference%20in%20Lean,license%20to%20facilitate%20further%20research). В отличие от полностью автономных доказчиков, Copilot действует **в паре с человеком**: математик контролирует процесс, а ИИ помогает рутинными или сложными местами. Эксперименты показали, что такой режим заметно ускоряет работу в Lean по сравнению с чисто ручным доказательством и стандартной автоматизацией Lean-тактиками [77](https://ar5iv.org/abs/2404.12534#:~:text=Using%20Lean%20Copilot%2C%20we%20build,license%20to%20facilitate%20further%20research) [78](https://ar5iv.org/abs/2404.12534#:~:text=relevant%20premises%20,all%20codes%20under%20a%20permissive). Все компоненты Lean Copilot открыты под лицензией МІТ [79](https://ar5iv.org/abs/2404.12534#:~:text=relevant%20premises%20,license%20to%20facilitate%20further%20research) и уже интегрированы в экосистему LeanDojo – это снижает порог для внедрения LLM в практику формализации математики.

- **Lean Finder** семантическая поисковая система по математической библиотеке Lean (mathlib), представленная Lu et al. в 2025. Lean Finder решает проблему, с которой часто сталкиваются пользователи ИТП: «Как найти нужную теорему или лемму в огромной базе mathlib по краткому описанию?». Существующие поиск-плагины в Lean в основном "информализуют" запрос – переводят естественный язык в формальное приближенно – но не учитывают реальный контекст запроса математика. Lean Finder идёт от обратного: он анализирует тысячи реальных обсуждений на форумах Lean (Zulip и др.), чтобы выяснить, как люди спрашивают о математических фактах, затем на основе этого синтезирует тренировочные запросы и специально тонко настраивает эмбеддинги для понимания намерений пользователя 82. Дополнительно система учится на откликах (feedback) – какие результаты выбирались, что полезно – чтобы улучшать ранжирование. В итоге Lean Finder достигает >30% прироста качества поиска по сравнению с предыдущими инструментами и даже превосходит GPT-4, использованного как поиск, в умении угадывать, какой именно факт нужен математику 83 84. Проект доступен онлайн 84: leanfinder.github.io предлагает удобный интерфейс, где можно задать вопрос по математике (на английском) – например, "существует ли изоморфизм между... при таких условиях?" – а система выдаст соответствующую теорему Lean и ссылку на её доказательство в mathlib. Lean Finder тем самым **снижает порог входа** в формальную математику для новых пользователей и ускоряет работу опытных, позволяя быстро находить нужные результаты.

- **ProofGym** инфраструктурный проект, облегчающий обучение и тестирование LLMдоказчиков на разных системах (Li et al., NeurIPS MATH-AI Workshop 2025). ProofGym предоставляет единый API на Python для взаимодействия с гетерогенными интерактивными доказателями – поддерживаются Coq, Isabelle/HOL, Lean – скрывая различия между ними 85 86. Он поддерживает как генерацию целых доказательств, так и пошаговый интерактивный режим, а также позволяет выполнять батched-верификацию (параллельно проверять множество доказательств) для ускорения экспериментов 85. ProofGym вводит единый формат описания состояния/результата доказательства, что упрощает логирование и сбор датасетов из взаимодействий модели с разными ИТП 87. Предварительные эксперименты показали значительный рост пропускной способности при проверке и поиске доказательств, без потери скорости на отдельный запрос 88. Проект нацелен на исследователей: c ProofGym они могут писать и сравнивать алгоритмы обучения доказчиков, которые будут сразу совместимы с несколькими системами. Таким образом, ProofGym унифицирует платформу для нейро-символьных исследований в доказательствах, позволяя переносить находки из одной системы (например, Lean) в другую (Coq) с минимальными усилиями.

- **ProofOptimizer** инструмент **оптимизации формальных доказательств**, представленный командой Принстон/Caltech (Gu *et al.*, 2025). Проблема: современные RL-доказчики (как AlphaProof) генерируют доказательства длиной в тысячи строк – они проходят проверку ядром, но слишком громоздки и трудны для понимания человеком 89 90. ProofOptimizer обучен автоматически сокращать и упрощать такие доказательства, сохраняя их корректность. Модель не требует ручной разметки сокращённых доказательств – вместо этого используется итеративное самосовершенствование (expert iteration): модель пытается укорачивать доказательство, Lean проверяет, что оптимизированная версия по-прежнему доказывает теорему, и это входит в обучение как положительный пример. На этапе вывода ProofOptimizer применяется к готовому длинному доказательству циклично: сокращает его шаг за шагом, каждый раз убеждаясь через Lean, что доказательство ещё валидно. Результаты впечатляют: доказательства задач MiniF2F стали короче на 87% (!), решения PutnamBench – на 57%, а громоздкие доказательства задач IMO (сгенерированные ранними версиями проверов) сократились на ~49% 93 61. Более компактные доказательства не только легче читать – они и проверяются Lean'ом гораздо быстрее, и даже могут улучшить обучение новых моделей, если использовать их как обучающие примеры <sup>94</sup> . ProofOptimizer тем самым снимает бутылочное горлышко логической избыточности, делая вклад ИИ-доказчиков более пригодным для анализа математиками.

Вывод: 2025 год ознаменовался бурным прогрессом на стыке ИИ и формальной математики. Появились всё более мощные доказчики теорем (от гигантских RL-моделей, берущих олимпиады, до открытых компактных проверов), улучшились алгоритмы автоформализации текстовых математических задач, созданы новые бенчмарки и инструменты для оценки, а также вспомогательные сервисы для математиков. Особенно примечательно, что крупные индустриальные лаборатории (Google DeepMind, Microsoft, OpenAI в сотрудничестве) активно вовлечены: например, DeepMind поднял планку состязательного решения задач до уровня золота IMO 5 6, а совместные усилия академических групп подарили сообществу открытые инструменты вроде Lean Copilot, ProofGym, новых датасетов и моделей. Всё это приближает цель, когда ИИ станет не просто решателем задач, но и полезным помощником математиков – способным понимать неформальные аргументы, переводить их в строгие доказательства и даже подсказывать новые идеи для теорем. Такое слияние нейросетей с формальными методами обещает революционизировать и обучение, и практику математических исследований в ближайшем будущем. 96

### **Источники:**

#### Основные публикации:

- [1](https://arxiv.org/html/2404.09939v1#:~:text=Proving%20theorems%20is%20a%20cornerstone,these%20advancements%2C%20later%20research%20extended) [2](https://arxiv.org/html/2404.09939v1#:~:text=Exploring%20learning,of%20progress%20and%20indicate%20the) [5](https://arxiv.org/html/2404.09939v1#:~:text=ATP%20aims%20to%20verify%20formal,a%20proof%20or%20refutation%20is) [7](https://arxiv.org/html/2404.09939v1#:~:text=GPT,repair%20the%20whole%20proof%20at) [8](https://arxiv.org/html/2404.09939v1#:~:text=match%20at%20L363%20first%C2%A0,first) [9](https://arxiv.org/html/2404.09939v1#:~:text=match%20at%20L511%202019%3B%20Polu,%282022%29%20further) [25](https://arxiv.org/html/2404.09939v1#:~:text=steps,Datasets%20for%20other%20proof) [51](https://arxiv.org/html/2404.09939v1#:~:text=achieves%2027.6,2024%29%2C%20outperforms%20a%20symbolic) [53](https://arxiv.org/html/2404.09939v1#:~:text=first%C2%A0,first) [56](https://arxiv.org/html/2404.09939v1#:~:text=large%20language%20models%2C%20has%20sparked,aims%20to%20serve%20as%20a) A Survey on Deep Learning for Theorem Proving — arXiv

- [1](https://www.nature.com/articles/s41586-025-09833-y?error=cookies_not_supported&code=d91a137e-8e07-4f43-8884-4efd4a223f83#:~:text=provides%20a%20mechanism%20for%20learning,3%7D%2C%20this%20performance%2C%20achieved%20with) [2](https://www.nature.com/articles/s41586-025-09833-y?error=cookies_not_supported&code=d91a137e-8e07-4f43-8884-4efd4a223f83#:~:text=on%20historical%20mathematics%20competition%20problems,solving) [3](https://www.nature.com/articles/s41586-025-09833-y?error=cookies_not_supported&code=d91a137e-8e07-4f43-8884-4efd4a223f83#:~:text=millions%20of%20related%20problem%20variants,learning%20at%20scale%20from%20grounded) Olympiad-level formal mathematical reasoning with reinforcement learning — Nature

- [4](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/#:~:text=Last%20year%2C%20Google%20DeepMind%E2%80%99s%20combined,approach%20elite%20human%20mathematical%20reasoning) [5](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/#:~:text=Breakthrough%20Performance%20at%20IMO%202025,with%20Gemini%20Deep%20Think) [6](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/#:~:text=IMO%20President%20Prof,Dolinar) [7](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/#:~:text=This%20achievement%20is%20a%20significant,hour%20competition%20time%20limit) Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad — Google DeepMind

- [8](https://arxiv.org/abs/2504.11354#:~:text=%3E%20Abstract%3AWe%20introduce%20Kimina,with%20pass%408192.%20Beyond) [9](https://arxiv.org/abs/2504.11354#:~:text=of,verification%20and%20informal%20mathematical%20intuition) [10](https://arxiv.org/abs/2504.11354#:~:text=proving%2C%20as%20showcased%20in%20this,efficiency%2C%20delivering%20strong%20results%20even) [11](https://arxiv.org/abs/2504.11354#:~:text=with%20minimal%20sampling%20,Prover) Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning — arXiv

- [12](https://arxiv.org/abs/2504.21801#:~:text=%3E%20Abstract%3AWe%20introduce%20DeepSeek,671B) [13](https://arxiv.org/abs/2504.21801#:~:text=collected%20through%20a%20recursive%20theorem,9) [14](https://arxiv.org/abs/2504.21801#:~:text=reasoning%20into%20a%20unified%20model,between%20formal%20and%20informal%20mathematical) [15](https://arxiv.org/abs/2504.21801#:~:text=pass%20ratio%20on%20the%20MiniF2F,language%20models%20is%20substantially%20narrowing) [16](https://arxiv.org/abs/2504.21801#:~:text=collection%20of%20325%20formalized%20problems%2C,language%20models%20is%20substantially%20narrowing) DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition — arXiv

- [17](https://arxiv.org/abs/2507.08649#:~:text=Reinforcement%20Learning%20,available%20at%3A%20this%20https%20URL) [18](https://arxiv.org/abs/2507.08649#:~:text=directly%20optimizes%20LLM%20reasoning%20trajectories,available%20at%3A%20this%20https%20URL) [19](https://arxiv.org/abs/2507.08649#:~:text=simple%20reward%20strategy,available%20at%3A%20this%20https%20URL) Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning — arXiv

- [20](https://yangky11.github.io/#:~:text=We%20introduce%20Goedel,million%20formal%20statements%20through%20autoformalization) [21](https://yangky11.github.io/#:~:text=Our%20Goedel,with%20a%2020x%20smaller%20model) Gödel-Prover — Kaiyu Yang

- [22](https://arxiv.org/html/2505.05758v4#:~:text=remains%20a%20formidable%20task,LLM%20on%20each%20remaining%20goal) [23](https://arxiv.org/html/2505.05758v4#:~:text=and%20sampling%20budgets,raises%20the%20state%E2%80%91of%E2%80%91the%E2%80%91art%20accuracy%20for) [24](https://arxiv.org/html/2505.05758v4#:~:text=agentic%20framework%20that%20combines%20the,among%20sub) [25](https://arxiv.org/html/2505.05758v4#:~:text=with%20a%20low%20top%E2%80%91%20budget,efficiency%20and%20correctness%2C%20suggesting%20a) [26](https://arxiv.org/html/2505.05758v4#:~:text=benchmark%2C%20we%20establish%20a%20new,ospanov%2FAPOLLO) [27](https://arxiv.org/html/2505.05758v4#:~:text=hundred,ospanov%2FAPOLLO) APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning — arXiv

- [28](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=efficient%20reinforcement%20learning%20framework%20for,on) [29](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=accuracy%20of%20Qwen2.5,MT%2FFormaRL) [30](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=formalizer,and%20pass%4016%20ac%02curacy) [31](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=And%20on%20uproof%20our%20method,MT%2FFormaRL) [32](https://openreview.net/pdf?id=Z2El1U94bq#:~:text=autoformalizers%20on%20both%20pass%401%20accuracy,MT%2FFormaRL) FormaRL: Efficient Reinforcement Learning Framework for Autoformalization — OpenReview

- [33](https://arxiv.org/abs/2508.04440#:~:text=,To) [34](https://arxiv.org/abs/2508.04440#:~:text=interpret%20real,5) [35](https://arxiv.org/abs/2508.04440#:~:text=understanding%20and%20informal,resulting%207B%20and%2032B%20models) [36](https://arxiv.org/abs/2508.04440#:~:text=further%20fuse%20and%20refine%20the,purpose%20and%20specialized%20models) [37](https://arxiv.org/abs/2508.04440#:~:text=reasoning.%20Notably%2C%20StepFun,purpose%20and%20specialized%20models) [38](https://huggingface.co/stepfun-ai/StepFun-Formalizer-32B#:~:text=stepfun,Models%20%C2%B7%20Usage%20%C2%B7%20License) [39](https://github.com/stepfun-ai/StepFun-Formalizer#:~:text=StepFun,into%20formal%20statements%20in) StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion — arXiv

- [40](https://arxiv.org/html/2510.06857v1#:~:text=models%20to%20training%20an%20end,start%20phase%20on) [41](https://arxiv.org/html/2510.06857v1#:~:text=Feedback%20,markedly%20outperforms%20a%20range%20of) [42](https://arxiv.org/html/2510.06857v1#:~:text=Feedback%20,superior%20performance%20further%20validated%20by) [43](https://arxiv.org/html/2510.06857v1#:~:text=synthetic%20tool,Feedback) [44](https://arxiv.org/html/2510.06857v1#:~:text=,judge%20approach) [45](https://arxiv.org/html/2510.06857v1#:~:text=revisions,Feedback) [46](https://arxiv.org/html/2510.06857v1#:~:text=human%20evaluations,Feedback) [60](https://arxiv.org/html/2510.06857v1#:~:text=,11354%2C%202025) [62](https://arxiv.org/html/2510.06857v1#:~:text=FormalMATH,mathematical%20statements%20synthesized%20by%20LLMs) Autoformalizer with Tool Feedback — arXiv

- [47](https://arxiv.org/abs/2505.02735#:~:text=artificial%20intelligence%2C%20hindered%20by%20limitations,based%20disproof%20filtering) [48](https://arxiv.org/abs/2505.02735#:~:text=benchmark%20comprising%205%2C560%20formally%20verified,of%20statements%20before%20manual%20verification) [49](https://arxiv.org/abs/2505.02735#:~:text=%28e,based%20theorem%20provers%20reveals%20significant) [50](https://arxiv.org/abs/2505.02735#:~:text=a%20novel%20human,excelling) [51](https://arxiv.org/abs/2505.02735#:~:text=evaluation%20of%20state,benchmark%20for%20benchmarking%20formal%20mathematical) [52](https://arxiv.org/abs/2505.02735#:~:text=practical%20sampling%20budgets%2C%20exhibiting%20pronounced,for%20benchmarking%20formal%20mathematical%20reasoning) [53](https://arxiv.org/abs/2505.02735#:~:text=reasoning%20scenarios%2C%20suggesting%20that%20human,for%20benchmarking%20formal%20mathematical%20reasoning) FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models — arXiv

- [54](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=manual%20curation%20and%20validation%20of,Multiple%20such) [55](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=for%20formalizing%20natural%20language%20problems,We%20analyze%20the%20performance) [56](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=math%02ematical%20theorem%20proving%2C%20curated%20using,These%20formulas%20are%20presented%20with) [57](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=problem%20statements%2C%20sourced%20from%20Indian,INDIMATHBENCH%20is%20available%20at) [58](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=efficient%20validation%20and%20repair%20by,com%2Fprmbiy%2FIndiMathBench) [59](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/IndiMathBench_MS_hosting.pdf#:~:text=are%20generated%20using%20an%20ensemble,com%2Fprmbiy%2FIndiMathBench) IndiMathBench — Microsoft Research

- [61](https://arxiv.org/abs/2510.15700#:~:text=reducing%20proof%20length,training%20data%20for%20supervised%20finetuning) [89](https://arxiv.org/abs/2510.15700#:~:text=reaching%20IMO%20gold,without%20requiring%20additional%20human%20supervision) [90](https://arxiv.org/abs/2510.15700#:~:text=systems%20like%20Lean%2C%20their%20excessive,At%20inference%20time) [91](https://arxiv.org/abs/2510.15700#:~:text=shelf%20LLMs%20,trained%20provers%20on%20standard) [92](https://arxiv.org/abs/2510.15700#:~:text=simplify%20Lean%20proofs%20without%20requiring,improve%20downstream%20prover%20performance%20when) [93](https://arxiv.org/abs/2510.15700#:~:text=Lean%20to%20verify%20simplifications%20and,training%20data%20for%20supervised%20finetuning) [94](https://arxiv.org/abs/2510.15700#:~:text=reducing%20proof%20length,training%20data%20for%20supervised%20finetuning) [95](https://arxiv.org/abs/2510.15700#:~:text=benchmarks%2C%20reducing%20proof%20length%20by,training%20data%20for%20supervised%20finetuning) ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations — arXiv

- [63](https://www.emergentmind.com/topics/bfs-prover-v2#:~:text=Mind%20www,V2%3A%20Automated%20Lean%20Theorem) [64](https://www.alphaxiv.org/overview/2507.08649v1#:~:text=Leanabell,correct%20and) BFS-Prover-V2: Scalable Theorem Prover — Emergent Mind

- [65](https://twitter.com/gm8xx8/status/1916942820671115744#:~:text=%F0%9D%9A%90m%F0%9D%9F%BE%F0%9D%9A%A1%F0%9D%9A%A1%F0%9D%9F%BE%20on%20X%3A%20,IMO%2C%20USAMO%2C%20APMO%2C%20textbooks) [66](https://twitter.com/gm8xx8/status/1916942820671115744#:~:text=CombiBench%3A%20Benchmarking%20LLM%20Capability%20for,IMO%2C%20USAMO%2C%20APMO%2C%20textbooks) [67](https://twitter.com/gm8xx8/status/1916942820671115744#:~:text=CombiBench%3A%20Benchmarking%20LLM%20Capability%20for,IMO%2C%20USAMO%2C%20APMO%2C%20textbooks) CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics — Twitter/X

- [68](https://www.reddit.com/r/mlscaling/comments/1opmv4y/google_deepmind_introducing_imobench_google/#:~:text=Reddit%20www,that%20judge%20models%20on) Introducing IMO-Bench — Reddit

- [69](https://www.alphaxiv.org/overview/2511.01846v1#:~:text=Towards%20Robust%20Mathematical%20Reasoning%20,solving) [70](https://www.alphaxiv.org/overview/2511.01846v1#:~:text=Towards%20Robust%20Mathematical%20Reasoning%20,solving) [71](https://arxiv.org/html/2511.01846v1#:~:text=Towards%20Robust%20Mathematical%20Reasoning%20,%2C%20DeepSeek%20V3) [72](https://arxiv.org/html/2511.01846v1#:~:text=We%20evaluate%20IMO,%2C%20DeepSeek%20V3) [73](https://paperverse.io/paper/6c722f2a-c627-4601-b60f-e368a0213d2a#:~:text=...%20paperverse.io%20%20IMO,getting) Towards Robust Mathematical Reasoning — arXiv / Paperverse

- [74](https://ar5iv.org/abs/2404.12534#:~:text=be%20critical,the%20effectiveness%20of%20our%20method) [75](https://ar5iv.org/abs/2404.12534#:~:text=inference%20in%20Lean,all%20codes%20under%20a%20permissive) [76](https://ar5iv.org/abs/2404.12534#:~:text=inference%20in%20Lean,license%20to%20facilitate%20further%20research) [77](https://ar5iv.org/abs/2404.12534#:~:text=Using%20Lean%20Copilot%2C%20we%20build,license%20to%20facilitate%20further%20research) [78](https://ar5iv.org/abs/2404.12534#:~:text=relevant%20premises%20,all%20codes%20under%20a%20permissive) [79](https://ar5iv.org/abs/2404.12534#:~:text=relevant%20premises%20,license%20to%20facilitate%20further%20research) Towards Large Language Models as Copilots for Theorem Proving in Lean — arXiv

- [80](https://ar5iv.labs.arxiv.org/html/2510.15940#:~:text=We%20present%20Lean%20Finder%2C%20a,by%20analyzing%20and%20clustering%20the) [81](https://ar5iv.labs.arxiv.org/html/2510.15940#:~:text=theorems%20and%20the%20steep%20learning,Evaluations%20on%20real) [82](https://ar5iv.labs.arxiv.org/html/2510.15940#:~:text=queries,based) [83](https://ar5iv.labs.arxiv.org/html/2510.15940#:~:text=mathematicians%E2%80%99%20preferences%20using%20diverse%20feedback,io) [84](https://ar5iv.labs.arxiv.org/html/2510.15940#:~:text=world%20queries%2C%20informalized%20statements%2C%20and,io) Lean Finder: Semantic Search for Mathlib That Understands User Intents — arXiv

- [85](https://openreview.net/forum?id=RrSQxcg6Nu#:~:text=theorem%20proving%2C%20but%20most%20systems,improvements%20for%20verification%20and%20proof) [86](https://openreview.net/forum?id=RrSQxcg6Nu#:~:text=evaluation,This%20paper%20focuses%20on%20system) [87](https://openreview.net/forum?id=RrSQxcg6Nu#:~:text=evaluation,scale%20training%20and) [88](https://openreview.net/forum?id=RrSQxcg6Nu#:~:text=behind%20a%20common%20Python%20API,are%20left%20as%20ongoing%20work) ProofGym: Unifying LLM-Based Theorem Proving Across Formal Systems — OpenReview