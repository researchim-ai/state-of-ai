# ИИ в кибербезопасности: глобальные тренды 2024–2025

## Введение

Искусственный интеллект (ИИ) стремительно меняет сферу кибербезопасности. С одной стороны, защитные системы получают мощные инструменты для анализа огромных объемов данных, автоматизации рутины и выявления скрытых угроз. С другой – те же технологии усиливают арсенал злоумышленников, позволяя создавать более изощренные атаки. В 2024–2025 годах эта «гонка вооружений» между атаками и защитой с использованием ИИ достигла нового уровня во всем мире. Согласно опросам, **61% компаний** планируют в ближайший год задействовать **генеративный ИИ** (например, большие языковые модели) в своем киберзащитном контуре, и более **трети** уже это сделали. Одновременно около **87% организаций по всему миру** столкнулись хотя бы с одной **кибератакой, усиленной ИИ**. Такой двусторонний характер влияния ИИ определяет ключевые тренды в кибербезопасности. Ниже представлен подробный обзор этих трендов – от научных разработок до бизнес-решений, от защиты инфраструктуры до новых методов атаки – с примерами за 2024–2025 годы.

## Тенденции и общий обзор

**Масштабное внедрение ИИ.** Кибербезопасность становится одним из крупнейших направлений применения ИИ. По оценкам, мировой рынок решений ИИ для кибербезопасности оценивался примерно в **\$24,8 млрд в 2024 году** и стремительно растет. Крупные компании активно инвестируют в исследования и разработки: кибербезопасностные фирмы в среднем тратят на R\&D больший процент доходов, чем другие ИТ-компании. Венчурные инвестиции в отрасль также бьют рекорды – в 2024 году вложения выросли на **43%** и достигли \$11,6 млрд. В этой области наблюдается **волна поглощений и партнерств**. Например, корпорация **Alphabet (Google)** анонсировала крупнейшую сделку – приобретение израильской компании **Wiz** (специалиста по облачной безопасности с упором на ИИ) за сумму до **\$32 млрд**. Другие технологические гиганты и лидеры отрасли также расширяют портфель ИИ-решений через покупки стартапов: **Proofpoint** приобрела проект **Normalyze** (платформа на базе ИИ для классификации конфиденциальных данных), **CrowdStrike** – компанию **Adaptive Shield** (автоматизация безопасности SaaS, сделка \~\$300 млн), а ряд стартапов получили крупные раунды финансирования (например, **Cyera** – \$300 млн в 2024 году при оценке \$3 млрд).

**Рост угроз и новые меры.** Повсеместное внедрение ИИ обоюдоостро: злоумышленники получают инструменты для создания более сложных атак, а защитники – средства противодействия. Почти **56% экспертов** считают, что ИИ пока больше играет на руку хакерам, чем защитникам. Автоматизация, генерация кода и самообучение – все эти возможности ИИ используются киберпреступниками для повышения масштабируемости и эффективности атак. В результате количество **AI-атак** растет – по одному из опросов, в 2023 году **87% организаций** уже столкнулись с угрозами, где применялись инструменты ИИ. В ответ индустрия инвестирует в **AI-усиленные средства защиты**. По данным IBM, компании, активно использующие ИИ и автоматизацию в кибербезопасности, экономят в среднем **\$2,2 млн** при устранении последствий инцидентов (за счёт более быстрого обнаружения и реагирования). Неудивительно, что специалисты по безопасности ожидают трансформации отрасли: ИИ рассматривается как **«решающий фактор»** для закрытия проблемных зон в современных системах защиты.

Далее в отчете подробнее рассматриваются ключевые направления применения ИИ: от выявления и предотвращения атак до расследования инцидентов и **Threat Intelligence**, а также противоположная сторона – использование ИИ самими злоумышленниками и новые подходы к защите систем на базе ИИ.

## Обнаружение и предотвращение кибератак с помощью ИИ

Одно из основных применений ИИ – **автоматическое обнаружение угроз** и проактивное предотвращение атак. Современные системы на базе машинного обучения способны анализировать сетевой трафик, логи и файлы в реальном времени, выявляя аномалии быстрее, чем это возможно вручную. Ниже приведены конкретные примеры за 2024–2025 годы, как ИИ усиливает различные аспекты киберзащиты:

* **Обнаружение вредоносного ПО.** Традиционные антивирусы дополняются моделями глубокого обучения, которые по поведению программы или по коду определяют, является ли она вредоносной. Например, компания HP в 2023 году выявила реальные случаи малвари, **частично сгенерированной ИИ** – киберпреступники использовали AI-инструменты для написания фрагментов кода вредоносных программ. ИИ также позволяет создавать **“полиморфные” вирусы** – самоизменяющиеся образцы, сложные для сигнатурного детектирования. Для противодействия этому исследуются подходы вроде **нейросетевой классификации** семейств малвари и поведенческий анализ с аномалией. В научной среде появляются методы с использованием **графовых нейронных сетей (GNN)** для поиска скрытых связей между элементами атак. Так, исследователи из Unit 42 (Palo Alto Networks) в 2024 году описали способ выявления целых семейств связанных вредоносных доменов: они обучили **графовую нейросеть** на связях между известными URL и IP, что позволило находить новые вредоносные домены, используемые теми же группами атакующих. Такие графовые модели помогают **проактивно “раскручивать” инфраструктуру атаки** – обнаруживать дополнительные узлы (серверы, домены) до того, как они нанесут вред.

* **Защита от фишинга**. ИИ используется для повышения эффективности фильтров спама и фишинга. **Обработка естественного языка (NLP)** с помощью трансформеров позволяет распознавать фишинговые письма по совокупности тонких признаков (стиль текста, несоответствие контекста и др.). В 2024 году Microsoft представила **AI-агента для триажa фишинга** в рамках своего продукта Defender – этот агент автоматически сортирует и приоритизирует поступающие оповещения о подозрительных письмах, снабжая аналитиков объяснениями, и **обучается на их обратной связи**. Благодаря генеративным моделям такие системы могут даже **генерировать рекомендации** по реагированию (например, шаблон предупреждения пользователям). Это важно, поскольку число фишинговых атак растет – только Microsoft в 2024 году зафиксировала **30 миллиардов фишинговых email** – и без автоматизации распознавания инцидент-респонс команды перегружены. Отдельное направление – противодействие **вишингу** (voice phishing, голосовому фишингу). Злоумышленники стали применять **синтез речи ИИ** для имитации голосов. По данным исследований, уже **до 80% телефонных социальных инженерных атак** (“вишинг”) используют поддельный голос, созданный нейросетью. В ответ появляются специальные инструменты – **детекторы ИИ-голоса**, способные отличить синтезированную речь от настоящей. Например, сервис *AI Voice Detector* заявляет, что обнаружил уже **90 тысяч образцов поддельных голосов** для 25 тысяч клиентов, используя собственные модели ИИ для анализа аудио. Такие решения интегрируются в корпоративные коммуникации (Zoom, Teams и пр.), предупреждая, если голос собеседника может быть фейком.

* **Выявление сложных сетевых атак (ATP/APT).** Продвинутые постоянные угрозы (APT) часто проявляют себя как малозаметные аномалии в огромном потоке событий. ИИ здесь незаменим для **поведенческого анализа**. Современные системы класса NDR (Network Detection and Response) и XDR (eXtended Detection and Response) применяют **анализ поведенческих шаблонов** пользователей и устройств (UEBA) на базе машинного обучения. Это позволяет выявлять нетипичную активность – например, боковое перемещение злоумышленника по сети, сбор данных перед эксфильтрацией и пр. – даже если отдельные действия не совпадают ни с одной известной сигнатурой. Такой подход помог, например, разоблачить сложную целевую атаку на облачную инфраструктуру почтового сервиса в одном из недавних кейсов: ИИ-модель заметила малозаметное отклонение в последовательности команд и тем самым указала на присутствие постороннего скрипта. Кроме того, ИИ используется и **проактивно**: для поиска **уязвимостей нулевого дня** до того, как их обнаружат хакеры. В 2024 году команда Google Project Zero совместно с исследователями DeepMind представила проект **Big Sleep**, где **автоматический агент на базе ИИ** ищет баги в программном обеспечении. Результат – первое в истории обнаружение ИИ **ранее неизвестной уязвимости** в широко используемом ПО: Big Sleep нашел критическую ошибку (переполнение буфера стека) в библиотеке SQLite, тут же исправленную разработчиками. Это демонстрирует потенциал ИИ не только реагировать на атаки, но и **упреждать их**, укрепляя безопасность до инцидентов.

Таким образом, ИИ к 2025 году пронизывает все уровни обнаружения угроз – от почтовых шлюзов до облачных брандмауэров – и позволяет защищаться от атак **в режиме реального времени** либо даже предупреждать их на ранних стадиях. При грамотном применении он снижает число пропущенных инцидентов и ускоряет реакцию на новые эксплойты, что особенно важно перед лицом возросшей сложности атак.

## Автоматизация реагирования на инциденты

Помимо выявления угроз, искусственный интеллект все шире используется для **автоматизации реагирования** – ускорения и частичного выполнения тех действий, которые ранее вручную делали команды кибербезопасности. В условиях постоянного дефицита кадров и множества параллельных атак автоматизация реагирования (SOAR – Security Orchestration, Automation and Response) с помощью ИИ стала приоритетом.

**Решения класса “Security Copilot”.** В 2024 году компания Microsoft вывела в глобальное использование продукт *Security Copilot* – это AI-ассистент для специалистов по безопасности, основанный на GPT-4. Он интегрирован с экосистемой Microsoft 365 и другими инструментами и способен по запросу **суммировать сведения об инциденте**, собрать таймлайн атак, предложить набор мер реагирования и даже автоматически запускать некоторые из них. В начале 2025 года Microsoft представила обновление Copilot: набор из **11 автономных агентов на базе ИИ**, которые берут на себя конкретные задачи – от фильтрации фишинговых атак до защиты данных и управления уязвимостями. Например, **агент по расследованию уязвимостей** может сам анализировать настройки доступа в облаке, обнаруживать опасные пробелы в политиках и рекомендовать их исправление. **Агент Threat Intelligence Briefing** автоматически готовит сводку актуальных внешних угроз, релевантных конкретной организации, на основе ее профиля риска. Такие AI-модули позволяют обрабатывать рутинные задачи за секунды, оставляя человеку надзор и принятие финальных решений.

**Обучение с подкреплением для защиты.** В академической среде разрабатываются подходы, где системы ИИ **учатся реагировать на атаки путем проб и ошибок**. Подход **reinforcement learning (обучение с подкреплением, ОСП)** зарекомендовал себя в сценариях, где нужно находить оптимальную стратегию. В 2024 году исследователи представили фреймворк **ARCS (Adaptive Reinforcement Learning for Cybersecurity Strategy)** – он использует глубокое обучение с подкреплением для адаптивного выбора стратегии реагирования на инцидент. Система рассматривает состояние инфраструктуры, тип атаки, возможные противодействия и с помощью симуляций выбирает действия, которые **минимизируют ущерб** и предотвращают развитие атаки. В отличие от статических плейбуков, такой подход позволяет вырабатывать **динамические меры**: например, сначала автоматически изолировать скомпрометированный узел (немедленная реакция), а параллельно вычислять долгосрочные шаги – усиление политик или патчинг уязвимости, которую эксплуатировал злоумышленник. Исследования показывают, что ОСП может значительно повысить эффективность реагирования, сокращая время от обнаружения до нейтрализации угрозы. В перспективе подобные алгоритмы лягут в основу автоматизированных **“киберохранных систем”**, которые сами предпринимают нужные действия в момент атаки (например, перенастраивают брандмауэр или обнуляют скомпрометированные учетные записи), уменьшая нагрузку на людей.

**Оркестрация и SOAR 2.0.** Многие коммерческие продукты уже включают элементы ИИ в традиционные платформы оркестрации безопасности. Например, современные SOAR-системы могут автоматически **генерировать кейс-менеджмент**: ИИ-составляет отчёт об инциденте на основе разрозненных логов, выделяет ключевые индикаторы компрометации (IoC) и заполняет тикет для реагирования. Далее оркестрация может по готовому сценарию запустить скрипты – всё это без участия человека либо с минимальным подтверждением. Некоторые EDR/XDR-решения предлагают функцию **авто-исцеления**: обнаружив подозрительный файл, система не только сигнализирует, но и автоматически проверяет его в изолированной среде (песочнице) с помощью ML-анализа, и при подтверждении угрозы – удаляет или помещает в карантин файл на всех узлах сети. В сетевой безопасности ИИ помогает динамически обновлять правила: например, обнаружив новую аномалию в трафике, система ИИ может сама создать **правило блокировки** на межсетевом экране (firewall) или в системе предотвращения вторжений (IPS), тем самым мгновенно закрыв выявленный вектор атаки.

**Экономия времени и ресурсов.** Применение ИИ для автоматизации приносит ощутимый эффект. По оценкам IBM, внедрение **систем безопасности с ИИ и автоматизацией** позволяет сократить средний цикл обнаружения и реагирования на инцидент почти на **100 дней**, что эквивалентно серьёзному уменьшению ущерба. Кроме того, автоматизация **снижает нагрузку на SOC**-аналитиков, позволяя небольшим командам защищать больше активов. Учитывая, что в кибербезопасности глобально ощущается нехватка квалифицированных кадров, такие “силовые множители” (force multipliers) как ИИ-ассистенты становятся крайне востребованы. По сути, каждое повторяемое действие – от закрытия ложных срабатываний до сбора телеметрии по инциденту – можно делегировать ИИ, оставив человеку творчество и сложные случаи.

## Threat Intelligence и аналитика угроз на базе ИИ

**Threat Intelligence (TI)** – мониторинг и анализ актуальных киберугроз – также претерпевает изменения под влиянием AI. Объем данных о потенциальных атаках, индикаторах компрометации, уязвимостях и активностях хакерских групп растет экспоненциально, и ИИ становится необходимым инструментом для **поиска “иголки в стоге сена”**.

**Анализ больших данных угроз.** Машинное обучение позволяет свести воедино разрозненные сигналы: логи серверов, сетевые потоки, сообщения об инцидентах из открытых источников, данные с даркнета и т.д. Современные платформы TI используют **алгоритмы кластеризации и классификации**, чтобы группировать индикаторы, относящиеся к одной кампании, и **предсказывать развитие угроз**. К примеру, если в разных частях мира появились новые образцы малвари с пересекающимися признаками, ML-модель может связать их и указать, что это вариации одной атаки, нацеленной на определенный сектор. Кроме того, **прогнозная аналитика** (predictive analytics) с помощью ИИ способна оценивать, какая из множества известных уязвимостей с большей вероятностью будет использована злоумышленниками следующей. Для этого учитываются исторические паттерны (что обычно эксплуатирует та или иная группировка), обсуждения на хакерских форумах, похожие по коду эксплойты и т.п. Таким путем ИИ помогает расставить приоритеты в устранении уязвимостей (vulnerability management), что чрезвычайно ценно при сотнях новых CVE ежемесячно.

**Графовые базы знаний и GNN.** Для ведения Threat Intelligence часто используются **графовые представления**: узлы – это IP, домены, хеши файлов, аккаунты, а ребра – связи между ними (общий сервер, принадлежность к одной кампании, и т.д.). **Графовые нейросети** (упомянутые ранее GNN) отлично подходят для выявления **скрытых зависимостей в таких графах угроз**. Например, Unit 42 показала, как GNN на графе инфраструктуры фишинговых кампаний выявляет новые фишинг-домены, которые иначе неочевидно связаны с известными (общие SSL-сертификаты, повторы в WHOIS и пр.). Аналогично, графовые модели помогают в **атрибуции атак** – установлении, какая группировка (Threat Actor) стоит за инцидентом. Обученная на исторических данных модель может по совокупности артефактов (семейство малвари, тип целей, используемые инструменты) выдать вероятного “виновника” или по крайней мере сузить круг. Это ускоряет реакцию, т.к. понятно, чего дальше ожидать от этой группы (например, известен их почерк ухода от расследования или типичные цели).

**ИИ для обработки естественного языка в TI.** Генеративные модели, такие как большие языковые модели (LLM), помогают специалистам TI справляться с информационным потоком. Они умеют **резюмировать отчеты об угрозах**, выделяя главные факты из длинных текстов. В 2024 году некоторые компании начали внедрять LLM для автоматического обзора ежедневных сводок: ИИ прочитывает десятки статей и отчетов (от IBM X-Force, Mandiant, BleepingComputer и др.) и выдает сжатое резюме ключевых инцидентов дня. Также LLM могут **переводить технический жаргон** в понятный языκ – например, подготовить для руководства компании краткое объяснение, чем опасна конкретная уязвимость и какие меры приняты. Это экономит время экспертов и улучшает **коммуникацию о рисках** внутри организации.

**Поиск уязвимостей с помощью ИИ.** Уже упомянутый проект Google *Big Sleep* – пример слияния TI и автоматизированного поиска уязвимостей: ИИ просматривает открытый исходный код популярных проектов, пытаясь обнаружить потенциальные бреши безопасности. Другой масштабный пример – запущенный в 2024 году конкурс **DARPA AI Cyber Challenge (AIxCC)**. В рамках этой двухлетней инициативы, поддержанной ведущими AI-компаниями (Anthropic, OpenAI, Google, Microsoft), команды разрабатывают системы на базе ИИ, способные **автоматически находить и исправлять уязвимости** в программном обеспечении. Каждой из семи отобранных команд DARPA предоставила грант \$1 млн на разработку таких решений. Финал конкурса пройдёт на DEF CON, и ожидается прорыв в технологиях **AI-ассистированного пентестинга и патчинга**. Эти примеры показывают, как ИИ не только реагирует на известные атаки, но и служит для **проактивной разведки угроз**, делая цифровую экосистему в целом безопаснее.

В результате, за 2024–2025 гг. Threat Intelligence превратился в сферу, где **человек и ИИ работают в связке**: ИИ выполняет трудоемкий анализ и первичную обработку, а эксперт по безопасности принимает финальные решения и обогащает модель знаниями. Такой симбиоз значительно повышает скорость обнаружения глобальных кампаний, позволяет компаниям быть в курсе актуальных тактик злоумышленников и опережать угрозы на шаг.

## Использование ИИ злоумышленниками: новые угрозы

К сожалению, ИИ оказался полезен не только “добрым парням”. 2024 год ознаменовался всплеском интереса к использованию AI-технологий в криминальной среде. Злоумышленники экспериментируют с генеративными моделями для создания атак, которые сложнее распознать и отразить.

* **Генерация убедительных фишинговых атак.** Раньше массовые фишинг-рассылки часто выдавали себя плохим языком и шаблонностью. Теперь же модели вроде GPT-3.5/4 позволяют автоматически генерировать **грамотные и контекстно релевантные фишинговые письма**, почти неотличимые от легитимной переписки. Исследование, опубликованное в 2023 году, показало, что около **60% пользователей** не смогли отличить сгенерированные ИИ фишинговые сообщения от настоящих – успех сопоставимый с работой опытных “социальных инженеров”. Злоумышленники могут скармливать LLM информацию о своей цели (например, данные из соцсетей) и получать на выходе **сильно персонализированные письма**, обращающиеся к жертве по имени, упоминающие актуальные детали (должность, недавние события) – такие атаки значительно повышают шанс успеха. Кроме текста, ИИ применяется для **подделки голоса и видео** – так называемые **deepfake**. В 2023-м и 2024-м произошли несколько громких инцидентов, когда крупные компании пострадали из-за *vishing*-мошенничества с поддельным голосом. Например, в сентябре 2023 года сеть казино MGM Resorts потерпела ущерб в \~\$100 млн после того, как хакеры с помощью **ИИ-клона голоса** обманули сотрудника техподдержки и получили доступ в сеть. В другом случае в Гонконге финансовый директор был обманут на \$25 млн через фейковый созвон в Zoom, где киберпреступник использовал как поддельное видео, так и имитировал голос босса компании. Таких атак раньше просто не существовало, а теперь даже осторожному человеку трудно отличить фейк: **каждый четвертый сотрудник** испытывает затруднения в различении настоящего голоса от синтезированного.

* **Малварь, написанная ИИ.** Генеративные модели кода (такие как ChatGPT, Copilot, CodeWhisperer) способны создавать работоспособный программный код по описанию. Это снижает **барьер входа в киберпреступность**. Теперь атакующему не обязательно быть продвинутым программистом – достаточно суметь правильно описать желаемое поведение малвари на понятном языке. Эксперты компании HP в 2023 году обнаружили **конкретные образцы вредоносного ПО, частично сгенерированные ChatGPT**. ИИ помог авторам быстро получить рабочие шаблоны, которые затем были слегка доработаны вручную. Кроме того, ИИ способен автоматически **обфусцировать код**, генерируя множество вариантов вредоноса, чтобы обходить детектирование. Вредоносные программы теперь могут обладать элементами *self-evolving* – например, менять порядок выполнения или шифровать свои части, опираясь на подсказки ML, чтобы дольше оставаться незамеченными. Такие подходы, усиливающие *полиморфизм атак*, ранее требовали серьезных навыков, а сейчас доступны более широкому кругу злоумышленников.

* **Инструменты “для атакующего” на базе ИИ.** В теневой части интернета появились специализированные AI-модели и боты, созданные **“для хакеров”**. Исследование Kaspersky Digital Footprint Intelligence показало, что только в 2023 году на даркнет-форумах было около **3000 обсуждений** применения ChatGPT и других LLM для незаконной деятельности. Киберпреступники обмениваются способами **джейлбрейка** (обхода ограничений) у популярных чат-ботов, обсуждают создание собственных моделей без этических фильтров. В результате уже предлагаются на продажу альтернативные боты – например, упоминаются проекты **FraudGPT, DarkBART, XXXGPT** – якобы версии языковых моделей **без цензуры**, специально заточенные под написание malware, фишинга, генерацию мошеннических схем. В даркнет-рекламе такие боты позиционируются как “ChatGPT без запретов” и продаются по подписке. Одновременно на подпольных рынках отмечен всплеск торговли **украденными аккаунтами ChatGPT** (для доступа к платной версии API) – найдено еще 3000 объявлений о продаже таких учетных записей и сервисов авто-регистрации. Это подтверждает высокий интерес злоумышленников: они готовы платить, лишь бы получить доступ к мощному ИИ для своих целей.

* **Дезинформация и социальное влияние.** 2024 год – период подготовки к выборам в ряде стран, и генеративный ИИ начал применяться для массированной **дезинформации**. Вредоносные кампании используют ИИ для создания **фейковых новостей**, полируя стиль изложения под реальные СМИ, а также генерируют видео с выступлениями **Deepfake-персон** – от имени политиков или знаменитостей – с провокационными заявлениями. В отсутствие регулирования интернета такие видео и новости могут распространяться без проверки фактов, манипулируя общественным мнением. Таким образом, ИИ задействован не только в “технических” атаках, но и в **социально-психологическом** направлении угроз, усложняя выявление информационных атак.

Несмотря на всю серьезность перечисленного, стоит отметить: эксперты предостерегают от излишней паники. Многие из AI-инструментов пока **не панацея для хакеров**, а лишь расширение возможностей. По оценке специалистов “Лаборатории Касперского”, генеративный ИИ и чат-боты **вряд ли радикально изменят ландшафт кибератак хотя бы в 2024 году**. Большинство атак уже и так автоматизированы по своему механизму, а ИИ скорее используется злоумышленниками для упрощения подготовки (снижения порога входа) или увеличения масштаба, но не создаёт неотразимых супер-атак. Тем не менее, организации должны учитывать появление этих новых тактик. Фиксация тысяч обсуждений на теневых форумах – сигнал о тенденции, и защитникам важно **оставаться информированными** о подобных экспериментах злоумышленников, чтобы заранее готовить меры противодействия.

## Защита систем ИИ и данные организаций

Параллельно с использованием ИИ для атаки и защиты встал новый вопрос: **как самим защищать системы ИИ и предотвращать их злонамеренное использование?** Появление больших языковых моделей и других генеративных систем в рабочем контуре организации привело к новым рискам – утечки данных через запросы к ИИ, уязвимости в самих AI-моделях, возможность обхода ограничений (prompt injection) и др. 2024–2025 годы характеризуются ростом внимания к **безопасности ИИ-систем** (AI Security).

**“Shadow AI” – неконтролируемое использование ИИ.** Более половины сотрудников крупных компаний начали применять в работе различные AI-инструменты (переводчики, чат-боты, помощники для кода) без формального одобрения ИТ или службы безопасности. Microsoft отмечает всплеск этого явления, называемого **“теневой ИИ”**: по их данным, **57% предприятий** видят рост инцидентов, связанных с несанкционированным использованием AI-сервисов работниками, тогда как **60% компаний** еще не внедрили специальных контролей для этого. Риски понятны – сотрудник может случайно скормить конфиденциальные данные внешней модели (например, попросив ChatGPT проанализировать кусок внутреннего кода или договор), и эти данные утекут за пределы компании. В ответ в 2024 году компании начали разрабатывать **политиκи управления ИИ**: регламентируется, какие инструменты можно использовать, какие данные вводить запрещено. Одновременно технически внедряются **средства предотвращения утечек (DLP)**, адаптированные под ИИ-приложения. Пример – **новые функции Microsoft Defender**, которые в 2025 году научились **блокировать передачу чувствительных данных** в веб-приложения на основе ИИ (такие как ChatGPT, Bing Chat, Google Bard/Gemini). Если пользователь пытается вставить текст, содержащий, скажем, номера кредитных карт или персональные данные, в окно чат-бота, то браузерное расширение Defender DLP это обнаружит и предотвратит отправку. Подобные механизмы позволяют пользоваться ИИ-сервисами безопаснее, не опасаясь, что ценная информация “просочится” в облако OpenAI или другой внешний сервис.

**Уязвимости и атаки на сами модели.** Появился новый класс угроз – атаки на модели ИИ: **инъекции в промпт**, **отравление данных обучения**, **выкачивание модели** (model extraction), генерация преднамеренных **адверсариальных примеров** для обмана AI. В ответ крупные вендоры усиливают меры: например, **Google** в 2024 году обновила программу вознаграждений за уязвимости (**bug bounty**), включив в нее специально **“генеративные ИИ угрозы”**. Теперь исследователям предлагается искать проблемы вроде обхода защитных фильтров, раскрытия приватных данных из модели или выполнения нежелательных действий при определенных запросах. Такой подход стимулирует **AI-редтиминг** – этическое тестирование безопасности ИИ-систем – и позволяет находить и устранять слабые места (например, случаи, когда злоумышленник специальной последовательностью подсказок добивается от чат-бота выдачи конфиденциальной информации или инструкций по взлому). OWASP в 2023 году запустила проект **“OWASP Top 10 для LLM & Generative AI”**, в котором собрала главные риски безопасности, связанные с большими языковыми моделями (LLM). В свежей редакции 2025 года акцент делается на такие проблемы, как **prompt injection** (внедрение вредоносных инструкций через ввод), **непреднамеренная утечка данных через контекст**, **уязвимости агентов на базе LLM**, недостатки в **авторизации API моделей** и др. Сообщество призывает внедрять **best practices** по защищенной интеграции LLM: например, четко ограничивать доступ моделей к критичным системам, фильтровать пользовательский ввод на наличие опасных шаблонов, применять **контейнеризацию** и “песочницы” при запуске AI-агентов, чтобы те не могли навредить инфраструктуре. В 2025 году на RSA Conference состоялся отдельный саммит OWASP, посвященный этим темам, что подчеркивает их актуальность.

**Метки и детекторы генерации.** Для борьбы с волной дезинформации и подделок большое внимание уделяется **отличию сгенерированного контента от реального**. Один из подходов – **водяные знаки в AI-контенте**. В конце 2024 года Google заявила, что будет **помечать скрытой меткой весь текст, сгенерированный ее моделью Gemini**. Эта “водяная печать” встроена в выход модели и позволяет однозначно установить, что данный текст создан ИИ – без неопределенностей вероятностных детекторов. Если другие крупные производители LLM внедрят нечто подобное, это серьезно затруднит использование ИИ для скрытой генерации фейковой информации. Для изображений и видео также разрабатываются алгоритмы-детекторы, а некоторые форматы (например, JPEG, MPEG) рассматривают идею на уровне стандарта включать поля для пометки сгенерированного контента. В сфере аудио, как отмечалось, уже есть и метки (напр. компания ElevenLabs ввела **классификатор речи**, определяющий, создан ли голос их инструментом) и внешние детекторы.

**Защита LLM на уровне провайдеров.** Сами разработчики больших моделей предпринимают шаги по безопасности: проводят регулярный **Red Teaming** своих ИИ (OpenAI, Anthropic и другие публикуют отчеты о том, как их модели пытались использовать злоумышленно и какие улучшения сделаны). Модели получают обновления с более строгими фильтрами на запрещенный контент. Также внедряются **ограничения контекста** – например, у OpenAI есть ограничения на объем принимаемой информации, чтобы снизить риск “утечки” её обратно. Другой подход – **differential privacy** и шифрование внутри ML, чтобы даже если злоумышленник получит доступ к весам модели, он не смог легко извлечь обучающие данные.

**Инфраструктура для безопасного ИИ.** В дополнение к программным мерам появляются и инфраструктурные. Google, Microsoft и другие предлагают сервисы **Confidential Computing** для обучения и запуска моделей ИИ в защищенных средах (SGX и аналогах), которые предотвращают просмотр данных даже при компрометации хоста. Это актуально для **федеративного обучения** – когда модель учится на данных нескольких организаций без обмена самими данными. Например, в упомянутом проекте Google- Swift по борьбе с мошенничеством применяется federated learning с использованием **доверенной среды выполнения (TEE)**: каждое банк обучает локальную модель на своих транзакциях, затем в защищенной среде происходит агрегация общих весов – при этом исходные данные банков не раскрываются ни друг другу, ни даже облачному провайдеру. Подобные архитектуры позволяют совместно использовать мощь ИИ, **не рискуя конфиденциальностью данных**. В сфере кибербезопасности федеративные подходы перспективны, например, для коллективного обучения детекторов аномалий по telemetry от множества компаний – так формируются более “умные” модели без создания централизованной уязвимой базы данных.

Подводя итог, в 2024–2025 гг. мы наблюдаем становление полноценного направления **“AI Security”** – защиты, ориентированной на сами системы ИИ и безопасное их внедрение. Организации начинают понимать, что развёртывая у себя мощные модели, они берут на себя новые обязательства по их защите. Формируются как практические инструменты (мониторинг и контроль использования ИИ, детекторы), так и **нормативные рамки и лучшие практики**, поддерживаемые сообществом (OWASP, ISO внедряет стандарты по управлению ИИ-рисками и т.д.). Это новое измерение кибербезопасности, которое будет только важнее по мере распространения AI в бизнес-процессах.

## Современные методы ИИ в кибербезопасности (2024–2025)

Прорывы последних лет во многом связаны с развитием конкретных методов машинного обучения и их адаптацией под задачи безопасности. Ниже перечислены некоторые из наиболее заметных подходов ИИ, применяемых в кибербезопасности в рассматриваемый период, с примерами их использования:

* **Графовые нейронные сети (Graph Neural Networks, GNN).** Графовые модели ИИ умеют выявлять сложные взаимосвязи в данных, представленных в виде графов. В кибербезопасности они применяются для анализа сетей связей между объектами атаки. Например, GNN используются для **поиска связанных вредоносных доменов и IP-адресов**: обучившись на уже известных индикаторах (командных серверах, фишинговых URL и т.п.), модель может обнаруживать новые узлы инфраструктуры, которые злоумышленники еще не успели активировать. Также графовые нейросети внедряются в **системы обнаружения вторжений** (IDS) – они строят “граф атаки”, где узлы – события и состояния, и помогают идентифицировать мультистадийные атаки, прослеживая всю цепочку от проникновения до выполнения вредоносных действий. В 2024 году появились научные работы, где GNN улучшали обнаружение малвари (например, анализируя граф вызовов функций в бинарном файле) и объяснимость решений (так как по графу можно визуализировать, какие связи стали причиной срабатывания).

* **Федеративное обучение.** Этот подход позволяет нескольким организациям или узлам **совместно тренировать ML-модель без обмена исходными данными**. В кибербезопасности федеративное обучение оказалось очень ценным там, где данные конфиденциальны (например, логи инцидентов банков) или распределены по множеству устройств (IoT-сети, мобильные устройства). В 2024 г. консорциум **Swift + Google Cloud** начал внедрение федеративной модели для выявления мошеннических транзакций между банками. Каждая организация обучает локальную модель на своих примерах мошенничества, а облачный сервер аккумулирует только обновления моделей, объединяя их в общую улучшенную модель (с применением методов конфиденциальной агрегации). Это позволяет выявлять сложные схемы мошенничества, затрагивающие сразу несколько банков, чего не мог сделать каждый банк отдельно. В более широком плане, федеративное обучение в кибербезопасности пригодно для коллективного выявления новых вирусов (антивирусные компании могут обмениваться **градиентами модели**, а не вирусными образцами) или для распределенных систем обнаружения аномалий (каждый узел сети учится на своей телеметрии, но итоговая модель видит общие черты атак). При этом соблюдается приватность данных – критичный фактор для законодательства о персональных данных и коммерческой тайны.

* **Обучение с подкреплением (Reinforcement Learning, RL).** RL-системы обучаются, взаимодействуя со средой, что похоже на моделирование “игры” между атакующим и защитником. В последние годы этот метод стал применяться для **автоматизации реагирования и принятия решений**. Например, как обсуждалось, RL может оптимизировать стратегии incident response: агент-программе предоставлен “симулятор” сети и атак, и она методом проб и ошибок учится минимизировать ущерб, пробуя разные контрмеры. Также RL используется для **тестирования защиты**: можно обучить нейросеть-нарушителя (attacker agent), которая будет изобретать новые способы проникновения, обходя поставленные защиты; параллельно нейросеть-защитник (defender agent) учится противодействовать. Так имитируется бесконечная игра “кошки-мышки”, результатом которой становятся все более устойчивые политики безопасности. В сфере анализа вредоносного ПО есть примеры применения RL для **динамического анализа**: агент решает, какие действия выполнить с подозрительным файлом (эмулировать запуск, выгрузить память, проверить сеть), чтобы скорее подтвердить зловредность – это ускоряет анализ сложных угроз. Исследования 2024 года подтвердили преимущества RL для кибербезопасности, подчеркнув, что он позволяет адаптироваться к неизвестным ранее атакам лучше, чем жестко запрограммированные алгоритмы.

* **Генеративные модели и большие языковые модели (LLM).** Генеративный ИИ (включая **GAN**, автокодировщики и трансформеры типа GPT) уже проявил себя как с положительной, так и с отрицательной стороны в кибербезопасности. Со стороны защиты, генеративные модели **помогают создавать синтетические данные** для обучения детекторов (например, порождать вариации сетевых атак, чтобы обогатить датасет для IDS). LLM, обученные на коде, используются для **автоматического поиска уязвимостей в исходном коде** – по сути, играют роль виртуального аудитора: модель указывает строки кода, где потенциально допущены ошибки безопасности (SQL-инъекции, переполнения и т.п.). Такие функции появились в инструментах разработчиков (например, GitHub Copilot теперь отмечает небезопасные места и предлагает исправления). Со стороны злоумышленников – генеративные модели применяются для **создания реалистичных фишинговых сообщений**, deepfake-контента (как упомянуто в разделе про угрозы) и даже для генерирования эксплойтов. Например, есть эксперименты, где GPT-4 удается сгенерировать эксплойт к простой уязвимости, зная описание проблемы – раньше на написание эксплойта уходили часы у эксперта, а модель справляется за минуты. Это заставляет защитников ускорять обмен информацией о патчах. Большие языковые модели внедряются и в продукты кибербезопасности: **интерактивные чат-боты для SOC-аналитиков**, которым можно задать вопрос на естественном языке (“Какие атакующие IP мы видели за последнюю неделю в сетях филиалов?”) и получить ответ/выборку данных без ручных запросов. Это значительно повышает **удобство и скорость работы** с системами мониторинга. Ключевой вызов – обеспечение корректности и надежности ответов LLM (борьба с галлюцинациями модели и неполнотой данных).

* **Другие методы.** Помимо вышеперечисленных, в 2024–2025 гг. развиваются и смежные подходы: **трансферное обучение** (Transfer Learning), когда модели, обученные выявлять одни типы угроз, адаптируются под другие с меньшими затратами данных; **объяснимый ИИ (XAI)** для кибербезопасности – чтобы системы машинного обучения давали понятные обоснования своим срабатываниям (важно для доверия аналитиков и соответствия регулированиям); **гибридные AI-архитектуры**, сочетающие детерминированные правила и ML, чтобы достичь как точности, так и предсказуемости. Наконец, **квантовый ИИ** начал появляться на горизонте: ряд компаний (напр. стартап SandboxAQ) привлекают инвестиции для применения квантовых вычислений в задачах ИИ и безопасности. Это пока скорее перспектива ближайшего будущего, но направление обозначилось.

Объединяя все эти методы в системах, разработчики стремятся создать **многоуровневую, “умную” защиту** – способную и выявлять известные шаблоны атак, и обобщать знания для борьбы с новыми угрозами, и автоматически принимать решения, и при этом быть устойчивой к попыткам обмана. 2024–2025 годы стали временем активных экспериментов и внедрения таких технологий в практику.

## Ключевые игроки, продукты и инициативы

В волне развития AI-секьюрити участвуют как давние лидеры кибербезопасности, так и технологические гиганты и молодые стартапы. Ниже перечислены некоторые наиболее заметные компании, продукты и инициативы (глобально, без привязки к стране), определившие облик отрасли в 2024–2025 годах:

* **Microsoft** – один из драйверов применения больших языковых моделей в безопасности. *Microsoft Security Copilot* (на базе GPT-4) стал первым массовым корпоративным AI-ассистентом для SOC, а в 2024–2025 гг. Microsoft интегрировала его во все свои продукты безопасности (Defender, Sentinel SIEM и др.). Добавление автономных **AI-агентов** для отдельных задач в 2025-м расширило возможности Copilot. Кроме того, Microsoft фокусируется на защите самих AI-систем: развивает функции по контролю *Shadow AI* (см. DLP для ChatGPT) и предлагает клиентам **многооблачный мониторинг** использования моделей ИИ (поддержка Azure OpenAI, OpenAI API, Llama, Google Gemini и др. в едином интерфейсе безопасности).

* **Google (Alphabet)** – активно использует возможности своего подразделения **DeepMind** для усиления безопасности. Помимо упомянутого проекта **Big Sleep** по поиску уязвимостей с ИИ, Google внедряет ИИ в облачные сервисы безопасности (например, системы аномалий в Google Cloud). В 2024 году Alphabet объявила о крупнейшей сделке по приобретению израильского стартапа **Wiz** – это платформа облачной безопасности, широко применяющая ИИ для обнаружения проблем конфигураций и автоматизации защиты, особенно в средах Kubernetes и multi-cloud. Сумма сделки может достичь **\$32 млрд**, что отражает уверенность Google в потенциале AI-защиты. Также Google одна из первых начала **помечать AI-контент** (все тексты от модели Gemini снабжаются незаметным маркером), и включила проверку AI-уязвимостей в программу вознаграждений за баги.

* **Palo Alto Networks** – лидер корпоративной безопасности, который интегрирует ИИ во множество своих продуктов. Облачная платформа **Prisma Cloud** позиционируется как **CNAPP (Cloud-Native Application Protection Platform)** и использует ИИ как “последний недостающий элемент головоломки” для полной защиты облачных сред. AI там служит *force multiplier* для **управления поверхностью атаки (ASM)** – быстрее собирает и обогащает данные об активах, уязвимостях, неправильно настроенных сервисах, что позволяет приоритезировать риски эффективнее. Также Prisma Cloud отдельно решает задачу **безопасности использования ИИ**: помогает компаниям обнаруживать утечки данных или небезопасное применение моделей ИИ в своих рабочих процессах. Другие линейки Palo Alto (например, межсетевой экран следующего поколения) также оснащены ML-движками для обнаружения неизвестных угроз. Кроме того, исследовательское подразделение **Unit 42** у Palo Alto активно публикует исследования по применению AI, что подкрепляет продукты свежими идеями (пример – статья о графовых нейросетях для поиска фишинг-инфраструктуры).

* **IBM Security** – IBM давно вкладывалась в когнитивные технологии для безопасности (платформа Watson for Cybersecurity). В 2024 г. IBM выпустила отчет, показавший значительное снижение ущерба от утечек данных при использовании AI и автоматизации. IBM интегрирует ИИ в свое решение *QRadar XDR*: там ML выявляет аномалии, а встроенный ассистент на естественном языке помогает расследовать инциденты. IBM также участвует в отраслевых инициативах по выработке стандартов безопасного ИИ и предоставляет сервисы по тестированию устойчивости AI-моделей (adversarial testing) для своих клиентов.

* **CrowdStrike** – один из лидеров рынка EDR, изначально прославившийся использованием **поведенческого ИИ** для обнаружения сложных угроз на конечных устройствах. Их облачная платформа Falcon постоянно обучает модели на триллионах событий, что дает очень быструю реакцию на новые атаки (часто быстрее, чем сигнатурные обновления конкурентов). В 2024 CrowdStrike расширила портфель AI-решений, купив компанию **Adaptive Shield** – это платформа для автоматизированного поиска слабых мест в настройках корпоративных приложений (SaaS Security Posture Management), которая также включает интеллектуальные алгоритмы анализа. CrowdStrike инвестирует и в **Threat Intelligence на базе AI** (их подразделение FalconX применяет ML для атрибуции атак и выдачи прогнозов угроз). Также компания экспериментирует с LLM для помощи своим аналитикам (чат-бот внутри консоли Falcon, отвечающий на запросы о телеметрии).

* **Darktrace** – британская компания, пионер применения **самообучающихся алгоритмов** (unsupervised ML) для сетевой безопасности. Ее продукт Enterprise Immune System еще до 2020 умел строить поведенческую модель сети и ловить аномалии. В 2024 Darktrace выпустила обновления с интеграцией **генеративного ИИ для пояснений** – система не только выявляет инцидент, но и сама пишет понятный отчет, *“почему это подозрительно”*, на основе своей внутренней модели. Darktrace также предлагает модули “АИ-аналитика” для анализа электронных писем (Antigena Email) и облачных рабочих нагрузок, продолжая тенденцию использования ИИ для автономной защиты (их маркетинг продвигает идею **“AI-астант, останавливающий атаки за секунды”**). Компания регулярно публикует **отчеты об угрозах** с акцентом на AI-aspects, например, обзор 2024 Threat Insights отметил рост AI-генерированных атак и описал случаи, когда их ML-модель ловила киберпреступников, использующих ChatGPT для маскировки действий.

* **Многочисленные стартапы и новые проекты.** 2024 год стал урожайным на стартапы в сфере AI for Cybersecurity:

  * **Wiz** – облачная безопасность, стала самым дорогим стартапом Израиля, фокусируется на поиске уязвимостей и неверных настроек в облаках с помощью автоматизации и ИИ; партнерство с **Dazz** усилило их возможности, предоставляя автоматическое исправление проблем (remediation) с AI-подсказками.
  * **Dazz** – израильский стартап (ныне часть Wiz), применяет ИИ для **приоритизации и устранения уязвимостей в облачной инфраструктуре**, сокращая среднее время исправления на **810% (в 9 раз) по собственным данным**.
  * **Normalyze** – калифорнийский стартап (куплен Proofpoint), использует AI для **обнаружения и классификации чувствительных данных** в облаках и базах, выявления рисков доступа и выстраивания remedation-планов.
  * **MIND AI** – израильский проект, привлекший \$11 млн, заявляет о применении **генеративного ИИ и автоматизации для предотвращения утечек данных** (DLP нового поколения).
  * **Cyera** – еще один представитель data security, за 2024 дважды привлек по \$100+ млн, разрабатывает **платформу киберустойчивости данных на базе ИИ**, помогающую компаниям видеть, где хранятся их критичные данные и как они защищены, с автоматизированными рекомендациями.
  * **SandboxAQ** – компания, работающая на стыке квантовых технологий и ИИ, получила \$300 млн инвестиций; одна из целей – использовать квантовые алгоритмы для улучшения AI-моделей в криптографии и безопасности.
  * Стартапы типа **Aiden (MeetAiden)** предлагают AI-решения для **автоматического hardening’а**: их ИИ-агент находит слабые настройки ПК и серверов и сам применяет политики улучшения безопасности.
  * **Adlumin** и **Tata** (консалтинговый гигант TCS) отмечали, что даже “низкоуровневые” атакующие теперь могут строить продуманные социальные атаки с помощью открытых AI-моделей, поэтому они внедряют сервисы, которые мониторят упоминания брендов компаний и их сотрудников в публичных данных с помощью NLP, чтобы предупреждать возможные фишинг-кампании.

* **Отраслевые инициативы и сотрудничества.** Наряду с коммерческими продуктами, 2024–2025 ознаменованы рядом совместных инициатив:

  * **DARPA AI Cyber Challenge (AIxCC)** – уже упомянутый большой конкурс под эгидой правительства США, привлекший поддержку OpenAI, Google, Anthropic, Microsoft. Цель – рывок в технологиях автоматической защиты ПО с помощью ИИ.
  * **Open-Source проекты**: активисты безопасности и ML запустили несколько проектов с открытым кодом – от инструментов для генерации фишинговых тестов с ChatGPT до репозиториев с подборками приемов RL для кибербезопасности. Например, на GitHub появилась сводная коллекция *Awesome Reinforcement Learning for Cybersecurity*, содержащая ссылки на академические статьи 2024 года и прототипы систем (для сетевой безопасности, для анализа вредоносов).
  * **OWASP GenAI Security Project** – сообщество специалистов по безопасности, занимающееся выработкой рекомендаций и образовательных материалов по безопасности генеративного ИИ (от отчетов по deepfake-угрозам до глоссария терминов). Они проводят митапы, выпустили “OWASP Top 10 Risks for LLM 2024” и продолжают обновлять список на 2025, что уже становится отраслевым стандартом де-факто.
  * **Консорциумы и альянсы.** Кибербезопасностные компании объединяются для обмена данными обучения: например, Cyber Threat Alliance (CTA) рассматривает возможность делиться обезличенными IoC для обучения общих моделей детектирования. **Cloud Security Alliance** в своем отчете 2024 призвала инвестировать в технологии, обеспечивающие приватность данных при использовании ИИ (включая федеративное обучение и дифференциальную приватность).
  * **Академические конференции**: топ-конференции по ИИ (NeurIPS, DEF CON AI Village, Black Hat AI Arsenal) все больше внимания уделяют безопасности. На NeurIPS 2024 отдельный трек был посвящен **безопасности и надежности LLM**, где представили 6 заметных исследований по повышению робастности языковых моделей. А на хакерской конференции DEF CON 2024 прошла масштабная сессия “Red Team LLM” – где участники пытались взломать сразу несколько предоставленных крупных моделей (под контролем Anthropic, OpenAI и Ко) и делились отчетами об уязвимостях. Такие мероприятия помогают улучшать модели в дальнейшем.

В совокупности, экосистема AI в кибербезопасности в 2024–2025 гг. представляет собой бурлящий котел инноваций: большие корпорации интегрируют AI повсеместно и обмениваются опытом через отраслевые союзы; стартапы привносят свежие идеи и решения нишевых проблем; научное сообщество предлагает новые алгоритмы и оценивает их эффективность. Все это движется к общей цели – сделать цифровой мир более защищенным, несмотря на усиление атакующих технологий.

## Заключение

**Искусственный интеллект стал неотъемлемой частью кибербезопасности**, кардинально преобразив методы защиты и атаки в 2024–2025 годах. С одной стороны, организации получили интеллектуальные средства, которые усиливают их оборону: от мгновенного обнаружения аномалий в потоках данных до автоматизированного реагирования и предсказательной аналитики угроз. AI-инструменты позволяют защитникам работать на упреждение, закрывая уязвимости и обнаруживая скрытые атаки до того, как те приведут к ущербу. С другой стороны, эти же технологии дали злоумышленникам новые возможности – более масштабные фишинговые кампании, реалистичные социальные инженерные уловки с deepfake, генерация вредоносного кода и многое другое. Баланс сил в киберпространстве сместился, требуя от всех участников игры повышенной изобретательности.

Главный вывод периода 2024–2025 гг.: **противостоять ИИ-усиленным угрозам можно только с помощью самого ИИ**. Кибербезопасность вступила в эпоху, когда скорость и сложность атак превышают человеческие возможности реагирования без поддержки машинного интеллекта. Поэтому компании повсеместно “вооружаются” AI-защитниками, а инвестиции в эту сферу бьют рекорды. Появляется даже своего рода «гонка вооружений», где каждая сторона старается превзойти другую в креативности использования искусственного интеллекта.

Тем не менее, у защитников есть ключевое преимущество – сотрудничество и обмен знаниями. От открытых исследований до совместных инициатив вроде DARPA Challenge – мировое сообщество работает над тем, чтобы сделать ИИ безопасным и направить его потенциал во благо. *Искусственный интеллект не панацея, но мощный союзник*: в связке с опытом и интуицией специалистов по безопасности он уже сейчас предотвращает бесчисленные инциденты. В ближайшие годы можно ожидать дальнейшего прогресса: более **прозрачных и объяснимых AI-моделей**, стандартов по безопасному внедрению ИИ, и новых инновационных решений на стыке разных областей (например, соединение возможностей ИИ с данными от миллиарда устройств в реальном времени для глобального “центра киберразведки”).

**Кибербезопасность на пороге 2025 года** – это динамичная сфера, в которой ИИ играет роль катализатора изменений. Те организации, которые научатся эффективно использовать AI и управлять сопутствующими рисками, получат существенное преимущество в защите своих цифровых активов. В то же время игнорирование этого тренда увеличивает уязвимость перед новыми видами атак. Как показывает практика последних двух лет, **баланс сил склоняется в пользу тех, кто “научил” ИИ стоять на своей стороне баррикад**. Именно поэтому тема применения ИИ в кибербезопасности будет оставаться в фокусе и дальше, а 2024–2025 годы войдут в историю как период бурного роста и становления этого направления.
